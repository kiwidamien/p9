[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "p9",
    "section": "",
    "text": ":::: {.columns}\n\n\nSeries\n\nggplot2-series\nThis series contains a great deal of tips, tricks and packages that you can use to level up your ggplot game.\n\n\npresto-series\nLookup SQL syntax quirks.\n\n\npython-series\nLookup pandas and python package management tricks.\n\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNamed aggregations\n\n\n\n\n\n\npandas\n\n\n\n\n\n\n\n\n\nOct 28, 2024\n\n\n1 min\n\n\n\n\n\n\n\nPandas: Transform like behavior using data from multiple columns\n\n\n\n\n\n\npandas\n\n\n\n\n\n\n\n\n\nOct 28, 2024\n\n\n1 min\n\n\n\n\n\n\n\nTimezones in Pandas\n\n\n\n\n\n\npandas\n\n\ntimezones\n\n\n\n\n\n\n\n\n\nOct 27, 2024\n\n\n2 min\n\n\n\n\n\n\n\nOrdering categories\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\ngallery\n\n\ncategorical\n\n\n\nCategories in legends are (by default) ordered alphabetically. To customize the ordering, you should be using pd.Categories\n\n\n\n\n\nOct 20, 2024\n\n\n3 min\n\n\n\n\n\n\n\nLogging - How to and best practices\n\n\n\n\n\n\npython\n\n\nlogging\n\n\n\nGiving some of the best logging practices, and clarifying some of the logging confusion.\n\n\n\n\n\nOct 17, 2024\n\n\n12 min\n\n\n\n\n\n\n\nTiming imports\n\n\n\n\n\n\npython\n\n\npackage-management\n\n\n\nTracing particularly slow imports\n\n\n\n\n\nOct 14, 2024\n\n\n2 min\n\n\n\n\n\n\n\nHistograms\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\nhistogram\n\n\n\nMaking simple and relative histograms\n\n\n\n\n\nOct 9, 2024\n\n\n4 min\n\n\n\n\n\n\n\nStats Models: Linear Regression\n\n\n\n\n\n\npandas\n\n\nstatsmodels\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n1 min\n\n\n\n\n\n\n\nPandas: Time-series on multiple columns (grouper)\n\n\n\n\n\n\npandas\n\n\n\n\n\n\n\n\n\nSep 10, 2024\n\n\n2 min\n\n\n\n\n\n\n\nGnatt Charts\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\ncategorical\n\n\n\nMaking a Gnatt chart\n\n\n\n\n\nAug 18, 2024\n\n\n1 min\n\n\n\n\n\n\n\nChanging the size of plots\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\n\nMaking plots wider\n\n\n\n\n\nMay 27, 2024\n\n\n1 min\n\n\n\n\n\n\n\nDealing with dates on axis\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\n\nDisplaying dates in plots\n\n\n\n\n\nMay 27, 2024\n\n\n2 min\n\n\n\n\n\n\n\nPublishing Python Packages\n\n\n\n\n\n\npython\n\n\npackage-management\n\n\n\nHow to push packages to artifactory with either setup.py or pyproject.toml\n\n\n\n\n\nMay 27, 2024\n\n\n1 min\n\n\n\n\n\n\n\nThe dice rolls problem\n\n\n\n\n\n\ninterview\n\n\npuzzles\n\n\n\n\n\n\n\n\n\nMay 22, 2024\n\n\n5 min\n\n\n\n\n\n\n\nAnnotated area charts with plotnine\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\ngallery\n\n\n\nGallery plot showing clean anntoations and usage of areas\n\n\n\n\n\nMay 11, 2024\n\n\n1 min\n\n\n\n\n\n\n\nEnsemble of Confidence Intervals\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\ngallery\n\n\n\nMake a plot with confidence intervals, and infeasible regions shaded out\n\n\n\n\n\nMay 11, 2024\n\n\n1 min\n\n\n\n\n\n\n\nDates in presto\n\n\n\n\n\n\npresto\n\n\nsql\n\n\ndates\n\n\n\nEverytime I go to do dates in presto I have to look up how to do conversions.\n\n\n\n\n\nMay 7, 2024\n\n\n1 min\n\n\n\n\n\n\n\nMaking a custom palette\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\n\nHow to create a custom palette in plotnine\n\n\n\n\n\nMay 2, 2024\n\n\n1 min\n\n\n\n\n\n\n\nMaking a waterfall chart\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\ngallery\n\n\n\nTranslates an example of a waterfall chart from the ggplot flipbook\n\n\n\n\n\nMay 1, 2024\n\n\n1 min\n\n\n\n\n\n\n\nAnnotating single points in datasets\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\n\nHow to annotate single data points in a plot\n\n\n\n\n\nMay 1, 2024\n\n\n1 min\n\n\n\n\n\n\n\nQuick EDA settings hacks\n\n\n\n\n\n\npandas\n\n\nmatplotlib\n\n\nnotebook\n\n\nEDA\n\n\n\nSettings that make notebooks easier to use, especially for EDA\n\n\n\n\n\nMay 1, 2024\n\n\n1 min\n\n\n\n\n\n\n\nWorking with categories\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\ncategorical\n\n\n\nWorking with categorical data types in plotnine\n\n\n\n\n\nApr 30, 2024\n\n\n1 min\n\n\n\n\n\n\n\nSplitting plots into functions\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\n\nShows how to decompose a plot into different functions, with the goal of being able to modularize the code used.\n\n\n\n\n\nApr 29, 2024\n\n\n2 min\n\n\n\n\n\n\n\nShading regions on a plot\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\n\nShowed how to shade a region on a plot\n\n\n\n\n\nApr 29, 2024\n\n\n1 min\n\n\n\n\n\n\n\nChanging labels to percentages\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\n\nCustomizing the labels on a scale is one of the things that is different from ggplot. We use a function to format the label.\n\n\n\n\n\nApr 29, 2024\n\n\n1 min\n\n\n\n\n\n\n\nMaking manual error bars / error regions\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\n\nShows how to add error bars/regions manually to plots, both as a region of uncertainty, and on individual data points.\n\n\n\n\n\nApr 28, 2024\n\n\n1 min\n\n\n\n\n\n\n\nMaking dual bar charts\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\n\nPopulations pyramids often show not just the number of people in each age bracket, but also seggregate it by gender.\n\n\n\n\n\nApr 28, 2024\n\n\n2 min\n\n\n\n\n\n\n\nForcing Variables to be Categorical\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\n\nPlotnine will treat numeric quantities as continuous, and generators continuous legends. We can use ‘factor’ to force the variable to be treated as cateogrical.\n\n\n\n\n\nApr 28, 2024\n\n\n1 min\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nApr 24, 2024\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "ggplot-series.html",
    "href": "ggplot-series.html",
    "title": "Series: ggplot2-tips",
    "section": "",
    "text": "Ordering categories\n\n\n\n\n\nCategories in legends are (by default) ordered alphabetically. To customize the ordering, you should be using pd.Categories\n\n\n\n\n\nOct 20, 2024\n\n\n3 min\n\n\n\n\n\n\n\nHistograms\n\n\n\n\n\nMaking simple and relative histograms\n\n\n\n\n\nOct 9, 2024\n\n\n4 min\n\n\n\n\n\n\n\nGnatt Charts\n\n\n\n\n\nMaking a Gnatt chart\n\n\n\n\n\nAug 18, 2024\n\n\n1 min\n\n\n\n\n\n\n\nChanging the size of plots\n\n\n\n\n\nMaking plots wider\n\n\n\n\n\nMay 27, 2024\n\n\n1 min\n\n\n\n\n\n\n\nDealing with dates on axis\n\n\n\n\n\nDisplaying dates in plots\n\n\n\n\n\nMay 27, 2024\n\n\n2 min\n\n\n\n\n\n\n\nAnnotated area charts with plotnine\n\n\n\n\n\nGallery plot showing clean anntoations and usage of areas\n\n\n\n\n\nMay 11, 2024\n\n\n1 min\n\n\n\n\n\n\n\nEnsemble of Confidence Intervals\n\n\n\n\n\nMake a plot with confidence intervals, and infeasible regions shaded out\n\n\n\n\n\nMay 11, 2024\n\n\n1 min\n\n\n\n\n\n\n\nMaking a custom palette\n\n\n\n\n\nHow to create a custom palette in plotnine\n\n\n\n\n\nMay 2, 2024\n\n\n1 min\n\n\n\n\n\n\n\nMaking a waterfall chart\n\n\n\n\n\nTranslates an example of a waterfall chart from the ggplot flipbook\n\n\n\n\n\nMay 1, 2024\n\n\n1 min\n\n\n\n\n\n\n\nAnnotating single points in datasets\n\n\n\n\n\nHow to annotate single data points in a plot\n\n\n\n\n\nMay 1, 2024\n\n\n1 min\n\n\n\n\n\n\n\nWorking with categories\n\n\n\n\n\nWorking with categorical data types in plotnine\n\n\n\n\n\nApr 30, 2024\n\n\n1 min\n\n\n\n\n\n\n\nSplitting plots into functions\n\n\n\n\n\nShows how to decompose a plot into different functions, with the goal of being able to modularize the code used.\n\n\n\n\n\nApr 29, 2024\n\n\n2 min\n\n\n\n\n\n\n\nShading regions on a plot\n\n\n\n\n\nShowed how to shade a region on a plot\n\n\n\n\n\nApr 29, 2024\n\n\n1 min\n\n\n\n\n\n\n\nChanging labels to percentages\n\n\n\n\n\nCustomizing the labels on a scale is one of the things that is different from ggplot. We use a function to format the label.\n\n\n\n\n\nApr 29, 2024\n\n\n1 min\n\n\n\n\n\n\n\nMaking manual error bars / error regions\n\n\n\n\n\nShows how to add error bars/regions manually to plots, both as a region of uncertainty, and on individual data points.\n\n\n\n\n\nApr 28, 2024\n\n\n1 min\n\n\n\n\n\n\n\nMaking dual bar charts\n\n\n\n\n\nPopulations pyramids often show not just the number of people in each age bracket, but also seggregate it by gender.\n\n\n\n\n\nApr 28, 2024\n\n\n2 min\n\n\n\n\n\n\n\nForcing Variables to be Categorical\n\n\n\n\n\nPlotnine will treat numeric quantities as continuous, and generators continuous legends. We can use ‘factor’ to force the variable to be treated as cateogrical.\n\n\n\n\n\nApr 28, 2024\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/pandas/timezones.html",
    "href": "posts/pandas/timezones.html",
    "title": "Timezones in Pandas",
    "section": "",
    "text": "Problem\nPandas has it’s on version of datetimes. If you recieve a datetime in UTC (but only because you know it from context, not because it was labelled), how do you\n\nassign a timezone\nconvert from one timezone to another\n\n\n\nSolution\nUse the datetime series accessor on a series s of datetime values:\n\ns.dt.tz_localize(\"tz name as string\") to attach a timezone to a series that does not already have one\ns.dt.tz_convert(\"new tz name as string\") to convert from the existing timezone to the new one specified\n\n\n\nExample\nLet’s artifically create a dataframe for a ghost on Hallowe’en, logging the days events. When converting to a datetime, we could add a timezone, but we won’t do that here (emulating the case when you get a timezone out of a database).\n\nimport pandas as pd \n\ndaily_ghoul_report = pd.DataFrame([\n    {'event': 'breakfast', 'timestamp': '2024-10-31 08:00:00'},\n    {'event': 'go to work', 'timestamp': '2024-10-31 09:00:00'},\n    {'event': 'project X', 'timestamp': '2024-10-31 11:00:00'},\n    {'event': 'finish work', 'timestamp': '2024-10-31 18:00:00'},\n    {'event': 'haunt houses', 'timestamp': '2024-10-31 22:00:00'}\n])\ndaily_ghoul_report['timestamp'] = pd.to_datetime(daily_ghoul_report['timestamp'])\n\nLet’s say that we knew this event happened in Salem, Massachusettes. Let’s assign the correct timestamp\n\ndaily_ghoul_report['timestamp'] = daily_ghoul_report['timestamp'].dt.tz_localize('US/Eastern')\ndaily_ghoul_report\n\n\n\n\n\n\n\n\n\nevent\ntimestamp\n\n\n\n\n0\nbreakfast\n2024-10-31 08:00:00-04:00\n\n\n1\ngo to work\n2024-10-31 09:00:00-04:00\n\n\n2\nproject X\n2024-10-31 11:00:00-04:00\n\n\n3\nfinish work\n2024-10-31 18:00:00-04:00\n\n\n4\nhaunt houses\n2024-10-31 22:00:00-04:00\n\n\n\n\n\n\n\n\nThe UTC offset has now been applied! Note this is not idempotent; if you try to tz_localize a timestamp that already has a timezone/UTC offset, then you will get an error.\nThis is because a tz_localize is supposed to make a timezone without changing the values of the times. If you have a timezone and you want the same time in a different timezone, you’ll want to convert using tz_convert (which changes the offset and the time, so that you are still referring to the same time in UTC).\nLet’s convert to the Pacific timezone:\n\ndaily_ghoul_report['pacific_time'] = daily_ghoul_report['timestamp'].dt.tz_convert('US/Pacific')\n\ndaily_ghoul_report\n\n\n\n\n\n\n\n\n\nevent\ntimestamp\npacific_time\n\n\n\n\n0\nbreakfast\n2024-10-31 08:00:00-04:00\n2024-10-31 05:00:00-07:00\n\n\n1\ngo to work\n2024-10-31 09:00:00-04:00\n2024-10-31 06:00:00-07:00\n\n\n2\nproject X\n2024-10-31 11:00:00-04:00\n2024-10-31 08:00:00-07:00\n\n\n3\nfinish work\n2024-10-31 18:00:00-04:00\n2024-10-31 15:00:00-07:00\n\n\n4\nhaunt houses\n2024-10-31 22:00:00-04:00\n2024-10-31 19:00:00-07:00\n\n\n\n\n\n\n\n\nImportant timezones you might want to use * UTC * US/Eastern * US/Central * US/Mountain * US/Pacific * NZ\nTo find a list of supported timezones, you can use the following code snippet:\n\nimport pytz  # What pandas uses under the hood\n\nsorted(pytz.all_timezones_set)\n\n['Africa/Abidjan',\n 'Africa/Accra',\n 'Africa/Addis_Ababa',\n 'Africa/Algiers',\n 'Africa/Asmara',\n 'Africa/Asmera',\n 'Africa/Bamako',\n 'Africa/Bangui',\n 'Africa/Banjul',\n 'Africa/Bissau',\n 'Africa/Blantyre',\n 'Africa/Brazzaville',\n 'Africa/Bujumbura',\n 'Africa/Cairo',\n 'Africa/Casablanca',\n 'Africa/Ceuta',\n 'Africa/Conakry',\n 'Africa/Dakar',\n 'Africa/Dar_es_Salaam',\n 'Africa/Djibouti',\n 'Africa/Douala',\n 'Africa/El_Aaiun',\n 'Africa/Freetown',\n 'Africa/Gaborone',\n 'Africa/Harare',\n 'Africa/Johannesburg',\n 'Africa/Juba',\n 'Africa/Kampala',\n 'Africa/Khartoum',\n 'Africa/Kigali',\n 'Africa/Kinshasa',\n 'Africa/Lagos',\n 'Africa/Libreville',\n 'Africa/Lome',\n 'Africa/Luanda',\n 'Africa/Lubumbashi',\n 'Africa/Lusaka',\n 'Africa/Malabo',\n 'Africa/Maputo',\n 'Africa/Maseru',\n 'Africa/Mbabane',\n 'Africa/Mogadishu',\n 'Africa/Monrovia',\n 'Africa/Nairobi',\n 'Africa/Ndjamena',\n 'Africa/Niamey',\n 'Africa/Nouakchott',\n 'Africa/Ouagadougou',\n 'Africa/Porto-Novo',\n 'Africa/Sao_Tome',\n 'Africa/Timbuktu',\n 'Africa/Tripoli',\n 'Africa/Tunis',\n 'Africa/Windhoek',\n 'America/Adak',\n 'America/Anchorage',\n 'America/Anguilla',\n 'America/Antigua',\n 'America/Araguaina',\n 'America/Argentina/Buenos_Aires',\n 'America/Argentina/Catamarca',\n 'America/Argentina/ComodRivadavia',\n 'America/Argentina/Cordoba',\n 'America/Argentina/Jujuy',\n 'America/Argentina/La_Rioja',\n 'America/Argentina/Mendoza',\n 'America/Argentina/Rio_Gallegos',\n 'America/Argentina/Salta',\n 'America/Argentina/San_Juan',\n 'America/Argentina/San_Luis',\n 'America/Argentina/Tucuman',\n 'America/Argentina/Ushuaia',\n 'America/Aruba',\n 'America/Asuncion',\n 'America/Atikokan',\n 'America/Atka',\n 'America/Bahia',\n 'America/Bahia_Banderas',\n 'America/Barbados',\n 'America/Belem',\n 'America/Belize',\n 'America/Blanc-Sablon',\n 'America/Boa_Vista',\n 'America/Bogota',\n 'America/Boise',\n 'America/Buenos_Aires',\n 'America/Cambridge_Bay',\n 'America/Campo_Grande',\n 'America/Cancun',\n 'America/Caracas',\n 'America/Catamarca',\n 'America/Cayenne',\n 'America/Cayman',\n 'America/Chicago',\n 'America/Chihuahua',\n 'America/Ciudad_Juarez',\n 'America/Coral_Harbour',\n 'America/Cordoba',\n 'America/Costa_Rica',\n 'America/Creston',\n 'America/Cuiaba',\n 'America/Curacao',\n 'America/Danmarkshavn',\n 'America/Dawson',\n 'America/Dawson_Creek',\n 'America/Denver',\n 'America/Detroit',\n 'America/Dominica',\n 'America/Edmonton',\n 'America/Eirunepe',\n 'America/El_Salvador',\n 'America/Ensenada',\n 'America/Fort_Nelson',\n 'America/Fort_Wayne',\n 'America/Fortaleza',\n 'America/Glace_Bay',\n 'America/Godthab',\n 'America/Goose_Bay',\n 'America/Grand_Turk',\n 'America/Grenada',\n 'America/Guadeloupe',\n 'America/Guatemala',\n 'America/Guayaquil',\n 'America/Guyana',\n 'America/Halifax',\n 'America/Havana',\n 'America/Hermosillo',\n 'America/Indiana/Indianapolis',\n 'America/Indiana/Knox',\n 'America/Indiana/Marengo',\n 'America/Indiana/Petersburg',\n 'America/Indiana/Tell_City',\n 'America/Indiana/Vevay',\n 'America/Indiana/Vincennes',\n 'America/Indiana/Winamac',\n 'America/Indianapolis',\n 'America/Inuvik',\n 'America/Iqaluit',\n 'America/Jamaica',\n 'America/Jujuy',\n 'America/Juneau',\n 'America/Kentucky/Louisville',\n 'America/Kentucky/Monticello',\n 'America/Knox_IN',\n 'America/Kralendijk',\n 'America/La_Paz',\n 'America/Lima',\n 'America/Los_Angeles',\n 'America/Louisville',\n 'America/Lower_Princes',\n 'America/Maceio',\n 'America/Managua',\n 'America/Manaus',\n 'America/Marigot',\n 'America/Martinique',\n 'America/Matamoros',\n 'America/Mazatlan',\n 'America/Mendoza',\n 'America/Menominee',\n 'America/Merida',\n 'America/Metlakatla',\n 'America/Mexico_City',\n 'America/Miquelon',\n 'America/Moncton',\n 'America/Monterrey',\n 'America/Montevideo',\n 'America/Montreal',\n 'America/Montserrat',\n 'America/Nassau',\n 'America/New_York',\n 'America/Nipigon',\n 'America/Nome',\n 'America/Noronha',\n 'America/North_Dakota/Beulah',\n 'America/North_Dakota/Center',\n 'America/North_Dakota/New_Salem',\n 'America/Nuuk',\n 'America/Ojinaga',\n 'America/Panama',\n 'America/Pangnirtung',\n 'America/Paramaribo',\n 'America/Phoenix',\n 'America/Port-au-Prince',\n 'America/Port_of_Spain',\n 'America/Porto_Acre',\n 'America/Porto_Velho',\n 'America/Puerto_Rico',\n 'America/Punta_Arenas',\n 'America/Rainy_River',\n 'America/Rankin_Inlet',\n 'America/Recife',\n 'America/Regina',\n 'America/Resolute',\n 'America/Rio_Branco',\n 'America/Rosario',\n 'America/Santa_Isabel',\n 'America/Santarem',\n 'America/Santiago',\n 'America/Santo_Domingo',\n 'America/Sao_Paulo',\n 'America/Scoresbysund',\n 'America/Shiprock',\n 'America/Sitka',\n 'America/St_Barthelemy',\n 'America/St_Johns',\n 'America/St_Kitts',\n 'America/St_Lucia',\n 'America/St_Thomas',\n 'America/St_Vincent',\n 'America/Swift_Current',\n 'America/Tegucigalpa',\n 'America/Thule',\n 'America/Thunder_Bay',\n 'America/Tijuana',\n 'America/Toronto',\n 'America/Tortola',\n 'America/Vancouver',\n 'America/Virgin',\n 'America/Whitehorse',\n 'America/Winnipeg',\n 'America/Yakutat',\n 'America/Yellowknife',\n 'Antarctica/Casey',\n 'Antarctica/Davis',\n 'Antarctica/DumontDUrville',\n 'Antarctica/Macquarie',\n 'Antarctica/Mawson',\n 'Antarctica/McMurdo',\n 'Antarctica/Palmer',\n 'Antarctica/Rothera',\n 'Antarctica/South_Pole',\n 'Antarctica/Syowa',\n 'Antarctica/Troll',\n 'Antarctica/Vostok',\n 'Arctic/Longyearbyen',\n 'Asia/Aden',\n 'Asia/Almaty',\n 'Asia/Amman',\n 'Asia/Anadyr',\n 'Asia/Aqtau',\n 'Asia/Aqtobe',\n 'Asia/Ashgabat',\n 'Asia/Ashkhabad',\n 'Asia/Atyrau',\n 'Asia/Baghdad',\n 'Asia/Bahrain',\n 'Asia/Baku',\n 'Asia/Bangkok',\n 'Asia/Barnaul',\n 'Asia/Beirut',\n 'Asia/Bishkek',\n 'Asia/Brunei',\n 'Asia/Calcutta',\n 'Asia/Chita',\n 'Asia/Choibalsan',\n 'Asia/Chongqing',\n 'Asia/Chungking',\n 'Asia/Colombo',\n 'Asia/Dacca',\n 'Asia/Damascus',\n 'Asia/Dhaka',\n 'Asia/Dili',\n 'Asia/Dubai',\n 'Asia/Dushanbe',\n 'Asia/Famagusta',\n 'Asia/Gaza',\n 'Asia/Harbin',\n 'Asia/Hebron',\n 'Asia/Ho_Chi_Minh',\n 'Asia/Hong_Kong',\n 'Asia/Hovd',\n 'Asia/Irkutsk',\n 'Asia/Istanbul',\n 'Asia/Jakarta',\n 'Asia/Jayapura',\n 'Asia/Jerusalem',\n 'Asia/Kabul',\n 'Asia/Kamchatka',\n 'Asia/Karachi',\n 'Asia/Kashgar',\n 'Asia/Kathmandu',\n 'Asia/Katmandu',\n 'Asia/Khandyga',\n 'Asia/Kolkata',\n 'Asia/Krasnoyarsk',\n 'Asia/Kuala_Lumpur',\n 'Asia/Kuching',\n 'Asia/Kuwait',\n 'Asia/Macao',\n 'Asia/Macau',\n 'Asia/Magadan',\n 'Asia/Makassar',\n 'Asia/Manila',\n 'Asia/Muscat',\n 'Asia/Nicosia',\n 'Asia/Novokuznetsk',\n 'Asia/Novosibirsk',\n 'Asia/Omsk',\n 'Asia/Oral',\n 'Asia/Phnom_Penh',\n 'Asia/Pontianak',\n 'Asia/Pyongyang',\n 'Asia/Qatar',\n 'Asia/Qostanay',\n 'Asia/Qyzylorda',\n 'Asia/Rangoon',\n 'Asia/Riyadh',\n 'Asia/Saigon',\n 'Asia/Sakhalin',\n 'Asia/Samarkand',\n 'Asia/Seoul',\n 'Asia/Shanghai',\n 'Asia/Singapore',\n 'Asia/Srednekolymsk',\n 'Asia/Taipei',\n 'Asia/Tashkent',\n 'Asia/Tbilisi',\n 'Asia/Tehran',\n 'Asia/Tel_Aviv',\n 'Asia/Thimbu',\n 'Asia/Thimphu',\n 'Asia/Tokyo',\n 'Asia/Tomsk',\n 'Asia/Ujung_Pandang',\n 'Asia/Ulaanbaatar',\n 'Asia/Ulan_Bator',\n 'Asia/Urumqi',\n 'Asia/Ust-Nera',\n 'Asia/Vientiane',\n 'Asia/Vladivostok',\n 'Asia/Yakutsk',\n 'Asia/Yangon',\n 'Asia/Yekaterinburg',\n 'Asia/Yerevan',\n 'Atlantic/Azores',\n 'Atlantic/Bermuda',\n 'Atlantic/Canary',\n 'Atlantic/Cape_Verde',\n 'Atlantic/Faeroe',\n 'Atlantic/Faroe',\n 'Atlantic/Jan_Mayen',\n 'Atlantic/Madeira',\n 'Atlantic/Reykjavik',\n 'Atlantic/South_Georgia',\n 'Atlantic/St_Helena',\n 'Atlantic/Stanley',\n 'Australia/ACT',\n 'Australia/Adelaide',\n 'Australia/Brisbane',\n 'Australia/Broken_Hill',\n 'Australia/Canberra',\n 'Australia/Currie',\n 'Australia/Darwin',\n 'Australia/Eucla',\n 'Australia/Hobart',\n 'Australia/LHI',\n 'Australia/Lindeman',\n 'Australia/Lord_Howe',\n 'Australia/Melbourne',\n 'Australia/NSW',\n 'Australia/North',\n 'Australia/Perth',\n 'Australia/Queensland',\n 'Australia/South',\n 'Australia/Sydney',\n 'Australia/Tasmania',\n 'Australia/Victoria',\n 'Australia/West',\n 'Australia/Yancowinna',\n 'Brazil/Acre',\n 'Brazil/DeNoronha',\n 'Brazil/East',\n 'Brazil/West',\n 'CET',\n 'CST6CDT',\n 'Canada/Atlantic',\n 'Canada/Central',\n 'Canada/Eastern',\n 'Canada/Mountain',\n 'Canada/Newfoundland',\n 'Canada/Pacific',\n 'Canada/Saskatchewan',\n 'Canada/Yukon',\n 'Chile/Continental',\n 'Chile/EasterIsland',\n 'Cuba',\n 'EET',\n 'EST',\n 'EST5EDT',\n 'Egypt',\n 'Eire',\n 'Etc/GMT',\n 'Etc/GMT+0',\n 'Etc/GMT+1',\n 'Etc/GMT+10',\n 'Etc/GMT+11',\n 'Etc/GMT+12',\n 'Etc/GMT+2',\n 'Etc/GMT+3',\n 'Etc/GMT+4',\n 'Etc/GMT+5',\n 'Etc/GMT+6',\n 'Etc/GMT+7',\n 'Etc/GMT+8',\n 'Etc/GMT+9',\n 'Etc/GMT-0',\n 'Etc/GMT-1',\n 'Etc/GMT-10',\n 'Etc/GMT-11',\n 'Etc/GMT-12',\n 'Etc/GMT-13',\n 'Etc/GMT-14',\n 'Etc/GMT-2',\n 'Etc/GMT-3',\n 'Etc/GMT-4',\n 'Etc/GMT-5',\n 'Etc/GMT-6',\n 'Etc/GMT-7',\n 'Etc/GMT-8',\n 'Etc/GMT-9',\n 'Etc/GMT0',\n 'Etc/Greenwich',\n 'Etc/UCT',\n 'Etc/UTC',\n 'Etc/Universal',\n 'Etc/Zulu',\n 'Europe/Amsterdam',\n 'Europe/Andorra',\n 'Europe/Astrakhan',\n 'Europe/Athens',\n 'Europe/Belfast',\n 'Europe/Belgrade',\n 'Europe/Berlin',\n 'Europe/Bratislava',\n 'Europe/Brussels',\n 'Europe/Bucharest',\n 'Europe/Budapest',\n 'Europe/Busingen',\n 'Europe/Chisinau',\n 'Europe/Copenhagen',\n 'Europe/Dublin',\n 'Europe/Gibraltar',\n 'Europe/Guernsey',\n 'Europe/Helsinki',\n 'Europe/Isle_of_Man',\n 'Europe/Istanbul',\n 'Europe/Jersey',\n 'Europe/Kaliningrad',\n 'Europe/Kiev',\n 'Europe/Kirov',\n 'Europe/Kyiv',\n 'Europe/Lisbon',\n 'Europe/Ljubljana',\n 'Europe/London',\n 'Europe/Luxembourg',\n 'Europe/Madrid',\n 'Europe/Malta',\n 'Europe/Mariehamn',\n 'Europe/Minsk',\n 'Europe/Monaco',\n 'Europe/Moscow',\n 'Europe/Nicosia',\n 'Europe/Oslo',\n 'Europe/Paris',\n 'Europe/Podgorica',\n 'Europe/Prague',\n 'Europe/Riga',\n 'Europe/Rome',\n 'Europe/Samara',\n 'Europe/San_Marino',\n 'Europe/Sarajevo',\n 'Europe/Saratov',\n 'Europe/Simferopol',\n 'Europe/Skopje',\n 'Europe/Sofia',\n 'Europe/Stockholm',\n 'Europe/Tallinn',\n 'Europe/Tirane',\n 'Europe/Tiraspol',\n 'Europe/Ulyanovsk',\n 'Europe/Uzhgorod',\n 'Europe/Vaduz',\n 'Europe/Vatican',\n 'Europe/Vienna',\n 'Europe/Vilnius',\n 'Europe/Volgograd',\n 'Europe/Warsaw',\n 'Europe/Zagreb',\n 'Europe/Zaporozhye',\n 'Europe/Zurich',\n 'GB',\n 'GB-Eire',\n 'GMT',\n 'GMT+0',\n 'GMT-0',\n 'GMT0',\n 'Greenwich',\n 'HST',\n 'Hongkong',\n 'Iceland',\n 'Indian/Antananarivo',\n 'Indian/Chagos',\n 'Indian/Christmas',\n 'Indian/Cocos',\n 'Indian/Comoro',\n 'Indian/Kerguelen',\n 'Indian/Mahe',\n 'Indian/Maldives',\n 'Indian/Mauritius',\n 'Indian/Mayotte',\n 'Indian/Reunion',\n 'Iran',\n 'Israel',\n 'Jamaica',\n 'Japan',\n 'Kwajalein',\n 'Libya',\n 'MET',\n 'MST',\n 'MST7MDT',\n 'Mexico/BajaNorte',\n 'Mexico/BajaSur',\n 'Mexico/General',\n 'NZ',\n 'NZ-CHAT',\n 'Navajo',\n 'PRC',\n 'PST8PDT',\n 'Pacific/Apia',\n 'Pacific/Auckland',\n 'Pacific/Bougainville',\n 'Pacific/Chatham',\n 'Pacific/Chuuk',\n 'Pacific/Easter',\n 'Pacific/Efate',\n 'Pacific/Enderbury',\n 'Pacific/Fakaofo',\n 'Pacific/Fiji',\n 'Pacific/Funafuti',\n 'Pacific/Galapagos',\n 'Pacific/Gambier',\n 'Pacific/Guadalcanal',\n 'Pacific/Guam',\n 'Pacific/Honolulu',\n 'Pacific/Johnston',\n 'Pacific/Kanton',\n 'Pacific/Kiritimati',\n 'Pacific/Kosrae',\n 'Pacific/Kwajalein',\n 'Pacific/Majuro',\n 'Pacific/Marquesas',\n 'Pacific/Midway',\n 'Pacific/Nauru',\n 'Pacific/Niue',\n 'Pacific/Norfolk',\n 'Pacific/Noumea',\n 'Pacific/Pago_Pago',\n 'Pacific/Palau',\n 'Pacific/Pitcairn',\n 'Pacific/Pohnpei',\n 'Pacific/Ponape',\n 'Pacific/Port_Moresby',\n 'Pacific/Rarotonga',\n 'Pacific/Saipan',\n 'Pacific/Samoa',\n 'Pacific/Tahiti',\n 'Pacific/Tarawa',\n 'Pacific/Tongatapu',\n 'Pacific/Truk',\n 'Pacific/Wake',\n 'Pacific/Wallis',\n 'Pacific/Yap',\n 'Poland',\n 'Portugal',\n 'ROC',\n 'ROK',\n 'Singapore',\n 'Turkey',\n 'UCT',\n 'US/Alaska',\n 'US/Aleutian',\n 'US/Arizona',\n 'US/Central',\n 'US/East-Indiana',\n 'US/Eastern',\n 'US/Hawaii',\n 'US/Indiana-Starke',\n 'US/Michigan',\n 'US/Mountain',\n 'US/Pacific',\n 'US/Samoa',\n 'UTC',\n 'Universal',\n 'W-SU',\n 'WET',\n 'Zulu']"
  },
  {
    "objectID": "posts/pandas/post.html",
    "href": "posts/pandas/post.html",
    "title": "Quick EDA settings hacks",
    "section": "",
    "text": "import pandas as pd \npd.set_option('display.min_rows', 500)\npd.set_option('display.max_rows', 500)\nSetting to None will also allow you to show all rows.\nI really like the ability to do this on a one-off basis too:\nfrom IPython.display import display \n\nwith pd.option_context('display.max_rows', 100, 'display.max_columns', 10):\n  display(df)"
  },
  {
    "objectID": "posts/pandas/post.html#setting-the-maximum-number-of-rows",
    "href": "posts/pandas/post.html#setting-the-maximum-number-of-rows",
    "title": "Quick EDA settings hacks",
    "section": "",
    "text": "import pandas as pd \npd.set_option('display.min_rows', 500)\npd.set_option('display.max_rows', 500)\nSetting to None will also allow you to show all rows.\nI really like the ability to do this on a one-off basis too:\nfrom IPython.display import display \n\nwith pd.option_context('display.max_rows', 100, 'display.max_columns', 10):\n  display(df)"
  },
  {
    "objectID": "posts/pandas/post.html#retina-display-quality",
    "href": "posts/pandas/post.html#retina-display-quality",
    "title": "Quick EDA settings hacks",
    "section": "Retina display quality",
    "text": "Retina display quality\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format='retina'"
  },
  {
    "objectID": "posts/pandas/named_aggregations.html",
    "href": "posts/pandas/named_aggregations.html",
    "title": "Named aggregations",
    "section": "",
    "text": "Problem\nIn pandas, we often want to do an aggregation with a groupby, and a rename simultaneously. Let’s see an example of this by getting tne minimum and maximum daily temperatures from a temperature dataset:\n\nimport pandas as pd \nimport vega_datasets\n\nfrom vega_datasets import data\n\nseattle_temperatures = data('seattle-temps')\nseattle_temperatures.head()\n\n\n\n\n\n\n\n\n\ndate\ntemp\n\n\n\n\n0\n2010-01-01 00:00:00\n39.4\n\n\n1\n2010-01-01 01:00:00\n39.2\n\n\n2\n2010-01-01 02:00:00\n39.0\n\n\n3\n2010-01-01 03:00:00\n38.9\n\n\n4\n2010-01-01 04:00:00\n38.8\n\n\n\n\n\n\n\n\nWe will do this the least sophisticated way: make a new column, group by it, and then apply an aggregation:\n\n(\n    seattle_temperatures\n    .assign(date_only=seattle_temperatures['date'].dt.date)\n    .groupby('date_only')\n    ['temp']\n    .agg(['min', 'max'])\n).head()\n\n\n\n\n\n\n\n\n\nmin\nmax\n\n\ndate_only\n\n\n\n\n\n\n2010-01-01\n38.6\n43.5\n\n\n2010-01-02\n38.8\n43.8\n\n\n2010-01-03\n39.0\n44.0\n\n\n2010-01-04\n39.2\n44.2\n\n\n2010-01-05\n39.3\n44.4\n\n\n\n\n\n\n\n\nThis works, but leaves awkward column names (min and max), with no reference to what they are the min or max of. It gets worse if we are trying to look at multiple columns of data:\n\n# Including the date timestamps as part of the groupby dosen't really make sense here\n# but including anyway\n(\n    seattle_temperatures\n    .assign(date_only=seattle_temperatures['date'].dt.date)\n    .groupby('date_only')\n    .agg({'temp': ['min', 'max'], 'date': 'count'})\n).head()\n\n\n\n\n\n\n\n\n\ntemp\ndate\n\n\n\nmin\nmax\ncount\n\n\ndate_only\n\n\n\n\n\n\n\n2010-01-01\n38.6\n43.5\n24\n\n\n2010-01-02\n38.8\n43.8\n24\n\n\n2010-01-03\n39.0\n44.0\n24\n\n\n2010-01-04\n39.2\n44.2\n24\n\n\n2010-01-05\n39.3\n44.4\n24\n\n\n\n\n\n\n\n\nNow we have context, but we also have a multi-index that we have to deal with.\n\n\nSolution\nUse the agg(new_column_name=(column_name, aggfunc), ...) syntax, to rename the columns and keep at a single level.\n\n\nExamples\nSolving the first aggregation (min and max temperatures per day):\n\n(\n    seattle_temperatures\n    .assign(date_only=seattle_temperatures['date'].dt.date)\n    .groupby('date_only')\n    .agg(daily_high_temp=('temp', 'max'), daily_low_temp=('temp', 'min'))\n).head()\n\n\n\n\n\n\n\n\n\ndaily_high_temp\ndaily_low_temp\n\n\ndate_only\n\n\n\n\n\n\n2010-01-01\n43.5\n38.6\n\n\n2010-01-02\n43.8\n38.8\n\n\n2010-01-03\n44.0\n39.0\n\n\n2010-01-04\n44.2\n39.2\n\n\n2010-01-05\n44.4\n39.3\n\n\n\n\n\n\n\n\nThe second aggregation is solved similarily:\n\n(\n    seattle_temperatures\n    .assign(date_only=seattle_temperatures['date'].dt.date)\n    .groupby('date_only')\n    .agg(\n        daily_high_temp=('temp', 'max'), \n        daily_low_temp=('temp', 'min'),\n        num_measurements=('date', 'count'),\n    )\n).head()\n\n\n\n\n\n\n\n\n\ndaily_high_temp\ndaily_low_temp\nnum_measurements\n\n\ndate_only\n\n\n\n\n\n\n\n2010-01-01\n43.5\n38.6\n24\n\n\n2010-01-02\n43.8\n38.8\n24\n\n\n2010-01-03\n44.0\n39.0\n24\n\n\n2010-01-04\n44.2\n39.2\n24\n\n\n2010-01-05\n44.4\n39.3\n24\n\n\n\n\n\n\n\n\nFinally, we can also use the Grouper to avoid creating an intermediate column, if we make the date column the index:\n\n(\n    seattle_temperatures\n    .set_index('date')\n    .groupby(pd.Grouper(freq='1d'))\n    .agg(daily_high_temp=('temp', 'max'), daily_low_temp=('temp', 'min'))\n).head()\n\n\n\n\n\n\n\n\n\ndaily_high_temp\ndaily_low_temp\n\n\ndate\n\n\n\n\n\n\n2010-01-01\n43.5\n38.6\n\n\n2010-01-02\n43.8\n38.8\n\n\n2010-01-03\n44.0\n39.0\n\n\n2010-01-04\n44.2\n39.2\n\n\n2010-01-05\n44.4\n39.3"
  },
  {
    "objectID": "posts/python-tips/timing_imports.html",
    "href": "posts/python-tips/timing_imports.html",
    "title": "Timing imports",
    "section": "",
    "text": "When importing particular packages, they are importing slowly. You’d like to find why, and if there is a way you can refactor your code / your team’s code to improve import speed.\n\n\nToo much is being done at import time\nImporting should typically not “do work” (e.g. it should not load datafiles, connect to databases, instantiate drivers, etc). The biggest exception to this is probably setting up loggers, because the first caller to logging.basicConfig can change what all the other loggers can see.\nBecause logging is a cross-cutting concern, loggers are also generally made as singletons at the module level.\nThe reason that imports can be so slow is if we have a package structure like\nworld/\n│\n├── africa/\n│   ├── __init__.py\n│   └── zimbabwe.py\n│\n├── europe/\n│   ├── __init__.py\n│   ├── greece.py\n│   ├── norway.py\n│   └── spain.py\n│\n└── __init__.py\nand you write import world.europe.spain into Python, then\n\nFirst world.__init__.py gets imported (if it hasn’t been imported already)\nThen world.europe.__init__.py gets imported (if it hasn’t been imported already)\nFinally, world.europe.spain.py gets imported\n\nIf you did import world.europe.greece after importing spain, then only world.europe.greece.py would be imported on this second call (each module is only imported once).\nBut this means if your __init__.py files anywhere in the tree “do work”, then every subpackage that you import is also going to be slow. This is one of the reasons for recommending that __init__.py files are kept relatively sparse."
  },
  {
    "objectID": "posts/python-tips/timing_imports.html#what-slow-imports-usually-indicate",
    "href": "posts/python-tips/timing_imports.html#what-slow-imports-usually-indicate",
    "title": "Timing imports",
    "section": "",
    "text": "Too much is being done at import time\nImporting should typically not “do work” (e.g. it should not load datafiles, connect to databases, instantiate drivers, etc). The biggest exception to this is probably setting up loggers, because the first caller to logging.basicConfig can change what all the other loggers can see.\nBecause logging is a cross-cutting concern, loggers are also generally made as singletons at the module level.\nThe reason that imports can be so slow is if we have a package structure like\nworld/\n│\n├── africa/\n│   ├── __init__.py\n│   └── zimbabwe.py\n│\n├── europe/\n│   ├── __init__.py\n│   ├── greece.py\n│   ├── norway.py\n│   └── spain.py\n│\n└── __init__.py\nand you write import world.europe.spain into Python, then\n\nFirst world.__init__.py gets imported (if it hasn’t been imported already)\nThen world.europe.__init__.py gets imported (if it hasn’t been imported already)\nFinally, world.europe.spain.py gets imported\n\nIf you did import world.europe.greece after importing spain, then only world.europe.greece.py would be imported on this second call (each module is only imported once).\nBut this means if your __init__.py files anywhere in the tree “do work”, then every subpackage that you import is also going to be slow. This is one of the reasons for recommending that __init__.py files are kept relatively sparse."
  },
  {
    "objectID": "posts/interview_questions/dice_rolls.html",
    "href": "posts/interview_questions/dice_rolls.html",
    "title": "The dice rolls problem",
    "section": "",
    "text": "I came across this Interview Cake problem:\n\nYou have a function rand7() that generates a random integer from 1 to 7. Use it to write a function rand5() that generates a random integer from 1 to 5.\nrand7() returns each integer with equal probability. rand5() must also return each integer with equal probability.\n\nThey had a solution, but also had a callout that I thought was interesting."
  },
  {
    "objectID": "posts/interview_questions/dice_rolls.html#wrinkle",
    "href": "posts/interview_questions/dice_rolls.html#wrinkle",
    "title": "The dice rolls problem",
    "section": "Wrinkle",
    "text": "Wrinkle\nAs far as I can tell, this pretty much matches the Interview Cake solution.\nThe format of Interview Cake is to give hints, and one of the hints suggested that “Did you know you can do this with only two calls to rand5() (per loop), not 3?”\nThis seemed to be odd, and very prescriptive (i.e. guess the answer I am thinking of) instead of “let’s think of the best answer”. Let’s make the assumption that calls to rand5() are really expensive. Is it really best to minimize the number of calls in the loop?\nAs we saw above, we are expected to run the loop above 1.2 times, and rand5() is called twice per loop. Ergo, the number of expected calls is 2.4. Since this is less than 3 (the minimum we could do with 3 calls per loop is 3 calls.)\nStill, it is interesting to know what 3 calls in a loop would look like:\n\nWe would be generating a number from 0 to \\(5**3 - 1\\), or 125 possible options.\nWe have 125 % 7= 6, so the probability of rejection is 6 / 125; probability of acceptance is (125 - 6)/125\nThe expected number of runs is 125/119 ~ 1.05\n\nThe expected number of calls to rand5() would be 1.05 x 3 = 3.15\nIn general, letting - \\(n\\) be the number of times rand5() is called - \\(N=5^N\\) as the number of options - \\(m = N \\mod 7\\) as the modulus We have a rejection probability of m/N, an acceptance probability per loop of (N-m)/N, and an expected number of loop executions of N/(N-m). The expected number of calls to rand5() is \\(n N / (N-m) \\leq nN / (N - 6)\\) as 6 is the worst case sceanario for the modulus.\nWe can get a rough feel for how the number of calls to rand5() grows as you make each loop more efficent by calling more times in a loop (exchanging a lower rejection rate per loop for more work per loop). The tradeoff is roughly n(1 + 6/N), from expanding the \\(N/(N - 6) = 1/(1-6/N) = (1 + 6/N + \\ldots)\\).\nSo the interview cake solution of only using two rand5() calls was correct:\n\nIt yields an infinite worst case (but so does every solution)\nIt yields a lower minumum number of calls (2), and a lower expected number of calls (2.4) than rolling three\nIt yields a higher variance, though (not shown)"
  },
  {
    "objectID": "posts/interview_questions/dice_rolls.html#going-one-step-deeper",
    "href": "posts/interview_questions/dice_rolls.html#going-one-step-deeper",
    "title": "The dice rolls problem",
    "section": "Going one step deeper",
    "text": "Going one step deeper\nLet’s say that we rand5() really was the long poll in the tent, and we could only call it n times. What would be the probability of a timeout?\n\n\n\nn dice\nP(loops &gt; 1)\nP(loops &gt; 2)\nP(loops &gt; 3)\n\n\n\n\n2\n16.0%\n2.56%\n0.41%\n\n\n3\n4.8%\n0.23%\n0.011%\n\n\n\nSo if we had time to roll up to 6 die, we could do 3 loops of 2 dice (and have a failure rate of 0.41%), or up to two loops of a 3 roll (with a 0.23% failure rate). If we did \\(n=6\\), there would only be a 0.0064% failure rate (cut a lot more guaranteed work per roll)."
  },
  {
    "objectID": "posts/ggplot2-tips/composing/composing-plots.html",
    "href": "posts/ggplot2-tips/composing/composing-plots.html",
    "title": "Splitting plots into functions",
    "section": "",
    "text": "Problem\nWe want to be able to build up our plots in library functions.\nThis is most useful when we want to annotate a standard graph – by having a function that creates the plot, and then being able to call the function and continue adding things to the plot. Conceptually:\nplot_sales(df)\n# Shows a plot of sales\n\n\n# This doesn't work, but want to highlight a particular month\nplot_sales(df) + p9.geom_rect(\n    mapping=p9.aes(xmin='2024-03-01', xmax='2024-04-01', ymin=float(\"-inf\"), ymax=float(\"inf\"))\n)\n\n\nSolution\nMake a list instead, and add the list.\nThe above example, done correctly, is\nplot_sales(df)\n# Shows a plot of sales\n\n\n# This DOES work\nplot_sales(df) + [\n    p9.geom_rect(mapping=p9.aes(xmin='2024-03-01', xmax='2024-04-01', ymin=float(\"-inf\"), ymax=float(\"inf\")))\n]\n\n\nMinimal Example\n\nimport plotnine as p9\nfrom plotnine.data import mtcars\n\n\noriginal = (\n    p9.ggplot(mtcars, p9.aes(x='mpg', y='hp'))\n    + p9.geom_point()\n)\noriginal\n\n\n\n\n\n\n\n\nLet’s say that this is the type of graph we would normally create (e.g. we could make a function that generates it, and only takes the data frame as input).\nIf we watned to take this same graph and add to it for a particular report, we can add a list of plotnine objects, as shown below:\n\noriginal + [\n    p9.geom_smooth(method='lm', color='blue'),\n    p9.labs(title='Do more powerful cars have worse milage?',\n            subtitle='Simple linear extrapolation', \n            y='horsepower'),\n    p9.theme_bw()\n]\n\n\n\n\n\n\n\n\nYou would not trust that linear extrapolation as it is starting to head below zero. There are more sophisticated smoothing options (e.g. using 'loess' as the method instead of 'lm') but they have additional dependencies. The emphasis here is on plotting.\n\n\nMore useful example - event annotation\nThe example above was a simple example, but not particularly motivating. Let’s look at an example of a conversion rate from emails.\nThis is a graph you produce frequently, so you have a function for it\n\nimport pandas as pd\nimport plotnine as p9\n\nemails = pd.read_csv('email.csv')\nemails['date'] = pd.to_datetime(emails['date'])\nemails.head()\n\n\n\n\n\n\n\n\n\ndate\nrecipients\nclicks\nctr\n\n\n\n\n0\n2024-02-01\n99886\n1456\n0.014577\n\n\n1\n2024-02-02\n100220\n1491\n0.014877\n\n\n2\n2024-02-03\n99637\n1498\n0.015035\n\n\n3\n2024-02-04\n99344\n1543\n0.015532\n\n\n4\n2024-02-05\n100091\n1559\n0.015576\n\n\n\n\n\n\n\n\n\ndef make_ctr_plot(email_df):\n    \"\"\"Creates a plotnine plot of email conversion rates against time\n    \n    email_df contains the following columns\n        - date (as a datetime object)\n        - ctr (click through rate, in the range 0-1)\n    \"\"\"\n    assert 'date' in email_df, 'need a date column'\n    assert 'ctr' in email_df, 'need a ctr column'\n    return (\n        p9.ggplot(email_df, p9.aes(x='date', y='ctr'))\n        + p9.geom_line(color='blue', alpha=0.7)\n        + p9.scale_y_continuous(labels=lambda labs:[f\"{value:.1%}\" for value in labs])\n        + p9.scale_x_date(breaks='2 week')\n        + p9.labs(x=\"\", y=\"Click-Thru Rate\")\n        + p9.theme_bw()\n    )\n\nmake_ctr_plot(emails)\n\n\n\n\n\n\n\n\nWe see in the marketing department materials that they launched a new template on 2024-02-21, which helps explain the increase.\nIt is less clear what happened on April 1st which caused the CTR to drop! Digging in a little bit:\n\n(\n    p9.ggplot(\n        emails.drop('ctr', axis=1).melt(['date']), \n        p9.aes(x='date', y='value', color='variable')\n    )\n    + p9.geom_line()\n)\n\n\n\n\n\n\n\n\nWe see we massively expanded our audience (recipients) and clicks. It makes sense that as you expand the audience, the CTR drops. We still see incremental clicks. But it makes sense that we would want to add that information!\n\nevents = pd.DataFrame([\n    {'date': '2024-02-19', 'label': 'New email launched'},\n    {'date': '2024-04-01', 'label': 'Audience expansion via XYZ.com'}\n])\nevents['date'] = pd.to_datetime(events['date'])\nevents\n\n\n\n\n\n\n\n\n\ndate\nlabel\n\n\n\n\n0\n2024-02-19\nNew email launched\n\n\n1\n2024-04-01\nAudience expansion via XYZ.com\n\n\n\n\n\n\n\n\n\nmake_ctr_plot(emails) + [\n    p9.geom_vline(mapping=p9.aes(xintercept='date'), data=events, linetype='dashed'),\n    p9.geom_text(mapping=p9.aes(x='date', y=0.0165, label='label'), data=events, angle=90, nudge_x=-2),\n    p9.geom_rect(mapping=p9.aes(\n        xmin=pd.to_datetime('2024-04-01'), xmax=emails['date'].max(), ymin=float(\"-inf\"), ymax=float(\"inf\")\n    ), alpha=0.005, fill='cyan'),\n    p9.annotate('text', x='2024-04-15', y=0.0183, label=\"Expanded TAM\")\n]\n\n\n\n\n\n\n\n\nThis way we can annotate graphs to highlight special events, without having to copy all the plotting code."
  },
  {
    "objectID": "posts/ggplot2-tips/dropping-categories.html",
    "href": "posts/ggplot2-tips/dropping-categories.html",
    "title": "Working with categories",
    "section": "",
    "text": "When creating a categorical datatype, part of the datatype is knowing all the different possibilities of a category. They will show up by default in plotnine, even if those categories don’t appear in the data. This can be cumbersome to deal with.\nWe will give an example using the mtcars dataset that shows how this can occur."
  },
  {
    "objectID": "posts/ggplot2-tips/dropping-categories.html#loading-data-and-showing-the-problem",
    "href": "posts/ggplot2-tips/dropping-categories.html#loading-data-and-showing-the-problem",
    "title": "Working with categories",
    "section": "Loading data and showing the problem",
    "text": "Loading data and showing the problem\nLet’s start by loading the miles-per-gallon example from plotnine, and converting the manufacturer to a categorical datatype. This might be done to save memory, or may come from some other transformation (e.g. pd.cut).\nWhen plotting all categories, there isn’t an issue.\n\nimport plotnine as p9\nimport pandas as pd\nfrom plotnine.data import mpg\n\nmpg['manufacturer'] = mpg['manufacturer'].astype('category')\nmpg.head()\n\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\n\n\n\n\n0\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\n1\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\n2\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\n3\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\n4\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n\n\n\n\n\n\n\n\n\n# Plot all categories\n(\n    p9.ggplot(mpg, mapping=p9.aes(x='hwy', y='cty', color=\"manufacturer\"))\n    + p9.geom_point()\n)\n\n\n\n\n\n\n\n\nLet’s restrict to some non-US manufacturers:\n\nto_track = [\n    'honda',\n    'hyundai',\n    'nissan',\n    'toyota',\n    'voltsswagen'\n]\nsubset = mpg[mpg['manufacturer'].cat.as_ordered().isin(to_track)].copy()\n\n\n# Plot just these categories\n(\n    p9.ggplot(subset, mapping=p9.aes(x='hwy', y='cty', color=\"manufacturer\"))\n    + p9.geom_point()\n)\n\n\n\n\n\n\n\n\nNote that the legend includes all the unused categories! We would like to eliminate this, especially if it allows more divergent colors.\n\nto_track = [\n    'honda',\n    'hyundai',\n    'nissan',\n    'toyota',\n    'voltsswagen'\n]\nsubset = mpg[mpg['manufacturer'].cat.as_ordered().isin(to_track)].copy()\n# This is the magic line\nsubset['manufacturer'] = subset['manufacturer'].cat.remove_unused_categories()\n\n\n# Plot just these categories\n(\n    p9.ggplot(subset, mapping=p9.aes(x='hwy', y='cty', color=\"manufacturer\"))\n    + p9.geom_point()\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/making-percentage-labels.html",
    "href": "posts/ggplot2-tips/making-percentage-labels.html",
    "title": "Changing labels to percentages",
    "section": "",
    "text": "Problem\nWe want to apply custom formatting to the labels on an axis.\nThe bad news is that we don’t have a shortcut for common formatters (e.g. percentages), but the good news is that we have a method that allows us a lot of flexibility.\n\n\nSolution\nWe need a function that takes an iterable of labels we have by default, and outputs an iterable of formatted labels (in the same order). For example\ndef percent_formatter(list_of_labels: list[str]) -&gt; list[str]:\n    return [f\"{label:.0%}\" for label in list_of_labels]\nWe can then pass this into one of the scale functions as the labels parameter, for example\np9.scale_x_continuous(labels=percent_formatter)\nBecause the formatters are frequently pretty simple, they are often implemented as lambda functions, rather than standalone functions.\n\n\nExample\nOur example is going to be pretty straightforward – looking at the rating distribution for a single product on Amazon.\n\nimport plotnine as p9\nimport pandas as pd\n\n\nlens_review = pd.DataFrame([\n    {'stars': 5, 'num_customers': 121},\n    {'stars': 4, 'num_customers': 6},\n    {'stars': 3, 'num_customers': 0},\n    {'stars': 2, 'num_customers': 1},\n    {'stars': 1, 'num_customers': 3}\n])\n\nlens_review['frac_customers'] = lens_review['num_customers'] / lens_review['num_customers'].sum()\n\n\n(\n    p9.ggplot(lens_review, p9.aes(x='stars', y='frac_customers'))\n    + p9.geom_bar(stat='identity')\n)\n\n\n\n\n\n\n\n\nLet’s make the y axis formatted as percentages\n\n(\n    p9.ggplot(lens_review, p9.aes(x='stars', y='frac_customers'))\n    + p9.geom_bar(stat='identity')\n    + p9.scale_y_continuous(labels=lambda labels: [f\"{label:.0%}\" for label in labels])\n)\n\n\n\n\n\n\n\n\nWe can also make it more similar to the Amazon reviews by flipping the axes, and removing some of the distracting background\n\n(\n    p9.ggplot(lens_review, p9.aes(x='stars', y='frac_customers'))\n    + p9.geom_bar(stat='identity', width=0.8, fill='orange')\n    + p9.scale_y_continuous(labels=lambda labels: [f\"{label:.0%}\" for label in labels])\n    + p9.scale_x_continuous(labels=lambda labels: [f\"1 star\" if label==1 else f\"{label:.0f} stars\" for label in labels])\n    + p9.coord_flip()\n    + p9.theme_bw()\n    + p9.theme(\n        panel_border = p9.element_blank(),\n        panel_grid = p9.element_blank(),\n    )\n    + p9.labs(x=\"\", y=\"\", title=\"Amazon ratings for lens\")\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/datetime-axes.html",
    "href": "posts/ggplot2-tips/datetime-axes.html",
    "title": "Dealing with dates on axis",
    "section": "",
    "text": "Dates are often cumbersome as their format is really long. This post gives a few techniques for formatting that dates a little nicer."
  },
  {
    "objectID": "posts/ggplot2-tips/datetime-axes.html#angled-text",
    "href": "posts/ggplot2-tips/datetime-axes.html#angled-text",
    "title": "Dealing with dates on axis",
    "section": "Angled text",
    "text": "Angled text\n\nUse p9.theme(axis_text_x=p9.element_text(angle=xx))\n\n\n(\n    p9.ggplot(weather_recent, p9.aes(x=\"DATE\")) \n    + p9.geom_line(mapping=p9.aes(y=\"TMAX\"), color=\"red\")\n    + p9.geom_line(mapping=p9.aes(y=\"TMIN\"), color=\"blue\")\n    + p9.labs(\n        title=\"Daily Highs and Lows in Seattle\", \n        y=\"Temperature (F)\", \n        x=\"\"\n    )\n    + p9.scale_x_date(breaks='1 month')\n    + p9.theme(axis_text_x=p9.element_text(angle=60))\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/datetime-axes.html#format-the-date-e.g.-yyyy-mm",
    "href": "posts/ggplot2-tips/datetime-axes.html#format-the-date-e.g.-yyyy-mm",
    "title": "Dealing with dates on axis",
    "section": "Format the date (e.g. YYYY-MM)",
    "text": "Format the date (e.g. YYYY-MM)\n\nUse scale_x_date(date_labels=date_format_string)\n\ndate_format_string defaults to ISO standard: \"%Y-%m-%d\"\n\n\n\n(\n    p9.ggplot(weather_recent, p9.aes(x=\"DATE\")) \n    + p9.geom_line(mapping=p9.aes(y=\"TMAX\"), color=\"red\")\n    + p9.geom_line(mapping=p9.aes(y=\"TMIN\"), color=\"blue\")\n    + p9.labs(\n        title=\"Daily Highs and Lows in Seattle\", \n        y=\"Temperature (F)\", \n        x=\"\"\n    )\n    + p9.scale_x_date(breaks='1 month', date_labels=\"%Y-%m\")\n    + p9.theme(axis_text_x=p9.element_text(angle=60, hjust=1))\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/datetime-axes.html#manually-set-the-labels",
    "href": "posts/ggplot2-tips/datetime-axes.html#manually-set-the-labels",
    "title": "Dealing with dates on axis",
    "section": "Manually set the labels",
    "text": "Manually set the labels\n\nUse the labels argument\nCan use an explicit list, but you give up plotnine determining the breaks for you\nCan use a lambda to take the Timestamp object, and transform it to a string\n\nHere is a way we can do the previous version (setting date_format to \"%Y-%m\").\n\n(\n    p9.ggplot(weather_recent, p9.aes(x=\"DATE\")) \n    + p9.geom_line(mapping=p9.aes(y=\"TMAX\"), color=\"red\")\n    + p9.geom_line(mapping=p9.aes(y=\"TMIN\"), color=\"blue\")\n    + p9.labs(\n        title=\"Daily Highs and Lows in Seattle\", \n        y=\"Temperature (F)\", \n        x=\"\"\n    )\n    + p9.scale_x_date(breaks='1 month', labels=lambda d: [entry.strftime(\"%Y-%m\") for entry in d])\n    + p9.theme(axis_text_x=p9.element_text(angle=60, hjust=1))\n)\n\n\n\n\n\n\n\n\nLet’s do a different version, which only shows the year in January\n\nlabel_func = lambda d: [date.strftime(\"%Y\") if date.month==1 else date.strftime(\"%m\") for date in d]\n\n(\n    p9.ggplot(weather_recent, p9.aes(x=\"DATE\")) \n    + p9.geom_line(mapping=p9.aes(y=\"TMAX\"), color=\"red\")\n    + p9.geom_line(mapping=p9.aes(y=\"TMIN\"), color=\"blue\")\n    + p9.labs(\n        title=\"Daily Highs and Lows in Seattle\", \n        y=\"Temperature (F)\", \n        x=\"\"\n    )\n    + p9.scale_x_date(breaks='1 month', labels=label_func)\n    + p9.theme(axis_text_x=p9.element_text(angle=60))\n)\n\n\n\n\n\n\n\n\nEven better:\n\nlabel_func = lambda d: [date.strftime(\"%Y\\n%b\") if date.month==1 else date.strftime(\"%b\") for date in d]\n\n(\n    p9.ggplot(weather_recent, p9.aes(x=\"DATE\")) \n    + p9.geom_line(mapping=p9.aes(y=\"TMAX\"), color=\"red\")\n    + p9.geom_line(mapping=p9.aes(y=\"TMIN\"), color=\"blue\")\n    + p9.labs(\n        title=\"Daily Highs and Lows in Seattle\", \n        y=\"Temperature (F)\", \n        x=\"\"\n    )\n    + p9.scale_x_date(breaks='2 month', labels=label_func)\n    + p9.theme()\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/datetime-axes.html#make-the-plot-wider",
    "href": "posts/ggplot2-tips/datetime-axes.html#make-the-plot-wider",
    "title": "Dealing with dates on axis",
    "section": "Make the plot wider",
    "text": "Make the plot wider\n\nUse the figure_size=(h, w) argument in p9.theme\n\n\n(\n    p9.ggplot(weather_recent, p9.aes(x=\"DATE\")) \n    + p9.geom_line(mapping=p9.aes(y=\"TMAX\"), color=\"red\")\n    + p9.geom_line(mapping=p9.aes(y=\"TMIN\"), color=\"blue\")\n    + p9.labs(\n        title=\"Daily Highs and Lows in Seattle\", \n        y=\"Temperature (F)\", \n        x=\"\"\n    )\n    + p9.scale_x_date(breaks='1 month', labels=lambda d: [entry.strftime(\"%Y-%m\") for entry in d])\n    + p9.theme(axis_text_x=p9.element_text(angle=60, hjust=1), figure_size=(10, 6))\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/making-categorical-variables.html",
    "href": "posts/ggplot2-tips/making-categorical-variables.html",
    "title": "Forcing Variables to be Categorical",
    "section": "",
    "text": "Problem\nPlotnine assumes that numeric variables are continuous, rather than discrete. When plotting, some attributes require discrete / categorical variables (e.g. shape). Some variables can support either continuous or discrete features (e.g. size, colour), but the legends can be clearer for discrete variables.\nExamples of where a numeric feature is categorical:\n\nCell ids in A/B tests (e.g. cell 1 is control, cell 2 is treatment)\nSKU ids (SKU 542354 and SKU 542355 should not be considered “close”, they are two arbitrary numbers that label the products)\n\nThe example that I will use here is the number of cylinders in a car in the mtcars dataset.\n\n\nSolution\nUse \"factor(variable_name)\" in the athestics, rather than just \"variable_name\".\n\n\nExample\n\nimport plotnine as p9\nfrom plotnine.data import mtcars \n\n# Example without factor, cylinders are discrete but the colour scale is \n# continuous \n(\n    p9.ggplot(mtcars, p9.aes(x='mpg', y='wt', color='cyl'))\n    + p9.geom_point()\n    + p9.theme_bw()\n)\n\n\n\n\nAn example that doesn’t use factor, so the color shows as a gradient\n\n\n\n\nUsing \"factor(cyl)\" to force the integer number of cylinders to be seen as discrete, even though it is a numeric variable.\n\n(\n    p9.ggplot(mtcars, p9.aes(x='mpg', y='wt', color='factor(cyl)'))\n    + p9.geom_point()\n    + p9.theme_bw()\n)\n\n\n\n\nAn example using factor; colours show discretely and are easier to identify\n\n\n\n\n\n(\n    p9.ggplot(mtcars, p9.aes(x='mpg', y='wt', color='factor(cyl)'))\n    + p9.geom_point()\n    + p9.theme_bw()\n    + p9.scale_color_discrete(name=\"Cylinders\")\n)\n\n\n\n\nAn example using factor and renaming the colour scale to something readable"
  },
  {
    "objectID": "posts/ggplot2-tips/gallery/gallery_annotated_area.html",
    "href": "posts/ggplot2-tips/gallery/gallery_annotated_area.html",
    "title": "Annotated area charts with plotnine",
    "section": "",
    "text": "Beautiful plot!\nThis is an amazing plot showing annotations and area plots. It was part of the 2024 Plotnine Gallery Contest.\nOriginal source for the post is here.\nImage from the original blog post is\n\n\n\nchart"
  },
  {
    "objectID": "posts/ggplot2-tips/waterfall/gallery-waterfall.html",
    "href": "posts/ggplot2-tips/waterfall/gallery-waterfall.html",
    "title": "Making a waterfall chart",
    "section": "",
    "text": "This makes a plot of the milk cow cost per head over year, but makes it a waterfall plot. This is taken from the ggplot flipbook, who in turn took their data from the #TidyTuesday project\n\nimport pandas as pd\nimport plotnine as p9\n\n# Original data available at\n# https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-01-29/milkcow_facts.csv\ncows = pd.read_csv(\"milkcow_facts.csv\").query('year&gt;=2004')\n\n\ncows.head()\n\n\n\n\n\n\n\n\n\nyear\navg_milk_cow_number\nmilk_per_cow\nmilk_production_lbs\navg_price_milk\ndairy_ration\nmilk_feed_price_ratio\nmilk_cow_cost_per_animal\nmilk_volume_to_buy_cow_in_lbs\nalfalfa_hay_price\nslaughter_cow_price\n\n\n\n\n24\n2004.0\n9010000.0\n18960\n1.708320e+11\n0.161\n0.052007\n3.10\n1580\n9813.664596\n95.133333\n0.5266\n\n\n25\n2005.0\n9050000.0\n19550\n1.769310e+11\n0.151\n0.046825\n3.24\n1770\n11721.854305\n102.525000\n0.5394\n\n\n26\n2006.0\n9137000.0\n19895\n1.817820e+11\n0.129\n0.050371\n2.57\n1730\n13410.852713\n107.708333\n0.4908\n\n\n27\n2007.0\n9189000.0\n20204\n1.856540e+11\n0.191\n0.067958\n2.80\n1830\n9581.151832\n130.583333\n0.4951\n\n\n28\n2008.0\n9314000.0\n20397\n1.899780e+11\n0.183\n0.091663\n2.01\n1950\n10655.737705\n161.333333\n0.5144\n\n\n\n\n\n\n\n\n\ncows['milk_cow_cost_per_animal_lag'] = cows['milk_cow_cost_per_animal'].shift(1)\ncows['percent_change'] = cows['milk_cow_cost_per_animal']/cows['milk_cow_cost_per_animal_lag'] - 1\ncows\n\n\n\n\n\n\n\n\n\nyear\navg_milk_cow_number\nmilk_per_cow\nmilk_production_lbs\navg_price_milk\ndairy_ration\nmilk_feed_price_ratio\nmilk_cow_cost_per_animal\nmilk_volume_to_buy_cow_in_lbs\nalfalfa_hay_price\nslaughter_cow_price\nmilk_cow_cost_per_animal_lag\npercent_change\n\n\n\n\n24\n2004.0\n9010000.0\n18960\n1.708320e+11\n0.161\n0.052007\n3.10\n1580\n9813.664596\n95.133333\n0.526600\nNaN\nNaN\n\n\n25\n2005.0\n9050000.0\n19550\n1.769310e+11\n0.151\n0.046825\n3.24\n1770\n11721.854305\n102.525000\n0.539400\n1580.0\n0.120253\n\n\n26\n2006.0\n9137000.0\n19895\n1.817820e+11\n0.129\n0.050371\n2.57\n1730\n13410.852713\n107.708333\n0.490800\n1770.0\n-0.022599\n\n\n27\n2007.0\n9189000.0\n20204\n1.856540e+11\n0.191\n0.067958\n2.80\n1830\n9581.151832\n130.583333\n0.495100\n1730.0\n0.057803\n\n\n28\n2008.0\n9314000.0\n20397\n1.899780e+11\n0.183\n0.091663\n2.01\n1950\n10655.737705\n161.333333\n0.514400\n1830.0\n0.065574\n\n\n29\n2009.0\n9202000.0\n20561\n1.892020e+11\n0.128\n0.072685\n1.78\n1390\n10859.375000\n122.916667\n0.443767\n1950.0\n-0.287179\n\n\n30\n2010.0\n9123000.0\n21142\n1.928770e+11\n0.163\n0.072030\n2.26\n1330\n8159.509202\n116.416667\n0.561000\n1390.0\n-0.043165\n\n\n31\n2011.0\n9199000.0\n21334\n1.962550e+11\n0.201\n0.107560\n1.90\n1420\n7064.676617\n176.083333\n0.683000\n1330.0\n0.067669\n\n\n32\n2012.0\n9237000.0\n21722\n2.006420e+11\n0.185\n0.121500\n1.52\n1430\n7729.729730\n206.083333\n0.777100\n1420.0\n0.007042\n\n\n33\n2013.0\n9224000.0\n21816\n2.012310e+11\n0.201\n0.117092\n1.75\n1380\n6865.671642\n205.830000\n0.775600\n1430.0\n-0.034965\n\n\n34\n2014.0\n9257000.0\n22259\n2.060540e+11\n0.240\n0.095100\n2.54\n1830\n7625.000000\n200.250000\n1.020400\n1380.0\n0.326087\n\n\n\n\n\n\n\n\nStart with a simple barplot to visualize the data\n\n(\n    p9.ggplot(cows)\n    + p9.aes(x='year', y='milk_cow_cost_per_animal')\n    + p9.geom_bar(stat='identity')\n    + p9.scale_x_continuous(breaks=cows.year.tolist())\n)\n\n\n\n\n\n\n\n\nNow transform it into a waterfall chart:\n\n(\n    p9.ggplot(cows)\n    # Main waterfall\n    + p9.aes(\n        xmin='year-0.4', xmax='year + 0.4', \n        ymax='milk_cow_cost_per_animal', ymin='milk_cow_cost_per_animal_lag')\n    + p9.geom_rect(fill='blue', alpha=0.3)\n    + p9.scale_x_continuous(breaks=cows.year.tolist())\n    + p9.geom_col(\n        data=cows[cows.year==2004], \n        mapping=p9.aes(x='year', y='milk_cow_cost_per_animal'), \n        fill='grey'\n    )\n    \n    # Dashed lines between\n    + p9.geom_segment(\n        data = cows[cows.year &lt; 2014], \n        mapping=p9.aes(x='year+0.4', xend='year+0.6', y='milk_cow_cost_per_animal', yend='milk_cow_cost_per_animal'), \n        linetype = \"dashed\", color = \"grey\")\n    + p9.theme_bw(base_family = \"Times\") \n    \n    # The % annotations\n    + p9.geom_text(\n        data=cows.dropna(),\n        mapping = p9.aes(\n            y = 'milk_cow_cost_per_animal', \n            x = 'year', \n            color = (cows.dropna().percent_change &gt; 0),\n            label=[f'{p:.0%}' for p in cows.dropna().percent_change]),  \n        size = 10, \n        nudge_y = [45 if p &gt; 0 else -45 for p in cows.dropna().percent_change],\n        show_legend = False)\n    + p9.scale_color_manual(values = (\"red\", \"grey\"))\n\n    # titles\n    + p9.labs(\n        x=\"\", \n        y=\"Cost per Cow (USD)\", \n        title = \"Cost of milk cows in the United States\", \n        subtitle = \"Per animal cost, 2004-2014\"\n    )\n)\n\n/Users/damienmartin/anaconda3/envs/blog/lib/python3.12/site-packages/plotnine/layer.py:364: PlotnineWarning: geom_rect : Removed 1 rows containing missing values."
  },
  {
    "objectID": "posts/ggplot2-tips/histograms.html",
    "href": "posts/ggplot2-tips/histograms.html",
    "title": "Histograms",
    "section": "",
    "text": "For many plotting methods, each row leads to a graphical element on the page. The simplest example is geom_point, where there is a 1:1 correspondence between rows and points on the plot.\nFor histograms, they preprocess the data. By default, they count the number of rows belonging to a bin, and then that group of rows corresponds to a bar. We can control transformations on this data (e.g. making a relative histogram), but need to know some specialized commands to do so."
  },
  {
    "objectID": "posts/ggplot2-tips/histograms.html#eliminate-stacking-solution-1-faceting",
    "href": "posts/ggplot2-tips/histograms.html#eliminate-stacking-solution-1-faceting",
    "title": "Histograms",
    "section": "Eliminate stacking: solution 1 – faceting",
    "text": "Eliminate stacking: solution 1 – faceting\nWe can use a facet plot to eliminate the stacking\n\n(\n    p9.ggplot(penguins.dropna(), p9.aes(x='body_mass_g', fill='sex'))\n    + p9.geom_histogram(binwidth=200)\n    + p9.labs(title='Distribution of penguin weights', subtitle=\"Facet to eliminate stacking\", x='mass (g)')\n    + p9.facet_wrap('sex')\n)\n\n\n\n\n\n\n\n\nThis has the downside that direct comparisons are more difficult e.g. there is a dip in weights, presumably from different species or islands. This dip is at 4 kg for female penguins, and around 4.75 kg for male penguins. It isn’t immediately obvious that these dips are in different places."
  },
  {
    "objectID": "posts/ggplot2-tips/histograms.html#eliminate-stacking-solution-2-overlay",
    "href": "posts/ggplot2-tips/histograms.html#eliminate-stacking-solution-2-overlay",
    "title": "Histograms",
    "section": "Eliminate stacking: solution 2 – overlay",
    "text": "Eliminate stacking: solution 2 – overlay\nWe can use the position=\"identity\" argument to geom_histogram to overlay the graph. By default, the argument to position is \"stack\".\n\n(\n    p9.ggplot(penguins.dropna(), p9.aes(x='body_mass_g', fill='sex'))\n    + p9.geom_histogram(binwidth=200, alpha=0.5, position=\"identity\")\n    + p9.labs(title='Distribution of penguin weights', subtitle=\"Use position='identity' to overlay the graphs\", x='mass (g)')\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/histograms.html#making-relative-histograms",
    "href": "posts/ggplot2-tips/histograms.html#making-relative-histograms",
    "title": "Histograms",
    "section": "Making relative histograms",
    "text": "Making relative histograms\nIf we just want to compare the distributions, we can pass in y='stat(density)' instead. The default for a histogram is 'stat(count)'. In R’s ggplot, this would be written as ..density.. instead.\n\n(\n    p9.ggplot(penguins.dropna(), p9.aes(x='body_mass_g', y='stat(density)', fill='sex'))\n    + p9.geom_histogram(binwidth=200, alpha=0.5, position=\"identity\")\n    + p9.labs(title='Distribution of penguin weights', subtitle=\"Use position='identity' to overlay the graphs\", x='mass (g)')\n    + p9.scale_y_continuous(labels=lambda v: [f\"{vv:.3%}\" for vv in v])\n)\n\n\n\n\n\n\n\n\nThis doesn’t make much of a difference, as the total gender numbers were pretty even:\n\npenguins.dropna().sex.value_counts()\n\nsex\nmale      168\nfemale    165\nName: count, dtype: int64\n\n\nWe can see a more dramatic result if we partition by species instead:\n\n(\n    p9.ggplot(penguins.dropna(), p9.aes(x='body_mass_g', fill='species'))\n    + p9.geom_histogram(binwidth=200, alpha=0.5, position=\"identity\")\n    + p9.labs(title='Distribution of penguin weights', subtitle=\"Count graph\", x='mass (g)')\n)\n\n\n\n\n\n\n\n\n\n(\n    p9.ggplot(penguins.dropna(), p9.aes(x='body_mass_g', y='stat(density)', fill='species'))\n    + p9.geom_histogram(binwidth=200, alpha=0.5, position=\"identity\")\n    + p9.labs(title='Distribution of penguin weights', subtitle=\"Relative Graph\", x='mass (g)')\n    + p9.scale_y_continuous(labels=lambda v: [f\"{vv:.3%}\" for vv in v])\n)\n\n\n\n\n\n\n\n\nIf you were intereseted in the question, “at a given mass, what % of penguins are which species?” you could use position=\"fill\" instead. This scales each bin from 0 to 100%, and tells us how much of the data is in each bin.\n\n(\n    p9.ggplot(penguins.dropna(), p9.aes(x='body_mass_g', y='stat(count)', fill='species'))\n    + p9.geom_histogram(binwidth=200, alpha=0.5, position=\"fill\")\n    + p9.labs(title='Distribution of penguin weights', subtitle=\"Filled Graph\", x='mass (g)')\n    + p9.scale_y_continuous(labels=lambda v: [f\"{vv:.0%}\" for vv in v])\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/histograms.html#putting-counts-above-the-bars",
    "href": "posts/ggplot2-tips/histograms.html#putting-counts-above-the-bars",
    "title": "Histograms",
    "section": "Putting counts above the bars",
    "text": "Putting counts above the bars\nThis is tricker than regular labels, because we are trying to access the result of a computation (if we already have the counts, we could use geom_label to add them automatically). Below is a way of adding when using the identity position using stat_bin. Example inspired from stack overflow.\n\n(\n    p9.ggplot(penguins.dropna(), p9.aes(x='body_mass_g', fill=\"species\"))\n    + p9.geom_histogram(binwidth=200, alpha=0.5, position=\"identity\")\n    + p9.labs(title='Distribution of penguin weights', subtitle=\"Count graph\", x='mass (g)')\n    + p9.stat_bin(binwidth=200, geom=\"text\", mapping=p9.aes(label=\"..count..\", color='species'), position=p9.position_identity(), nudge_y=1)\n)\n\n\n\n\n\n\n\n\nIf we have the slightly simpler use-case of only one species, the position_stack provides a slightly nicer interface. Note that vjust is as a fraction of the bar height (so 1 places right on top of the bar). This defaults at 0.5 (inside the bar). You can still use nudge_y if you want the values to be a constant offset upwards.\n\n(\n    p9.ggplot(penguins.dropna(), p9.aes(x='body_mass_g'))\n    + p9.geom_histogram(binwidth=200, alpha=0.5)\n    + p9.labs(title='Distribution of penguin weights', subtitle=\"Count graph\", x='mass (g)')\n    + p9.stat_bin(binwidth=200, geom=\"text\", mapping=p9.aes(label=\"..count..\"), position=p9.position_stack(vjust=1.1))\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/histograms.html#building-histograms-with-pre-collected-data",
    "href": "posts/ggplot2-tips/histograms.html#building-histograms-with-pre-collected-data",
    "title": "Histograms",
    "section": "Building histograms with pre-collected data",
    "text": "Building histograms with pre-collected data\nWe can imagine that we pull data already binned from some external source (e.g. a database), and we want to plot that, as we only need the summary statistics\n\nprebinned = ( \n    penguins\n    .assign(mass_bin=lambda x: x.body_mass_g//100)\n    .assign(individuals=1)\n    .groupby('mass_bin')\n    ['individuals']\n    .count()\n    .reset_index()\n)\n# our simulation of what we pull\nprebinned.head()\n\n\n\n\n\n\n\n\n\nmass_bin\nindividuals\n\n\n\n\n0\n27.0\n1\n\n\n1\n28.0\n2\n\n\n2\n29.0\n6\n\n\n3\n30.0\n7\n\n\n4\n31.0\n7\n\n\n\n\n\n\n\n\n\n(\n    p9.ggplot(prebinned, p9.aes(x='mass_bin'))\n    + p9.geom_line(mapping=p9.aes(y='individuals'))\n)\n\n\n\n\n\n\n\n\nWe can turn this into a bar chart / histogram in the following way:\n\n(\n    p9.ggplot(prebinned, p9.aes(x='mass_bin', y='individuals'))\n    + p9.geom_bar(stat=\"identity\")\n)"
  },
  {
    "objectID": "posts/presto/dates.html",
    "href": "posts/presto/dates.html",
    "title": "Dates in presto",
    "section": "",
    "text": "Common date problem format problems\nConvert a string to a datetime object\nSELECT DATE_PARSE('2020-06-10', '%Y-%m-%d')\nConvert one date format to another\nSELECT DATE_FORMAT(DATE_PARSE('2020-06-10', '%Y-%m-%d'), '%Y%m%d')\n\n\nCreate a date spine\nWant to do weekly smoothing, but worried that some days might be missing from the date range? Here is a way to generate a date range that you can left join to.\nSELECT\n    *\nFROM UNNEST(\n        SEQUENCE(\n            FROM_ISO8601_DATE('2010-01-20'),\n            FROM_ISO8601_DATE('2010-01-24'),\n            INTERVAL '1' DAY\n        )\n    )\n AS t1(date_array)\n\n\nDate Differences\nSELECT DATE_DIFF('day', DATE('2024-09-16'), DATE('2024-09-20'))\nThis returns 4. Things that can catch you:\n\nHas to be day (not days) for the unit\nThe earlier date goes first (unlike subtraction, where we would start with the later date)"
  },
  {
    "objectID": "presto-series.html",
    "href": "presto-series.html",
    "title": "Series: Presto tips",
    "section": "",
    "text": "Dates in presto\n\n\n\n\n\nEverytime I go to do dates in presto I have to look up how to do conversions.\n\n\n\n\n\nMay 7, 2024\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/ggplot2-tips/categories-with-ordering.html",
    "href": "posts/ggplot2-tips/categories-with-ordering.html",
    "title": "Ordering categories",
    "section": "",
    "text": "Problem\nThe default way that non-numeric data is ordered in plot-nine is lexigraphical, rather than by meaningful order.\nFor example, if you are working for Toyota, you might want the Toyota cars to be listed at the top of the legends. By default, it will to listed after Audi, Chevy, Ford, Mitsubishi, and Tesla.\n\n\nSolution\nReplace the strings with a categorical data type. For the car example above:\ndf['manufacturer'] = pd.Categorical(\n    df['manufacturer'],\n    categories = [\"Toyota\", ....],  # Order the others how you would like\n    ordered=True\n)\nThen legends will respect the order of the categories, rather than the lexigraphical ordering.\n\n\nExample\nLet’s look at the monthly meat production built into plotnine. We will first convert it from wide form to long form.\n\nimport plotnine as p9\nfrom plotnine.data import meat as meat_source\nimport pandas as pd\n\nmeat = meat_source.melt('date', var_name='meat_type', value_name='weight_M_lbs').dropna()\nmeat\n\n\n\n\n\n\n\n\n\ndate\nmeat_type\nweight_M_lbs\n\n\n\n\n0\n1944-01-01\nbeef\n751.0\n\n\n1\n1944-02-01\nbeef\n713.0\n\n\n2\n1944-03-01\nbeef\n741.0\n\n\n3\n1944-04-01\nbeef\n650.0\n\n\n4\n1944-05-01\nbeef\n681.0\n\n\n...\n...\n...\n...\n\n\n6715\n2023-08-01\nturkey\n489.2\n\n\n6716\n2023-09-01\nturkey\n431.2\n\n\n6717\n2023-10-01\nturkey\n500.3\n\n\n6718\n2023-11-01\nturkey\n449.2\n\n\n6719\n2023-12-01\nturkey\n393.0\n\n\n\n\n5652 rows × 3 columns\n\n\n\n\nLet’s start by making a simple plot:\n\n(\n    p9.ggplot(meat, p9.aes(x='date', y='weight_M_lbs', color='meat_type'))\n    + p9.geom_line()\n    + p9.labs(title=\"US Meat Production\", x=\"\", y=\"Weight (Millions lbs)\")\n)\n\n\n\n\n\n\n\n\nSome things we would like to clean up:\n\n\n\n\n\n\n\nProblem\nSolution\n\n\n\n\nSeasonal fluctuations\nUse a rolling average\n\n\nTop meats (broilers, beed, pork, turkey) are on positions 1, 2, 4, 5 on list\nUse categoricals instead\n\n\nOther_chicken is similar to color to lamb_and_mutton\nCombine less popular categories\n\n\n\nThe rolling average is a relatively easy one:\n\nmeat['weight_rolling'] = (\n    meat\n    .sort_values('date')\n    .groupby('meat_type')['weight_M_lbs']\n    .transform(lambda x: x.rolling(12).mean())\n)\n\n\n(\n    p9.ggplot(meat, p9.aes(x='date', y='weight_rolling', color='meat_type'))\n    + p9.geom_line()\n    + p9.labs(\n        title=\"US Meat Production\", \n        subtitle=\"12-month moving average\", \n        x=\"\", \n        y=\"Weight (Millions lbs)\"\n    )\n)\n\n/Users/damienmartin/anaconda3/envs/blog/lib/python3.12/site-packages/plotnine/geoms/geom_path.py:100: PlotnineWarning: geom_path: Removed 11 rows containing missing values.\n\n\n\n\n\n\n\n\n\nNow let’s lump the other categories below the top 5 into “other”. We will redo our moving averages after combining (otherwise we can get weird zero issues as we combine NA and ordinary values)\n\nmeat['meat_type'] = (\n    meat['meat_type']\n    .apply(lambda meat_type: meat_type if meat_type in {'beef', 'pork', 'broilers', 'turkey'} else 'other')\n)\nmeat_simple = meat.groupby(['date', 'meat_type'])['weight_M_lbs'].sum().reset_index()\nmeat_simple['weight_rolling'] = (\n    meat_simple\n    .sort_values('date')\n    .groupby('meat_type')['weight_M_lbs']\n    .transform(lambda x: x.rolling(12).mean())\n)\nmeat_simple = meat_simple.dropna()\n\nThis simplifies the number of lines we have to deal with\n\n(\n    p9.ggplot(meat_simple, p9.aes(x='date', y='weight_rolling', color='meat_type'))\n    + p9.geom_line()\n    + p9.labs(\n        title=\"US Meat Production\", \n        subtitle=\"12-month moving average\", \n        x=\"\", \n        y=\"Weight (Millions lbs)\"\n    )\n)\n\n\n\n\n\n\n\n\nNow let’s deal with the order on the legend: “other” should be at the bottom of the list. Let’s order by the popularity in 2000: * Broilers * Beef * Pork * Turkey * Other\nLet’s do it manually first:\n\nmeat_simple['meat_type'] = pd.Categorical(\n    meat_simple['meat_type'], \n    categories=['broilers', 'beef', 'pork', 'turkey','other'], \n    ordered=True\n)\n\nUsing the same code as before, we now have\n\n(\n    p9.ggplot(meat_simple, p9.aes(x='date', y='weight_rolling', color='meat_type'))\n    + p9.geom_line()\n    + p9.labs(\n        title=\"US Meat Production\", \n        subtitle=\"12-month moving average\", \n        x=\"\", \n        y=\"Weight (Millions lbs)\"\n    )\n)\n\n\n\n\n\n\n\n\nWe can fix the colors in just a moment. Let’s address the conversion, which was done manually here. A less error-prone way of doing it would be to do exactly as we did – decide that the order was going to be determined by the values on a particular date. We can then do the order programmatically (this was a tip taken from this lovely plot)\n\n# reset\nmeat_simple['meat_type'] = meat_simple['meat_type'].astype('str')\n\nordering = meat_simple[meat_simple['date'] == '2000-01-01'].sort_values(\n    by='weight_rolling', ascending=False)['meat_type']\nmeat_simple['meat_type'] = pd.Categorical(\n    meat_simple['meat_type'],\n    categories=ordering,\n    ordered=True\n)\n\nNext, let’s pick a better palette, and rename the categories. We will also - move the legend inside the plot - eliminate the extra space around along the axes - eliminate the background shading\n\n(\n    p9.ggplot(meat_simple, p9.aes(x='date', y='weight_rolling', color='meat_type'))\n    + p9.geom_line()\n    + p9.labs(\n        title=\"US Meat Production\", \n        subtitle=\"12-month moving average\", \n        x=\"\", \n        y=\"Weight (Millions lbs)\"\n    )\n    + p9.scale_colour_discrete(\n        s=0.8,\n        l=0.5,\n        labels=lambda values: [v.title() for v in values],\n        name=\" \",  # Have to have a space, using an empty string will be ignored\n    )\n    + p9.scale_y_continuous(limits=(0, 4000), expand=(0,0))\n    + p9.scale_x_date(expand=(0,0))\n    + p9.theme_bw()\n    + p9.theme(legend_position=(0.05, 0.99))\n)\n\n\n\n\n\n\n\n\nWe can also eliminate the outside frame\n\n(\n    p9.ggplot(meat_simple, p9.aes(x='date', y='weight_rolling', color='meat_type'))\n    + p9.geom_line()\n    + p9.labs(\n        title=\"US Meat Production\", \n        subtitle=\"12-month moving average\", \n        x=\"\", \n        y=\"Weight (Millions lbs)\"\n    )\n    + p9.scale_colour_discrete(\n        s=0.8,\n        l=0.5,\n        labels=lambda values: [v.title() for v in values],\n        name=\" \",  # Have to have a space, using an empty string will be ignored\n    )\n    + p9.scale_y_continuous(limits=(0, 4000), expand=(0,0))\n    + p9.scale_x_date(expand=(0,0))\n    + p9.theme_bw()\n    + p9.theme(\n        legend_position=(0.05, 0.99), \n        panel_border=p9.element_blank(),\n        axis_line=p9.element_line(size=1, color='#AAAAAA')\n    )\n)\n\n\n\n\n\n\n\n\nWe could also forgo the legend and label the lines directly\n\nline_label = meat_simple[meat_simple.date=='2005-01-01'].copy().assign(\n    meat_label=lambda x: x['meat_type'].str.title()\n)\nline_label\n\n\n\n\n\n\n\n\n\ndate\nmeat_type\nweight_M_lbs\nweight_rolling\nmeat_label\n\n\n\n\n3276\n2005-01-01\nbeef\n1915.6\n2044.800000\nBeef\n\n\n3277\n2005-01-01\nbroilers\n2884.1\n2843.658333\nBroilers\n\n\n3278\n2005-01-01\nother\n66.5\n71.750000\nOther\n\n\n3279\n2005-01-01\npork\n1704.8\n1704.650000\nPork\n\n\n3280\n2005-01-01\nturkey\n455.3\n455.791667\nTurkey\n\n\n\n\n\n\n\n\n\n(\n    p9.ggplot(meat_simple, p9.aes(x='date', y='weight_rolling', color='meat_type'))\n    + p9.geom_line()\n    + p9.labs(\n        title=\"US Meat Production\", \n        subtitle=\"12-month moving average\", \n        x=\"\", \n        y=\"Weight (Millions lbs)\"\n    )\n    + p9.scale_colour_discrete(\n        s=0.8,\n        l=0.5,\n        guide=None\n        \n    )\n    + p9.geom_label(\n        data=line_label,\n        mapping=p9.aes(label='meat_label'),\n        nudge_y=200\n    )\n    + p9.scale_y_continuous(limits=(0, 4000), expand=(0,0))\n    + p9.scale_x_date(expand=(0,0))\n    + p9.theme_bw()\n    + p9.theme(\n        panel_border=p9.element_blank(), \n        axis_line=p9.element_line(size=1, color='#AAAAAA'),\n    )\n)\n\n\n\n\n\n\n\n\nIf you don’t like the label and line overlap, you can manually manipulate the label frame. Unfortunately, the nudge_y does not get passed in from the mapping.\n\n# This is the same as before\nline_label = meat_simple[meat_simple.date=='2005-01-01'].copy().assign(\n    meat_label=lambda x: x['meat_type'].str.title()\n)\n# Move away from the lines / each other, taking the nudge_y into account\n# This is a \"guess and check\" for finalized figures\nadjustments = {'pork': -450, 'broilers': 250, 'beef': 200}\nfor meat, shift in adjustments.items():\n    line_label.loc[line_label.meat_type==meat, 'weight_rolling'] += shift \n\n\n(\n    p9.ggplot(meat_simple, p9.aes(x='date', y='weight_rolling', color='meat_type'))\n    + p9.geom_line()\n    + p9.labs(\n        title=\"US Meat Production\", \n        subtitle=\"12-month moving average\", \n        x=\"\", \n        y=\"Weight (Millions lbs)\"\n    )\n    + p9.scale_colour_discrete(\n        s=0.8,\n        l=0.5,\n        guide=None\n        \n    )\n    + p9.geom_label(\n        data=line_label,\n        mapping=p9.aes(label='meat_label'),\n        nudge_y=200,\n        size=10\n    )\n    + p9.scale_y_continuous(limits=(0, 4000), expand=(0,0))\n    + p9.scale_x_date(expand=(0,0))\n    + p9.theme_bw()\n    + p9.theme(\n        panel_border=p9.element_blank(), \n        axis_line=p9.element_line(size=1, color='#AAAAAA'),\n        plot_subtitle=p9.element_text(size=10)\n    )\n)\n\n\n\n\n\n\n\n\nThis last example completely eliminated the need for our original problem (turning the meat into a category with some order, to force the legend to have a sensible ordering). Note that if we pivot to something like facets, the ordering also follows the categorical ordering:\n\n(\n    p9.ggplot(meat_simple, p9.aes(x='date', y='weight_rolling'))\n    + p9.geom_line(color='red')\n    + p9.labs(\n        title=\"US Meat Production\", \n        subtitle=\"12-month moving average\", \n        x=\"\", \n        y=\"Weight (Millions lbs)\"\n    )\n    + p9.scale_y_continuous(limits=(0, 4000), expand=(0,0))\n    + p9.scale_x_date(expand=(0,0))\n    + p9.theme_bw()\n    + p9.theme(\n        panel_border=p9.element_blank(), \n        axis_line=p9.element_line(size=1, color='#AAAAAA'),\n        plot_subtitle=p9.element_text(size=10)\n    )\n    + p9.facet_grid('meat_type')\n)\n\n\n\n\n\n\n\n\nOne improvement we might make to this plot: show all the other lines at the same time in grey, so we can see how a particular line compares\n\n(\n    p9.ggplot(meat_simple, p9.aes(x='date', y='weight_rolling'))\n    + p9.geom_line(\n        data=meat_simple.rename(columns={'meat_type': 'meat_group'}), \n        mapping=p9.aes(group='meat_group'),\n        color='grey',\n        alpha=0.3\n    )           \n    + p9.geom_line(color='red')  # add on top of grey line, but only for this facet\n    + p9.labs(\n        title=\"US Meat Production\", \n        subtitle=\"12-month moving average\", \n        x=\"\", \n        y=\"Weight (Millions lbs)\"\n    )\n    + p9.scale_y_continuous(limits=(0, 4000), expand=(0,0))\n    + p9.scale_x_date(expand=(0,0))\n    + p9.theme_bw()\n    + p9.theme(\n        panel_border=p9.element_blank(), \n        axis_line=p9.element_line(size=1, color='#AAAAAA'),\n        plot_subtitle=p9.element_text(size=10),\n        panel_grid=p9.element_blank(),\n        figure_size=(6, 10)\n    )\n    + p9.facet_grid('meat_type')\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/annotating-a-single-point.html",
    "href": "posts/ggplot2-tips/annotating-a-single-point.html",
    "title": "Annotating single points in datasets",
    "section": "",
    "text": "We don’t want to annotate all data points, only one or two extraordinary ones."
  },
  {
    "objectID": "posts/ggplot2-tips/annotating-a-single-point.html#alternative",
    "href": "posts/ggplot2-tips/annotating-a-single-point.html#alternative",
    "title": "Annotating single points in datasets",
    "section": "Alternative",
    "text": "Alternative\nThis approach isn’t quite as attractive, but it is a lot less manual (you don’t really have to look at the data and decide exactly where to put the labels).\n\nmtcars['rank'] = mtcars['hp'].rank(ascending=False)\n\n\n(\n    p9.ggplot(mtcars, mapping=p9.aes(x='mpg', y='hp', label='name'))\n    + p9.geom_point()\n    + p9.geom_label(data=mtcars[mtcars['rank']==1], color='orange', va=\"bottom\", ha=\"left\", alpha=0.8)\n    + p9.geom_point(data=mtcars[mtcars['rank']==1], color='orange', size=2)\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/making-plots-wider.html",
    "href": "posts/ggplot2-tips/making-plots-wider.html",
    "title": "Changing the size of plots",
    "section": "",
    "text": "We want to control the size of plots in plotnine."
  },
  {
    "objectID": "posts/ggplot2-tips/making-plots-wider.html#example",
    "href": "posts/ggplot2-tips/making-plots-wider.html#example",
    "title": "Changing the size of plots",
    "section": "Example",
    "text": "Example\nLet’s start with a standard, default plot.\n\nimport plotnine as p9\nfrom plotnine.data import mtcars\n\n\n(\n    p9.ggplot(mtcars, mapping=p9.aes(x=\"wt\", y=\"mpg\", color=\"factor(cyl)\"))\n    + p9.geom_point()\n)\n\n\n\n\n\n\n\n\nNow we can change the title of the legend (to be something a little friendlier) and the figure size\n\n(\n    p9.ggplot(mtcars, mapping=p9.aes(x=\"wt\", y=\"mpg\", color=\"factor(cyl)\"))\n    + p9.geom_point()\n    + p9.scale_color_discrete(name=\"# Cylinders\")\n    + p9.theme(\n        figure_size=(8, 4)\n    )\n)\n\n\n\n\n\n\n\n\nThemes can also place legends inside the box (which can free more space)\n\n(\n    p9.ggplot(mtcars, mapping=p9.aes(x=\"wt\", y=\"mpg\", color=\"factor(cyl)\"))\n    + p9.geom_point()\n    + p9.scale_color_discrete(name=\"# Cylinders\")\n    + p9.theme(\n        figure_size=(8, 4),\n        legend_position=(1,1)\n    )\n)\n\n\n\n\n\n\n\n\nWhen using a theme, place it after any custom theme you want to use\n\n(\n    p9.ggplot(mtcars, mapping=p9.aes(x=\"wt\", y=\"mpg\", color=\"factor(cyl)\"))\n    + p9.geom_point()\n    + p9.scale_color_discrete(name=\"# Cylinders\")\n    + p9.theme_538()\n    + p9.theme(\n        figure_size=(8, 4),\n        legend_position=(1,1)\n    )\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/gallery/error_bars_with_shaded_region.html",
    "href": "posts/ggplot2-tips/gallery/error_bars_with_shaded_region.html",
    "title": "Ensemble of Confidence Intervals",
    "section": "",
    "text": "This shows how to create a plot of Monte Carlo’ed confidence intervals. In this particular problem, we know that the parameter we are estimating – earliest time to failure (ETTF) – cannot be greater than the smallest failure time in the data set.\nThe data set shows: - The true value of ETTF as a dashed line - The value of the smallest data point (dot) - The confidence interval - Shades the region of values higher than the smallest value, as an experimenter in this regieme would know this is an infeasible regieme for the ETTF.\nShows how to shade a region, and remove the background grid points / borders / etc.\nThe model in this case is to use data to estimate the ETTF \\(\\Theta\\), where the probability of failure follows the distribution\n\\[ p(t) = \\exp(-(t-\\Theta)) \\text{ if }t &gt; \\Theta,\\text{ 0 otherwise}\\]\n\n\nCode\nimport numpy as np\nimport plotnine as p9\nimport pandas as pd\n\n\nnp.random.seed(26)\n\nTHETA = 10\nN_SIM = 20\nSAMPLE_SIZE = 40\n\nworlds = THETA + np.random.exponential(size=(SAMPLE_SIZE, N_SIM))\nmeans = worlds.mean(axis=0)\nsmallest = worlds.min(axis=0)\n\n# intervals for Theta\nci_lower = (means - 1) - 1.96*1/np.sqrt(SAMPLE_SIZE)\nci_upper = (means - 1) + 1.96*1/np.sqrt(SAMPLE_SIZE)\n\n# How often is THETA in the CI?\nvalue_in_ci =  ((ci_lower&lt;=THETA) & (THETA &lt;= ci_upper)).mean()\n# How often do we some infeasible values?\n# Here means the ci_upper is greater than the lowest value in the data set\nhas_some_infeasible = (ci_upper &gt; smallest).mean()\n# How often is the entire CI infeasible (meaning we know from the data set\n# that the value logically cannot be in the interval)\nentire_ci_infeasible = (ci_lower &gt; smallest).mean()\n\nplot_data = pd.DataFrame({'lower': ci_lower, 'upper': ci_upper, 'smallest': smallest}).reset_index()\ntitle = f\"Confidence Intervals for ETTF (True value = 10 s, sample size is {SAMPLE_SIZE})\"\n(\n    p9.ggplot(\n        plot_data, \n        p9.aes(y='index')\n    )\n    + p9.geom_errorbarh(mapping=p9.aes(xmin='lower', xmax='upper'))\n    + p9.geom_point(mapping=p9.aes(x='smallest'), color='red')\n    + p9.labs(x=\"\", y=\"\", title=title)\n    + p9.geom_vline(xintercept=THETA, linetype=\"dashed\")\n    + p9.theme_bw()\n    + p9.theme(\n        axis_ticks=p9.element_blank(),\n        panel_grid_major=p9.element_blank(),\n        panel_border=p9.element_blank(),\n    )\n    + p9.scale_y_continuous(breaks=[])\n    + p9.geom_rect(\n        mapping=p9.aes(xmax=float('inf'), xmin='smallest', ymin='index-0.5', ymax='index+0.5'), \n        alpha=0.5\n    )\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/making-boxes-on-ggplots.html",
    "href": "posts/ggplot2-tips/making-boxes-on-ggplots.html",
    "title": "Shading regions on a plot",
    "section": "",
    "text": "Problem\nWe want to shade a region on a plot. Note that matplotlib and altair can have annoyances about finding the “edges” of the range.\n\n\nSolution\n\nUse p9.geom_rect with p9.aes(xmin=..., xmax=..., ymin=..., ymax=...) as its first argument\nUse the alpha argument to make the box semi-transparent\n\n\n\nExample\nWe will make a normal distribution, with the negative region blocked out\n\nimport pandas as pd\nimport plotnine as p9\nimport scipy.stats\nimport numpy as np\n\n\nx_values = np.linspace(-2, 6, 100)\ny_values = scipy.stats.norm(2, 1).pdf(x_values)\n\nshifted_gaussian_df = pd.DataFrame({'x': x_values, 'density': y_values})\nshifted_gaussian_df.head()\n\n\n\n\n\n\n\n\n\nx\ndensity\n\n\n\n\n0\n-2.000000\n0.000134\n\n\n1\n-1.919192\n0.000184\n\n\n2\n-1.838384\n0.000252\n\n\n3\n-1.757576\n0.000343\n\n\n4\n-1.676768\n0.000463\n\n\n\n\n\n\n\n\nLet’s start by just plotting the normal distribution:\n\n(\n    p9.ggplot(shifted_gaussian_df, p9.aes(x='x', y='density'))\n    + p9.geom_line()\n)\n\n\n\n\n\n\n\n\nLet’s shade the area of the plot with \\(x &lt; 0\\)\n\n(\n    p9.ggplot(shifted_gaussian_df, p9.aes(x='x', y='density'))\n    + p9.geom_line()\n    + p9.geom_rect(\n        mapping=p9.aes(xmin=float(\"-inf\"), xmax=0, ymin=float(\"-inf\"), ymax=float(\"inf\")),\n        alpha=0.005, fill='red'\n    )\n    + p9.theme_bw()\n    # Add some text as well\n    + p9.annotate(\"text\", x=-1, y=0.35, label=\"Rejection zone\", color=\"blue\")\n    + p9.labs(x=\"\", y=\"\", title=\"A gaussian distribution faking a decision process\")\n)\n\n\n\n\n\n\n\n\nIf you want the area under the curve instead, it looks a little different:\n\n(\n    p9.ggplot(shifted_gaussian_df, p9.aes(x='x', y='density'))\n    + p9.geom_area(data=shifted_gaussian_df[shifted_gaussian_df['x'] &gt;= 0], fill=\"blue\")\n    + p9.geom_line()\n    + p9.geom_rect(\n        mapping=p9.aes(xmin=float(\"-inf\"), xmax=0, ymin=float(\"-inf\"), ymax=float(\"inf\")),\n        alpha=0.005, fill='red'\n    )\n    + p9.theme_bw()\n    # Add some text as well\n    + p9.annotate(\"text\", x=-1, y=0.35, label=\"Rejection zone\", color=\"blue\")\n    + p9.labs(x=\"\", y=\"\", title=\"A gaussian distribution faking a decision process\")\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/gnattt_charts.html",
    "href": "posts/ggplot2-tips/gnattt_charts.html",
    "title": "Gnatt Charts",
    "section": "",
    "text": "We are going to make a Gnatt chart, which shows projects from different priorities, start-end dates, and names.\nThings we will need to be able to do:\n\nOrder categorical variables\nMake plots treat continuous variables as categorical\nHide legends\n\nThe overall approach will be to use thick lines"
  },
  {
    "objectID": "posts/ggplot2-tips/gnattt_charts.html#first-attempt",
    "href": "posts/ggplot2-tips/gnattt_charts.html#first-attempt",
    "title": "Gnatt Charts",
    "section": "First attempt",
    "text": "First attempt\nSimply create a line graph with thick lines. To do this, we need to have the start and end be in the same column. We will melt the data to do this\n\ngnatt_melted = gnatt.melt([\"name\", \"priority\"], var_name='timestamp_type', value_name='timestamp_val').sort_values('timestamp_val')\ngnatt_melted\n\n\n\n\n\n\n\n\n\nname\npriority\ntimestamp_type\ntimestamp_val\n\n\n\n\n0\ndraft post\n1\nstart\n2024-02-15 00:00:00\n\n\n1\nreview post\n1\nstart\n2024-02-20 00:00:00\n\n\n8\ndraft post\n1\nmid\n2024-02-20 00:00:00\n\n\n9\nreview post\n1\nmid\n2024-02-24 00:00:00\n\n\n4\ndraft post\n1\nend\n2024-02-25 00:00:00\n\n\n2\nprep outreach emails\n2\nstart\n2024-02-26 00:00:00\n\n\n5\nreview post\n1\nend\n2024-02-28 00:00:00\n\n\n10\nprep outreach emails\n2\nmid\n2024-03-01 00:00:00\n\n\n6\nprep outreach emails\n2\nend\n2024-03-05 00:00:00\n\n\n3\npublish\n1\nstart\n2024-03-07 00:00:00\n\n\n11\npublish\n1\nmid\n2024-03-07 12:00:00\n\n\n7\npublish\n1\nend\n2024-03-08 00:00:00\n\n\n\n\n\n\n\n\n\n(\n    p9.ggplot(gnatt_melted, p9.aes(x='timestamp_val', y='name', group='name', color='factor(priority)'))\n    + p9.geom_line(size=10)\n    + p9.scale_x_date(breaks='1 week')\n)\n\n\n\n\n\n\n\n\nProblems with this graph:\n\nOrdering of the y-axis\ny-labels could be moved into the bars\nCan use text labels to replace y text label, and the priority\nRemove the labels on axes as they are not adding value"
  },
  {
    "objectID": "posts/ggplot2-tips/gnattt_charts.html#second-attempt---ordering-categories",
    "href": "posts/ggplot2-tips/gnattt_charts.html#second-attempt---ordering-categories",
    "title": "Gnatt Charts",
    "section": "Second attempt - Ordering categories",
    "text": "Second attempt - Ordering categories\nggplot / plotnine makes arranging categories much harder than it needs to be. The way that I have found that works is to make the names explicitly categorical:\n\ngnatt['label'] = gnatt.apply(lambda row: f\"{row[\"name\"]} (P{row.priority})\", axis=1)\n# Here is the categorical part\ngnatt['name'] = pd.Categorical(gnatt['name'], categories=gnatt.sort_values('start')['name'].tolist()[::-1], ordered=True)\n\ngnatt_melted = gnatt.drop('label', axis=1).melt([\"name\", \"priority\"], var_name='timestamp_type', value_name='timestamp_val').sort_values('timestamp_val')\n\nNow the same plotting code will line the categories up properly:\n\n(\n    p9.ggplot(gnatt_melted, p9.aes(x='timestamp_val', y='name', group='name', color='factor(priority)'))\n    + p9.geom_line(size=10)\n    + p9.scale_x_date(breaks='1 week')\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/gnattt_charts.html#third-attempt-tidying-up",
    "href": "posts/ggplot2-tips/gnattt_charts.html#third-attempt-tidying-up",
    "title": "Gnatt Charts",
    "section": "Third attempt: Tidying up",
    "text": "Third attempt: Tidying up\nNow let’s make some of the other improvements!\n\n(\n    p9.ggplot(gnatt_melted, p9.aes(x='timestamp_val', y='name', group='name', color='factor(priority)'))\n    + p9.geom_line(size=10)\n    + p9.scale_x_date(breaks='1 week')\n    + p9.labs(title='Blog release timeline', x='', y='')\n    + p9.geom_text(data=gnatt, mapping=p9.aes(label='label', x='mid', y='name'), color=\"black\")\n    + p9.scale_color_discrete(guide=None)\n    + p9.scale_y_discrete(labels= lambda v:[\"\" for _ in v]) \n    + p9.theme_bw()\n    + p9.theme(figure_size=(8, 3), )\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/making-manual-error-bars.html",
    "href": "posts/ggplot2-tips/making-manual-error-bars.html",
    "title": "Making manual error bars / error regions",
    "section": "",
    "text": "Problem\nThere are lots of examples of being able to do a linear regression, and have plotnine draw the region of uncertainity automatically.\nThere are fewer examples (or they are harder to find) of how to add error bars or error regions that you have already calculated.\n\n\nSolution\nFor error regions, we use geom_ribbon:\np9.geom_ribbon(mapping=p9.aes(ymin='columnname', ymax='columnname'))\nFor error bars, we use geom_errorbar or geom_pointrange:\np9.geom_errorbar(mapping=p9.aes(ymin='columnname', ymax='columnname'))\np9.geom_pointrange(mapping=p9.aes(ymin='columnname', ymax='columnname'))\nThe difference is that the geom_errorbar draws crossbars at the top and bottom, while geom_pointrange only draws the vertical line.\n\n\nExample of a ribbon\nLet’s look at an A/B test, where we are looking for a change in conversion from an email\n\nimport pandas as pd \nimport plotnine as p9\nimport numpy as np\n\n\nemails = pd.read_csv('composing/email.csv')\nemails['date'] = pd.to_datetime(emails['date'])\nemails.head()\n\n\n\n\n\n\n\n\n\ndate\nrecipients\nclicks\nctr\n\n\n\n\n0\n2024-02-01\n99886\n1456\n0.014577\n\n\n1\n2024-02-02\n100220\n1491\n0.014877\n\n\n2\n2024-02-03\n99637\n1498\n0.015035\n\n\n3\n2024-02-04\n99344\n1543\n0.015532\n\n\n4\n2024-02-05\n100091\n1559\n0.015576\n\n\n\n\n\n\n\n\n\nemails['lower'] =  emails['ctr'] - 1.96 * np.sqrt(emails['ctr']*(1-emails['ctr']) / emails['recipients'])\nemails['upper'] =  emails['ctr'] + 1.96 * np.sqrt(emails['ctr']*(1-emails['ctr']) / emails['recipients'])\n\n\n(\n    p9.ggplot(emails, p9.aes(x='date', y='ctr'))\n    + p9.geom_line()\n    + p9.geom_ribbon(mapping=p9.aes(ymin='lower', ymax='upper'), fill='blue', alpha=0.3)\n    + p9.labs(x=\"\", y=\"click-thru-rate\")\n)\n\n\n\n\n\n\n\n\n\n\nExample of error bars\nWe can use the same dataset with points and error bars instead:\n\n(\n    p9.ggplot(emails, p9.aes(x='date', y='ctr'))\n    + p9.geom_point()\n    + p9.geom_errorbar(mapping=p9.aes(ymin='lower', ymax='upper'), alpha=0.3)\n    + p9.labs(x=\"\", y=\"click-thru-rate\")\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/dual_bar/dual_bar.html",
    "href": "posts/ggplot2-tips/dual_bar/dual_bar.html",
    "title": "Making dual bar charts",
    "section": "",
    "text": "Problem\nShow a bar chart in two different directions from the central dividing line (two bars).\nThe typical example is showing the male and female population by age bracket in a population pyramid.\n\n\nSolution\nWe can use geom_rect or geom_tile to create filled rectangle.\n\ngeom_rect takes the coordinates of the upper-left and lower-right corners of the rectangle.\ngeom_fill takes the coordiantes of the center of the rectangle, as well as it’s width and height.\n\n\n\nExample 1\nFrom the US Census we have data on the US population by age bracket and gender. We want to make a population pyramid out of this data. We start by loading the data and converting the data into long-form:\n\nimport pandas as pd\nimport plotnine as p9\n\npop = (\n    pd.read_csv('2022_acs_us_pop.csv')\n    [['age_bracket_label', 'pop_male', 'pop_female']]\n    .rename(\n        columns={'pop_male': 'male', 'pop_female': 'female'}\n    ).melt(\n        'age_bracket_label',\n        var_name='gender',\n        value_name='population'\n    )\n)\npop.head()\n\n\n\n\n\n\n\n\n\nage_bracket_label\ngender\npopulation\n\n\n\n\n0\nUnder 5 years\nmale\n9725644\n\n\n1\n5 to 9 years\nmale\n10210019\n\n\n2\n10 to 14 years\nmale\n10974635\n\n\n3\n15 to 19 years\nmale\n11196816\n\n\n4\n20 to 24 years\nmale\n11400730\n\n\n\n\n\n\n\n\nBefore creating the population pyramid, let’s look at what we get from a using geom_bar (it is a stacked bar chart)\n\n(\n    p9.ggplot(pop, p9.aes(x='age_bracket_label', y='population', fill='gender'))\n    + p9.geom_bar(stat='identity')\n)\n\n\n\n\n\n\n\n\n\npop['signed_pop'] = pop.apply(lambda row:  row.population if row.gender == 'male'  else -row.population, axis=1)\n# We are ending one side of the rectangle on 0,\n# and the other side on 'signed_pop', so the center is \n# half of the signed_pop\npop['center'] = pop['signed_pop'] / 2\n\n\npop.age_bracket_label.unique().tolist()\n\n['Under 5 years',\n '5 to 9 years',\n '10 to 14 years',\n '15 to 19 years',\n '20 to 24 years',\n '25 to 29 years',\n '30 to 34 years',\n '35 to 39 years',\n '40 to 44 years',\n '45 to 49 years',\n '50 to 54 years',\n '55 to 59 years',\n '60 to 64 years',\n '65 to 69 years',\n '70 to 74 years',\n '75 to 79 years',\n '80 to 84 years',\n '85 years and over']\n\n\nThe p9.geom_tile takes an x, y, width and height to draw a filled rectangle. The x and y positions are the locations are the “center” of the rectangle, which is why we have calculated the center attribute.\n\n(\n    p9.ggplot(pop, p9.aes(x='age_bracket_label', y='population', fill='gender'))\n    + p9.geom_tile(mapping=p9.aes(y='center', width=1, height='signed_pop'))\n)\n\n\n\n\n\n\n\n\nWe can rotate the text, but let’s swap the axes. We will also order the categories correctly.\n\n(\n    p9.ggplot(pop, p9.aes(y='age_bracket_label', x='population', fill='gender'))\n    + p9.geom_tile(mapping=p9.aes(x='center', height=1, width='signed_pop'))\n    + p9.labs(y=\"\")\n    # This gets the order the same as they appear in the dataframe\n    # (otherwise is alphabetical)\n    + p9.scale_y_discrete(limits=pop.age_bracket_label.unique().tolist())\n)\n\n\n\n\n\n\n\n\nLet’s also format the population, so we are not showing the female population as negative\n\n(\n    p9.ggplot(pop, p9.aes(y='age_bracket_label', x='population', fill='gender'))\n    + p9.geom_tile(mapping=p9.aes(x='center', height=0.9, width='signed_pop'), alpha=0.6)\n    + p9.labs(y=\"\", title=\"Population Pyramid (US 2022)\", x=\"\")\n    # This gets the order the same as they appear in the dataframe\n    # (otherwise is alphabetical)\n    + p9.scale_y_discrete(limits=pop.age_bracket_label.unique().tolist())\n    + p9.scale_x_continuous(labels=lambda labs: [f\"{abs(l/1e6):.0f} M\" for l in labs])\n    + p9.theme_linedraw()\n    + p9.theme(panel_border=p9.element_blank())\n)\n\n\n\n\n\n\n\n\n\n\nExample 2\nThis example is taken from the ggplot examples of Albert Rapp (the original post is called “How to create diverging bar plots”)."
  },
  {
    "objectID": "posts/ggplot2-tips/palette/making-a-fixed-color-palette.html",
    "href": "posts/ggplot2-tips/palette/making-a-fixed-color-palette.html",
    "title": "Making a custom palette",
    "section": "",
    "text": "Example\nWe are going to use the Kaggle retail sales dataset, and make predictions for Store 1. The modelling is not very sophisticated (no partial pooling amongst the different stores, or use of exogeneous variables). Instead I built two simple models:\n\nA SARIMA model\nA Prophet model (generalized additive components)\n\nThis is just to give a couple of different forecasts to track. The business requirement is that we always want the actuals to be black.\n\nimport pandas as pd\nimport plotnine as p9\n\nsales = pd.read_pickle('single_store_forecasts.pickle')\ntoday = sales.loc[sales['forecast']=='actuals', 'Date'].max()\nsales['is_future'] = sales['Date'] &gt; today\nsales\n\n\n\n\n\n\n\n\n\nDate\nWeekly_Sales\nforecast\nis_future\n\n\n\n\n0\n2010-02-05\n1.643691\nactuals\nFalse\n\n\n45\n2010-02-12\n1.641957\nactuals\nFalse\n\n\n90\n2010-02-19\n1.611968\nactuals\nFalse\n\n\n135\n2010-02-26\n1.409728\nactuals\nFalse\n\n\n180\n2010-03-05\n1.554807\nactuals\nFalse\n\n\n...\n...\n...\n...\n...\n\n\n503\n2013-10-18\n1.585226\nprophet\nTrue\n\n\n504\n2013-10-19\n1.582930\nprophet\nTrue\n\n\n505\n2013-10-20\n1.581724\nprophet\nTrue\n\n\n506\n2013-10-21\n1.581733\nprophet\nTrue\n\n\n507\n2013-10-22\n1.583054\nprophet\nTrue\n\n\n\n\n704 rows × 4 columns\n\n\n\n\nLet’s make the simplest plot out of the box:\n\n(\n    p9.ggplot(\n        sales, \n        p9.aes(x='Date', y='Weekly_Sales', group='forecast', color='forecast')\n    )\n    + p9.geom_line()\n    + p9.labs(x=\"\", y=\"Weekly Sales (in $M)\")\n)\n\n\n\n\n\n\n\n\nWe can make a version that only applies to the actuals:\n\n(\n    p9.ggplot(\n        sales, \n        p9.aes(x='Date', y='Weekly_Sales', group='forecast', color='forecast', linetype='is_future')\n    )\n    + p9.geom_line()\n    + p9.labs(x=\"\", y=\"Weekly Sales (in $M)\")\n    + p9.scale_color_manual(\n        values={\"actuals\": \"black\",}\n    )\n)\n\n/Users/damienmartin/anaconda3/envs/blog/lib/python3.12/site-packages/plotnine/scales/scale_manual.py:44: PlotnineWarning: The palette of scale_color_manual can return a maximum of 1 values. 3 were requested from it.\n/Users/damienmartin/anaconda3/envs/blog/lib/python3.12/site-packages/plotnine/scales/scale_manual.py:44: PlotnineWarning: The palette of scale_color_manual can return a maximum of 1 values. 3 were requested from it.\n\n\n\n\n\n\n\n\n\n\n# We can also be more prescriptive\n(\n    p9.ggplot(\n        sales, \n        p9.aes(x='Date', y='Weekly_Sales', group='forecast', color='forecast', linetype='is_future')\n    )\n    + p9.geom_line(alpha=0.3)\n    + p9.geom_line(data=sales[sales['forecast']=='actuals'], alpha=1)\n    + p9.labs(x=\"\", y=\"Weekly Sales (in $M)\")\n    + p9.scale_color_manual(\n        values={\"actuals\": \"black\", \"SARIMA\": \"red\", \"prophet\": \"blue\"}\n    )\n    + p9.theme_bw()\n    + p9.geom_vline(xintercept=today, linetype='dotted')\n    + p9.annotate('text', x=today, y=2.3, label='Today', nudge_x=-20, angle=90)\n)"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\nWhere to find Jupyter Cell output codes: https://quarto.org/docs/reference/cells/cells-jupyter.html#code-output"
  },
  {
    "objectID": "posts/python-tips/publish_python.html",
    "href": "posts/python-tips/publish_python.html",
    "title": "Publishing Python Packages",
    "section": "",
    "text": "Suppose you have a company python package index with the name our-user-libs (the default is to publish to PYPI).\nThere are two ways to publish to a package index:"
  },
  {
    "objectID": "posts/python-tips/publish_python.html#old-way-setup.py",
    "href": "posts/python-tips/publish_python.html#old-way-setup.py",
    "title": "Publishing Python Packages",
    "section": "Old way (setup.py)",
    "text": "Old way (setup.py)\npython setup.py sdist upload -r our-user-libs"
  },
  {
    "objectID": "posts/python-tips/publish_python.html#new-way-pyproject.toml",
    "href": "posts/python-tips/publish_python.html#new-way-pyproject.toml",
    "title": "Publishing Python Packages",
    "section": "New way (pyproject.toml)",
    "text": "New way (pyproject.toml)\npython -m twine upload -r our-user-libs dist/\nYou’ll need to install the twine package first."
  },
  {
    "objectID": "posts/python-tips/logging_things.html",
    "href": "posts/python-tips/logging_things.html",
    "title": "Logging - How to and best practices",
    "section": "",
    "text": "Logging in Python is very flexible, but has some odd defaults that make it non-intuitive to use.\nThe simplest example is the following code, which goes not print out anything:\n\nimport logging\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)  # Should print at level logging.INFO and above\n\nlogger.info(\"Hello\")\n\nSee? Nothing printed.\nWe can try the following solution:\n\nimport logging\n\nlogger = logging.getLogger('example')\nlogger.setLevel(logging.INFO)\nlogger.info('one')   # nothing printed\nlogging.info('two')  # nothing printed\nlogger.info('three') # prints / logs \"three\"\n\nINFO:example:three\n\n\nWhy is only three printed? We will see why after looking at the next example, which fixes these problems.\n\nimport logging\n\nlogging.basicConfig()\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)  # Should print at level logging.DEBUG and above\n\nlogger.info(\"Hello\")\n\nINFO:__main__:Hello\n\n\nThe addition of logging.basicConfig() now allows you to set the level in a way that you expect!\nThe problem is that python has a hierarchical system of handlers, and calling logging.basicConfig() affects the root handler. Logging messages start at the root hander, and it decides whether the logger you are defining even gets to see the message. In our previous example, where we tried to log one, two, and three, the call to logging.info implicitly called the logging.basicConfig(), which then affects the all loggers from here on out. This call is changing the root logging handler, something we will define later on.\nTo show that this is a change that persists, let’s run the same code sample in the same kernel:\n\nimport logging\n\nlogger = logging.getLogger('example')\nlogger.setLevel(logging.INFO)\nlogger.info('one')   # nothing printed\nlogging.info('two')  # nothing printed\nlogger.info('three') # prints / logs \"three\"\n\nINFO:example:one\nINFO:example:three"
  },
  {
    "objectID": "posts/python-tips/logging_things.html#i-just-want-something-that-works-for-simple-apps",
    "href": "posts/python-tips/logging_things.html#i-just-want-something-that-works-for-simple-apps",
    "title": "Logging - How to and best practices",
    "section": "I just want something that works for simple apps",
    "text": "I just want something that works for simple apps\nIf you just want to get logging, the basic solution is\n\nCall logging.basicConfig() early. This means doing it before other modules!\n\nThe problems with this approach are: 1. The call to logging.basicConfig() affects the root logger, so all loggigng is affected. This causes noisy libraries to propogate their error messages. 2. The call to logging.basicConfig() is only processed once, so you have to make sure it isn’t called by any of your imports. This also means that your import is going to fight with everyone else’s. It isnt’ a problem if you are all trying to set the level to DEBUG in the basic config.\nCalling logging.basicConfig() acts on the root logger, and because logging is handled in a hierarchy (with the root logger at the root), this can make it hard to reason about. There is only one root logger in your application, so you have to be aware of these side affects for not only everything you use, but everything you import.\nThis stackoverflow answer discusses this in more detail."
  },
  {
    "objectID": "posts/python-tips/logging_things.html#the-better-approach",
    "href": "posts/python-tips/logging_things.html#the-better-approach",
    "title": "Logging - How to and best practices",
    "section": "The better approach",
    "text": "The better approach\nA better approach is (from mCoding’s excellent overview)\n\nCreate a logger for your own script\nLeave propogation turned on to the root logger\nPut all complicated things (filters, formatters, etc) all on the root logger, not in your custom logger\nUse the logger you created, not the root logger, when logging messages. Your logger will propagate to the root logger, which will then use the filters and formatters to “do-the-right-thing” (TM).\n\nIn code, you might do this:\nimport logging\nimport logging.config\n\nlogger = logging.getLogger(\"my_app\")\n\nlogger_config = {\n    \"version\": 1,  # always 1, allows future updates\n    \"disable_existing_loggers\": False,\n    \"filters\":{},\n    \"formatters\": {\n        \"simple\": {\n            \"format\": \"%(levelname)s %(message)s\",\n        }\n    },\n    \"handlers\": {\n        \"stderr\": {\n            \"class\": \"logging.StreamHandler\",   # Print to screen\n            \"formatter\": \"simple\",              # References a key in the formatters dict\n            \"stream\": \"ext://sys.stderr\",\n        }\n    },\n    \"loggers\": {\n        \"root\": {\"level\": \"DEBUG\", \"handlers\": [\"stderr\"]}   # handlers references a key in handlers dict\n    }\n}\n\nlogging.config.dictConfig(config=logger_config)\n\n# use your logger\nlogger.info(\"This is my logger, and is good\")\n\n# don't use the root logger directly\nlogging.info(\"This is using the root logger, and this is bad\")"
  },
  {
    "objectID": "posts/python-tips/logging_things.html#a-single-logger",
    "href": "posts/python-tips/logging_things.html#a-single-logger",
    "title": "Logging - How to and best practices",
    "section": "A single logger",
    "text": "A single logger\nHere are the basic parts of logging:\n\nlogger: The is the the thing you make when calling my_logger = getLogger(\"my_awesome_logger\"). The logger is responsible for creating LogRecords, which is a data structure containing the information we want to log. It does this with a variety of methods: my_logger.debug(\"message\") creates a LogRecord with level DEBUG, etc. In addition, a logger contains the following parts:\n\nAn internal level (numeric, but commonly aliased to DEBUG, INFO, WARNING, ERROR, CRITICAL) for the logger that can be set and changed.\nA filter, for determining which records\nA collection of handlers (see below)\nA propagation flag\n\nlog records: A log record is recreated when you call my_logger.debug(message), my_logger.info(message), etc. It creates a data structure that contains the message you gave it, as well as injecting a collection of metadata (e.g. when the message was called, what module it was called in, what function you were in, what line number you were on, what the level of the message is (debug vs info vs warning vs error vs critical), etc. The various fields you can access are listed in the documentation here.\nformatters: A way of taking a log record, and turning into a string that can be printed, stored in a file, a database, etc.\nhandlers: A handler takes a log record, and decides what to do with it. Each handler typically deals with place to put logs, called a sink. If you wanted to print logs to std out and save them in a file, you would typically have two handlers: one handler for printing to std out, and a separate handler for each. A handler typically has\n\nAn internal level (numeric, but commonly aliased to DEBUG, INFO, WARNING, ERROR, CRITICAL), just like the logger (this is a source of much confusion)\nA filter, for determining if this log record should be handled\nA formatter, which takes a log record and turns it into a string. This is the thing that is printed / stored in a file / stored in a database, etc.\nA sink: where we put the non-filtered strings we created from log records\n\n\nHere is a graphical representation of a single logger:\n\n\n\nStructure of a Logger (mCoding inspired)\n\n\nWhen you call my_logger.info(\"my message\"), here is what happens:\n\nThe logger first checks it’s log level. If it is INFO or lower, then the logger creates the log record. If the logger’s level is INFO or higher, we don’t even make the record.\nThe logger then creates the LogRecord, which contains the message \"my_message\", as well as other meta data (e.g. when the LogRecord was created, the module/function/line number of the call, etc).\nThe logger then checks if the LogRecord should be filtered out. It has access to all the data in the log record, not just the level. If it should be filtered out, we stop here.\nThe logger then passes the LogRecord to each handler. Each handler now goes through a similar set of steps:\n\nEach handler checks it’s level, to see if it is INFO or lower. The handler stops if it’s level is higher than INFO.\nEach handler with level INFO or below sees if it should filter out this message (e.g. some records might be okay to go on a screen, but not saved to disk)\nFor each handler with level INFO or below, that didn’t filter out the message, then formats the message, and sends it to it’s sink.\n\nThe LogRecord is then propagated on to the parent logger of this logger (!!).\n\nIf my_logger rejects the LogRecord, we stop on steps 1 or 3 – we never get here. Being rejected by the logger means no propagation.\nIf my_logger doesn’t reject the LogRecord, we do this step after all the handlers for my_logger – even if every handler for my_logger rejected the message.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nHere are a couple of “gotchas”\n\nIf you have a handler with a level of DEBUG, but the logger has a higer level (e.g. INFO), you won’t see any DEBUG messages. This is because the logger is dropping the messages before the handler gets a chance to see it.\nIf you have a logger that drops a message (by level or filter), it will not be propagated to other loggers.\nIf the logger doesn’t drop a message, it will be propagated, even if no logger handles it.\n\n\n\nThe reason for this is if you wanted to block messages from Pandas, you only have to manipulate the logger to block at the logger e.g. to block Pandas DEBUG messages while keeping INFO and higher, but keep debug messages from your application, you would only need to set the LEVEL on the pandas logger to INFO, and the logger for your application to DEBUG.\nSo in addition to multiple handlers, we also have multiple loggers. Let’s deal with that next."
  },
  {
    "objectID": "posts/python-tips/logging_things.html#the-logging-tree",
    "href": "posts/python-tips/logging_things.html#the-logging-tree",
    "title": "Logging - How to and best practices",
    "section": "The logging tree",
    "text": "The logging tree\nWhen you type import logging into your application, you get a root logger. Any other logger you create is a descendent of this root.\nWhen you call logging.getLogger(\"silly_logger\"), then Python will create a logger object called \"silly_logger\" if it doesn’t exist; once the logger exists it will be returned. You will only have logger called \"silly_logger\" at any time in your program. This logger will (by default) pass any message it doesn’t drop on to the root logger.\nWhen you call logging.getLogger(\"my_app.X.awesome_logger\"), you get the three loggers (either returning them if they exist, or creating them): * my_app * my_app.X * my_app.X.awesome_logger and a link to the my_app.X.awesome_logger is returned. Each one of these loggers propagates any non-dropped messages up the chain to the next logger, with the my_app logger propagating to the root.\nAs a diagram, we have\n\n\n\n\n\nflowchart BT\n  AWESOME[my_app.X.awesome_logger] --&gt; MAX[my_app.X] --&gt; MA[my_app] --&gt; Root\n  S[silly_logger] --&gt; Root\n\n\n\n\n\n\n\n\nIt is possible to change loggers so they don’t propagate upward, but by default they do. Because it is very common for imports to have their own loggers, you should assume that the root logger will have messages passed to it, even if you turn propagation off in your loggers."
  },
  {
    "objectID": "posts/python-tips/logging_things.html#going-back-to-the-initial-problem",
    "href": "posts/python-tips/logging_things.html#going-back-to-the-initial-problem",
    "title": "Logging - How to and best practices",
    "section": "Going back to the initial problem",
    "text": "Going back to the initial problem\nWhen we started this article, we looked at a simple problem of why logging didn’t work until the root logger or basicConfig was called. Let’s look at it again\n\nimport logging\n\nlogger = logging.getLogger('test')\nlogger.setLevel(logging.DEBUG)\nlogger.info(\"This should be seen, but won't be\")\nlogger.warning(\"This will be seen\")\nlogger.info(\"But this won't be\")\n\n# Now use the root logger\nlogging.info(\"Hi\")\n\nlogger.info(\"Bye!\")\n\nWhat we get out is just\nThis will be seen\nINFO:test:Bye!\nThe order of events are roughly:\n\nLine 3: We make a new logger called test. This logger does not have any handlers. The root logger does have a handler, which prints to standard error, and WARNING or higher by default.\nLine 4: We set the warning level of the test logger to DEBUG.\nLine 5: We send the test logger an INFO message.\n\nIt accepts it, but has no handlers. It makes a log record, and propagates on to the root logger.\nThe root logger has a default level of WARNING, and ignores this message.\n\nLine 6: We sent the test logger a WARNING message.\n\nIt accepts it, but has not handlers. It makes a log record, and propagates on to the root logger.\nThe root logger has a default level of WARNING, so it will handle this message.\nThe root logger’s default formatter just prints the message\n\nLine 7: We sent the test logger an INFO message.\n\nSame as line 5, it passes the LogRecord onto the root logger, and the root logger ignores it.\n\nLine 10: We send the root logger its first message directly (rather than it being passed). Or more precisely, we are calling info in the logging module.\n\nYou can see from the code in Github, this sees we have no root configured yet, so it calls basicConfig().\nFrom the docstring of basicConfig() you can see it does nothing, unless we don’t have handlers for the root logger.\nIn that case, it will add one (a stream), set a basic format string with a bit more information, and not require a level anymore.\n\nLine 12: We send the test logger an INFO message.\n\nIt accepts it, but has no handlers. It propagates it to the root logger.\nThe root logger now accepts any level after the basicConfig(), and has the formatter updated. From the LogRecord, it produces INFO:test:Bye!\n\n\nIf this code is in a Jupyter notebook and you ran in again, you would have different output, precisely because the root logger has already been configured."
  },
  {
    "objectID": "posts/pandas/stats_models_regression.html",
    "href": "posts/pandas/stats_models_regression.html",
    "title": "Stats Models: Linear Regression",
    "section": "",
    "text": "Problem\nEverytime I want to do a linear regression with statsmodels, I have to look up the imports for linear regressions and formulas, so here it is.\nWe want to do a linear regression of Y on X1 (numeric) interacting with a discrete feature category\n\nimport pandas as pd\nimport numpy as np\n\nX_cat1 = np.random.uniform(low=0, high=10, size=100) \nY_cat1 = 3 * X_cat1 + np.random.rand(100)\n\nX_cat2 = np.random.uniform(low=0, high=10, size=100)\nY_cat2 = 2.5 * X_cat2 + 2*np.random.rand(100)\n\nX_cat3 = np.random.uniform(low=0, high=10, size=100)\nY_cat3 = X_cat3 + 0.5*np.random.rand(100)\n\ndata = pd.concat(\n    [\n        pd.DataFrame({'X': X_cat1, 'category': 'cat1', 'Y': Y_cat1}),\n        pd.DataFrame({'X': X_cat2, 'category': 'cat2', 'Y': Y_cat2}),\n        pd.DataFrame({'X': X_cat3, 'category': 'cat3', 'Y': Y_cat3}),\n    ]\n).reset_index(drop=True)\n\ndata\n\n\n\n\n\n\n\n\n\nX\ncategory\nY\n\n\n\n\n0\n9.178447\ncat1\n28.017036\n\n\n1\n5.934363\ncat1\n18.521566\n\n\n2\n9.746314\ncat1\n29.970701\n\n\n3\n3.484075\ncat1\n10.955776\n\n\n4\n1.067588\ncat1\n3.801292\n\n\n...\n...\n...\n...\n\n\n295\n7.024184\ncat3\n7.421889\n\n\n296\n7.752491\ncat3\n8.027998\n\n\n297\n5.215522\ncat3\n5.620281\n\n\n298\n8.841100\ncat3\n8.887814\n\n\n299\n1.441572\ncat3\n1.738866\n\n\n\n\n300 rows × 3 columns\n\n\n\n\n\n# statsmodels imports\nimport statsmodels.formula.api as smf\n\nmodel = smf.ols('Y ~ X:category', data=data).fit()\n\n\nmodel.summary()\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nY\nR-squared:\n0.997\n\n\nModel:\nOLS\nAdj. R-squared:\n0.997\n\n\nMethod:\nLeast Squares\nF-statistic:\n3.569e+04\n\n\nDate:\nSun, 29 Sep 2024\nProb (F-statistic):\n0.00\n\n\nTime:\n19:49:57\nLog-Likelihood:\n-185.90\n\n\nNo. Observations:\n300\nAIC:\n379.8\n\n\nDf Residuals:\n296\nBIC:\n394.6\n\n\nDf Model:\n3\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.6070\n0.049\n12.304\n0.000\n0.510\n0.704\n\n\nX:category[cat1]\n2.9904\n0.010\n288.525\n0.000\n2.970\n3.011\n\n\nX:category[cat2]\n2.5620\n0.011\n238.462\n0.000\n2.541\n2.583\n\n\nX:category[cat3]\n0.9419\n0.012\n80.026\n0.000\n0.919\n0.965\n\n\n\n\n\n\nOmnibus:\n24.061\nDurbin-Watson:\n2.022\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n28.088\n\n\nSkew:\n0.664\nProb(JB):\n7.96e-07\n\n\nKurtosis:\n3.695\nCond. No.\n6.84\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "posts/pandas/apply_across_multiple_columns.html",
    "href": "posts/pandas/apply_across_multiple_columns.html",
    "title": "Pandas: Transform like behavior using data from multiple columns",
    "section": "",
    "text": "The problem\nWe have a couple of different patterns for aggregating data collections:\n\ngroupby([group_cols]).apply: Allows us to group, and then apply an arbitrary function on the group. Resulting dataframe will have a different index (the grouped variable).\ngroupby([group_cols])['column'].transform: Allows us to group, and then apply a transformation on the column data, and return the aggregate result for each row. ie. the index returned will be the same (we preserve rows)\n\nFor example, if we had grades of different houses in a potions class:\n\n\n\n\n\n\n\n\n\n\nname\nhouse\nclass\ngrade\n\n\n\n\n0\nHarry\nGriffindor\npotions\n68\n\n\n1\nHermionie\nGriffindor\npotions\n99\n\n\n2\nRon\nGriffindor\npotions\n34\n\n\n3\nNeville\nGriffindor\npotions\n55\n\n\n4\nDracro\nSlytherin\npotions\n85\n\n\n5\nGoyle\nSlytherin\npotions\n70\n\n\n6\nCrabble\nSlytherin\npotions\n75\n\n\n7\nDaphne\nSlytherin\npotions\n88\n\n\n\n\n\n\n\n\nApply creates a new index:\n\ngrades.groupby('house')['grade'].apply(lambda x: x.mean())\n\nhouse\nGriffindor    64.0\nSlytherin     79.5\nName: grade, dtype: float64\n\n\nWhereas transform will create a new row:\n\ngrades['house_mean'] = grades.groupby('house')['grade'].transform(lambda x: x.mean())\ngrades\n\n\n\n\n\n\n\n\n\nname\nhouse\nclass\ngrade\nhouse_mean\n\n\n\n\n0\nHarry\nGriffindor\npotions\n68\n64.0\n\n\n1\nHermionie\nGriffindor\npotions\n99\n64.0\n\n\n2\nRon\nGriffindor\npotions\n34\n64.0\n\n\n3\nNeville\nGriffindor\npotions\n55\n64.0\n\n\n4\nDracro\nSlytherin\npotions\n85\n79.5\n\n\n5\nGoyle\nSlytherin\npotions\n70\n79.5\n\n\n6\nCrabble\nSlytherin\npotions\n75\n79.5\n\n\n7\nDaphne\nSlytherin\npotions\n88\n79.5\n\n\n\n\n\n\n\n\nThis isn’t super useful, but we could turn it into something like a z-score:\n\ngrades['z_score_within_house'] = grades.groupby('house')['grade'].transform(lambda x: (x-x.mean())/x.std())\ngrades\n\n\n\n\n\n\n\n\n\nname\nhouse\nclass\ngrade\nhouse_mean\nz_score\nz_score_within_house\n\n\n\n\n0\nHarry\nGriffindor\npotions\n68\n64.0\n0.146977\n0.146977\n\n\n1\nHermionie\nGriffindor\npotions\n99\n64.0\n1.286046\n1.286046\n\n\n2\nRon\nGriffindor\npotions\n34\n64.0\n-1.102326\n-1.102326\n\n\n3\nNeville\nGriffindor\npotions\n55\n64.0\n-0.330698\n-0.330698\n\n\n4\nDracro\nSlytherin\npotions\n85\n79.5\n0.652730\n0.652730\n\n\n5\nGoyle\nSlytherin\npotions\n70\n79.5\n-1.127443\n-1.127443\n\n\n6\nCrabble\nSlytherin\npotions\n75\n79.5\n-0.534052\n-0.534052\n\n\n7\nDaphne\nSlytherin\npotions\n88\n79.5\n1.008764\n1.008764"
  },
  {
    "objectID": "posts/pandas/grouper.html",
    "href": "posts/pandas/grouper.html",
    "title": "Pandas: Time-series on multiple columns (grouper)",
    "section": "",
    "text": "This problem is one that seems simple, but the “missing date” problem can make it really tricky. Let’s start with a motivating example. We have some daily data, but some of the days are missing (e.g. we have no sales)\n\nimport pandas as pd\nimport plotnine as p9\n\nsales = pd.DataFrame([\n    {'date': '2024-01-01', 'num_sold': 10},\n    {'date': '2024-01-02', 'num_sold': 12},\n    {'date': '2024-01-03', 'num_sold': 6},\n    {'date': '2024-01-04', 'num_sold': 16},\n    #{'date': '2024-01-05', 'num_sold': 22},\n    {'date': '2024-01-06', 'num_sold': 3},\n    #{'date': '2024-01-07', 'num_sold': 8},\n    #{'date': '2024-01-08', 'num_sold': 12},\n    #{'date': '2024-01-09', 'num_sold': 14},\n    {'date': '2024-01-10', 'num_sold': 9},\n    {'date': '2024-01-11', 'num_sold': 15},\n    {'date': '2024-01-12', 'num_sold': 20},\n    {'date': '2024-01-13', 'num_sold': 5},\n    #{'date': '2024-01-14', 'num_sold': 7},\n    #{'date': '2024-01-15', 'num_sold': 16},\n    {'date': '2024-01-16', 'num_sold': 18},\n    {'date': '2024-01-17', 'num_sold': 10},\n    {'date': '2024-01-18', 'num_sold': 17},\n    {'date': '2024-01-19', 'num_sold': 19},\n    {'date': '2024-01-20', 'num_sold': 6},\n    #{'date': '2024-01-21', 'num_sold': 6},\n    #{'date': '2024-01-22', 'num_sold': 18},\n    {'date': '2024-01-23', 'num_sold': 22},\n    {'date': '2024-01-24', 'num_sold': 12},\n    #{'date': '2024-01-25', 'num_sold': 21},\n    {'date': '2024-01-26', 'num_sold': 22},\n    {'date': '2024-01-27', 'num_sold': 8},\n    #{'date': '2024-01-28', 'num_sold': 5},\n    {'date': '2024-01-29', 'num_sold': 17},\n    {'date': '2024-01-30', 'num_sold': 8},\n    {'date': '2024-01-31', 'num_sold': 11},\n    {'date': '2024-02-01', 'num_sold': 13},\n    {'date': '2024-02-02', 'num_sold': 4},\n])\nsales['date'] = pd.to_datetime(sales['date'])\n\nWe have some sales, with some dates missing (here it is the 5, 7, 8, 9, 14, 15, 21, 22, 25, and 28th of Jan). Maybe the store was closed, or we just didn’t have any sales that day. If we calculated a naive moving average, or a weekly sum, it can be challenging.\n\nsales\n\n\n\n\n\n\n\n\n\ndate\nnum_sold\n\n\n\n\n0\n2024-01-01\n10\n\n\n1\n2024-01-02\n12\n\n\n2\n2024-01-03\n6\n\n\n3\n2024-01-04\n16\n\n\n4\n2024-01-06\n3\n\n\n5\n2024-01-10\n9\n\n\n6\n2024-01-11\n15\n\n\n7\n2024-01-12\n20\n\n\n8\n2024-01-13\n5\n\n\n9\n2024-01-16\n18\n\n\n10\n2024-01-17\n10\n\n\n11\n2024-01-18\n17\n\n\n12\n2024-01-19\n19\n\n\n13\n2024-01-20\n6\n\n\n14\n2024-01-23\n22\n\n\n15\n2024-01-24\n12\n\n\n16\n2024-01-26\n22\n\n\n17\n2024-01-27\n8\n\n\n18\n2024-01-29\n17\n\n\n19\n2024-01-30\n8\n\n\n20\n2024-01-31\n11\n\n\n21\n2024-02-01\n13\n\n\n22\n2024-02-02\n4\n\n\n\n\n\n\n\n\nJust doing a lag of 7 would be completely wrong!\nThere are a couple of ways of doing this. For a single data source, the easiest is to resample on a date index, and then sum:\n\nsales.set_index('date').resample('W')['num_sold'].sum()\n\ndate\n2024-01-07    47\n2024-01-14    49\n2024-01-21    70\n2024-01-28    64\n2024-02-04    53\nFreq: W-SUN, Name: num_sold, dtype: int64\n\n\nWe can see even if we eliminate a complete week that the zero still shows\n\nsales[\n    (sales['date'] &lt; '2024-01-14') | (sales['date'] &gt; '2024-01-21')\n].set_index('date').resample('W')['num_sold'].sum()\n\ndate\n2024-01-07    47\n2024-01-14    49\n2024-01-21     0\n2024-01-28    64\n2024-02-04    53\nFreq: W-SUN, Name: num_sold, dtype: int64\n\n\n\n\nSome plotting software makes seeing the missing dates easier / harder, so be careful. With a default pandas plot, it can be difficult to notice the missing dates:\n\nsales.plot(x='date', y='num_sold')\n\n\n\n\n\n\n\n\nAdding points to the graph (possible in plotnine or using matplotlib) makes it easier to notice.\n\n(\n    p9.ggplot(sales, p9.aes(x='date', y='num_sold'))\n    + p9.geom_line() + p9.geom_point()\n    + p9.scale_x_date(breaks='4 day')\n)\n\n\n\n\n\n\n\n\n\n\n\nSuppose we had two SKUs we wanted to track: sales of apples and oranges. Then the index trick doesn’t work as well. Let’s use a much smaller data frame (from this stackoverflow question)\n\nsimple = pd.DataFrame([\n    {'fruit': 'apple', 'date': '2017-07-11', 'quantity': 20},\n    {'fruit': 'orange', 'date': '2017-07-14', 'quantity': 20},\n    {'fruit': 'apple', 'date': '2017-07-14', 'quantity': 70},\n    {'fruit': 'orange', 'date': '2017-07-25', 'quantity': 40},\n    {'fruit': 'apple', 'date': '2017-07-20', 'quantity': 30},\n])\nsimple['date'] = pd.to_datetime(simple['date'])\nsimple\n\n\n\n\n\n\n\n\n\nfruit\ndate\nquantity\n\n\n\n\n0\napple\n2017-07-11\n20\n\n\n1\norange\n2017-07-14\n20\n\n\n2\napple\n2017-07-14\n70\n\n\n3\norange\n2017-07-25\n40\n\n\n4\napple\n2017-07-20\n30\n\n\n\n\n\n\n\n\nYou could do a pivot on the fruits, and use a similar resample and sum. This does not scale to having hundreds or thousands of different products.\nInstead, you can use a pandas.Grouper on the date. This allows us to transform the column, and then group by the transformed output!\n\n(\n    simple.groupby(['fruit', pd.Grouper(key='date', freq='W-MON')])['quantity']\n    .sum()\n    .reset_index()\n    .sort_values('date')\n)\n\n\n\n\n\n\n\n\n\nfruit\ndate\nquantity\n\n\n\n\n0\napple\n2017-07-17\n90\n\n\n2\norange\n2017-07-17\n20\n\n\n1\napple\n2017-07-24\n30\n\n\n3\norange\n2017-07-31\n40\n\n\n\n\n\n\n\n\n\n\n\n\nsimple2 = pd.DataFrame([\n    {'fruit': 'apple', 'date': '2017-07-11', 'quantity': 20},\n    {'fruit': 'orange', 'date': '2017-07-14', 'quantity': 20},\n    {'fruit': 'apple', 'date': '2017-07-14', 'quantity': 70},\n    {'fruit': 'orange', 'date': '2017-07-25', 'quantity': 40},\n    {'fruit': 'apple', 'date': '2017-07-20', 'quantity': 30},\n    {'fruit': 'apple', 'date': '2017-07-30', 'quantity': 30},\n    {'fruit': 'apple', 'date': '2017-08-05', 'quantity': 30},\n    {'fruit': 'apple', 'date': '2017-08-12', 'quantity': 30},\n])\nsimple2['date'] = pd.to_datetime(simple2['date'])\nsimple2\n\n\n\n\n\n\n\n\n\nfruit\ndate\nquantity\n\n\n\n\n0\napple\n2017-07-11\n20\n\n\n1\norange\n2017-07-14\n20\n\n\n2\napple\n2017-07-14\n70\n\n\n3\norange\n2017-07-25\n40\n\n\n4\napple\n2017-07-20\n30\n\n\n5\napple\n2017-07-30\n30\n\n\n6\napple\n2017-08-05\n30\n\n\n7\napple\n2017-08-12\n30\n\n\n\n\n\n\n\n\n\n(\n    simple2.groupby(['fruit', pd.Grouper(key='date', freq='W-MON')])['quantity']\n    .sum()\n    .reset_index()\n    .sort_values('date')\n)\n\n\n\n\n\n\n\n\n\nfruit\ndate\nquantity\n\n\n\n\n0\napple\n2017-07-17\n90\n\n\n5\norange\n2017-07-17\n20\n\n\n1\napple\n2017-07-24\n30\n\n\n2\napple\n2017-07-31\n30\n\n\n6\norange\n2017-07-31\n40\n\n\n3\napple\n2017-08-07\n30\n\n\n4\napple\n2017-08-14\n30"
  },
  {
    "objectID": "posts/pandas/grouper.html#side-note-plots",
    "href": "posts/pandas/grouper.html#side-note-plots",
    "title": "Pandas: Time-series on multiple columns (grouper)",
    "section": "",
    "text": "Some plotting software makes seeing the missing dates easier / harder, so be careful. With a default pandas plot, it can be difficult to notice the missing dates:\n\nsales.plot(x='date', y='num_sold')\n\n\n\n\n\n\n\n\nAdding points to the graph (possible in plotnine or using matplotlib) makes it easier to notice.\n\n(\n    p9.ggplot(sales, p9.aes(x='date', y='num_sold'))\n    + p9.geom_line() + p9.geom_point()\n    + p9.scale_x_date(breaks='4 day')\n)"
  },
  {
    "objectID": "posts/pandas/grouper.html#making-it-harder",
    "href": "posts/pandas/grouper.html#making-it-harder",
    "title": "Pandas: Time-series on multiple columns (grouper)",
    "section": "",
    "text": "Suppose we had two SKUs we wanted to track: sales of apples and oranges. Then the index trick doesn’t work as well. Let’s use a much smaller data frame (from this stackoverflow question)\n\nsimple = pd.DataFrame([\n    {'fruit': 'apple', 'date': '2017-07-11', 'quantity': 20},\n    {'fruit': 'orange', 'date': '2017-07-14', 'quantity': 20},\n    {'fruit': 'apple', 'date': '2017-07-14', 'quantity': 70},\n    {'fruit': 'orange', 'date': '2017-07-25', 'quantity': 40},\n    {'fruit': 'apple', 'date': '2017-07-20', 'quantity': 30},\n])\nsimple['date'] = pd.to_datetime(simple['date'])\nsimple\n\n\n\n\n\n\n\n\n\nfruit\ndate\nquantity\n\n\n\n\n0\napple\n2017-07-11\n20\n\n\n1\norange\n2017-07-14\n20\n\n\n2\napple\n2017-07-14\n70\n\n\n3\norange\n2017-07-25\n40\n\n\n4\napple\n2017-07-20\n30\n\n\n\n\n\n\n\n\nYou could do a pivot on the fruits, and use a similar resample and sum. This does not scale to having hundreds or thousands of different products.\nInstead, you can use a pandas.Grouper on the date. This allows us to transform the column, and then group by the transformed output!\n\n(\n    simple.groupby(['fruit', pd.Grouper(key='date', freq='W-MON')])['quantity']\n    .sum()\n    .reset_index()\n    .sort_values('date')\n)\n\n\n\n\n\n\n\n\n\nfruit\ndate\nquantity\n\n\n\n\n0\napple\n2017-07-17\n90\n\n\n2\norange\n2017-07-17\n20\n\n\n1\napple\n2017-07-24\n30\n\n\n3\norange\n2017-07-31\n40"
  },
  {
    "objectID": "posts/pandas/grouper.html#how-does-this-handle-missing-dates",
    "href": "posts/pandas/grouper.html#how-does-this-handle-missing-dates",
    "title": "Pandas: Time-series on multiple columns (grouper)",
    "section": "",
    "text": "simple2 = pd.DataFrame([\n    {'fruit': 'apple', 'date': '2017-07-11', 'quantity': 20},\n    {'fruit': 'orange', 'date': '2017-07-14', 'quantity': 20},\n    {'fruit': 'apple', 'date': '2017-07-14', 'quantity': 70},\n    {'fruit': 'orange', 'date': '2017-07-25', 'quantity': 40},\n    {'fruit': 'apple', 'date': '2017-07-20', 'quantity': 30},\n    {'fruit': 'apple', 'date': '2017-07-30', 'quantity': 30},\n    {'fruit': 'apple', 'date': '2017-08-05', 'quantity': 30},\n    {'fruit': 'apple', 'date': '2017-08-12', 'quantity': 30},\n])\nsimple2['date'] = pd.to_datetime(simple2['date'])\nsimple2\n\n\n\n\n\n\n\n\n\nfruit\ndate\nquantity\n\n\n\n\n0\napple\n2017-07-11\n20\n\n\n1\norange\n2017-07-14\n20\n\n\n2\napple\n2017-07-14\n70\n\n\n3\norange\n2017-07-25\n40\n\n\n4\napple\n2017-07-20\n30\n\n\n5\napple\n2017-07-30\n30\n\n\n6\napple\n2017-08-05\n30\n\n\n7\napple\n2017-08-12\n30\n\n\n\n\n\n\n\n\n\n(\n    simple2.groupby(['fruit', pd.Grouper(key='date', freq='W-MON')])['quantity']\n    .sum()\n    .reset_index()\n    .sort_values('date')\n)\n\n\n\n\n\n\n\n\n\nfruit\ndate\nquantity\n\n\n\n\n0\napple\n2017-07-17\n90\n\n\n5\norange\n2017-07-17\n20\n\n\n1\napple\n2017-07-24\n30\n\n\n2\napple\n2017-07-31\n30\n\n\n6\norange\n2017-07-31\n40\n\n\n3\napple\n2017-08-07\n30\n\n\n4\napple\n2017-08-14\n30"
  },
  {
    "objectID": "python-series.html",
    "href": "python-series.html",
    "title": "Series: python and pandas tips",
    "section": "",
    "text": "Logging - How to and best practices\n\n\n\n\n\nGiving some of the best logging practices, and clarifying some of the logging confusion.\n\n\n\n\n\nOct 17, 2024\n\n\n12 min\n\n\n\n\n\n\n\nTiming imports\n\n\n\n\n\nTracing particularly slow imports\n\n\n\n\n\nOct 14, 2024\n\n\n2 min\n\n\n\n\n\n\n\nPublishing Python Packages\n\n\n\n\n\nHow to push packages to artifactory with either setup.py or pyproject.toml\n\n\n\n\n\nMay 27, 2024\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pandas-series.html",
    "href": "pandas-series.html",
    "title": "Series: Pandas tips",
    "section": "",
    "text": "Named aggregations\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2024\n\n\n1 min\n\n\n\n\n\n\n\nPandas: Transform like behavior using data from multiple columns\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2024\n\n\n1 min\n\n\n\n\n\n\n\nTimezones in Pandas\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2024\n\n\n2 min\n\n\n\n\n\n\n\nStats Models: Linear Regression\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n1 min\n\n\n\n\n\n\n\nPandas: Time-series on multiple columns (grouper)\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2024\n\n\n2 min\n\n\n\n\n\n\n\nQuick EDA settings hacks\n\n\n\n\n\nSettings that make notebooks easier to use, especially for EDA\n\n\n\n\n\nMay 1, 2024\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  }
]