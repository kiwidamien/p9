[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "p9",
    "section": "",
    "text": ":::: {.columns}\n\n\nSeries\n\nggplot2-series\nThis series contains a great deal of tips, tricks and packages that you can use to level up your ggplot game.\n\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nThe dice rolls problem\n\n\n\n\n\n\ninterview\n\n\npuzzles\n\n\n\n\n\n\n\n\n\nMay 22, 2024\n\n\n4 min\n\n\n\n\n\n\n\nEnsemble of Confidence Intervals\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\ngallery\n\n\n\nMake a plot with confidence intervals, and infeasible regions shaded out\n\n\n\n\n\nMay 11, 2024\n\n\n1 min\n\n\n\n\n\n\n\nDates in presto\n\n\n\n\n\n\npresto\n\n\nsql\n\n\ndates\n\n\n\nEverytime I go to do dates in presto I have to look up how to do conversions.\n\n\n\n\n\nMay 7, 2024\n\n\n1 min\n\n\n\n\n\n\n\nMaking a custom palette\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\n\nHow to create a custom palette in plotnine\n\n\n\n\n\nMay 2, 2024\n\n\n1 min\n\n\n\n\n\n\n\nMaking a waterfall chart\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\ngallery\n\n\n\nTranslates an example of a waterfall chart from the ggplot flipbook\n\n\n\n\n\nMay 1, 2024\n\n\n1 min\n\n\n\n\n\n\n\nAnnotating single points in datasets\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\n\nHow to annotate single data points in a plot\n\n\n\n\n\nMay 1, 2024\n\n\n1 min\n\n\n\n\n\n\n\nQuick EDA settings hacks\n\n\n\n\n\n\npandas\n\n\nmatplotlib\n\n\nnotebook\n\n\nEDA\n\n\n\nSettings that make notebooks easier to use, especially for EDA\n\n\n\n\n\nMay 1, 2024\n\n\n1 min\n\n\n\n\n\n\n\nSplitting plots into functions\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\n\nShows how to decompose a plot into different functions, with the goal of being able to modularize the code used.\n\n\n\n\n\nApr 29, 2024\n\n\n2 min\n\n\n\n\n\n\n\nShading regions on a plot\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\n\nShowed how to shade a region on a plot\n\n\n\n\n\nApr 29, 2024\n\n\n1 min\n\n\n\n\n\n\n\nChanging labels to percentages\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\n\nCustomizing the labels on a scale is one of the things that is different from ggplot. We use a function to format the label.\n\n\n\n\n\nApr 29, 2024\n\n\n1 min\n\n\n\n\n\n\n\nMaking manual error bars / error regions\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\n\nShows how to add error bars/regions manually to plots, both as a region of uncertainty, and on individual data points.\n\n\n\n\n\nApr 28, 2024\n\n\n1 min\n\n\n\n\n\n\n\nMaking dual bar charts\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\n\nPopulations pyramids often show not just the number of people in each age bracket, but also seggregate it by gender.\n\n\n\n\n\nApr 28, 2024\n\n\n2 min\n\n\n\n\n\n\n\nForcing Variables to be Categorical\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\n\nPlotnine will treat numeric quantities as continuous, and generators continuous legends. We can use ‘factor’ to force the variable to be treated as cateogrical.\n\n\n\n\n\nApr 28, 2024\n\n\n1 min\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nApr 24, 2024\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/interview_questions/dice_rolls.html",
    "href": "posts/interview_questions/dice_rolls.html",
    "title": "The dice rolls problem",
    "section": "",
    "text": "I came across this Interview Cake problem:\n\nYou have a function rand7() that generates a random integer from 1 to 7. Use it to write a function rand5() that generates a random integer from 1 to 5.\nrand7() returns each integer with equal probability. rand5() must also return each integer with equal probability.\n\nThey had a solution, but also had a callout that I thought was … misplaced."
  },
  {
    "objectID": "posts/interview_questions/dice_rolls.html#wrinkle",
    "href": "posts/interview_questions/dice_rolls.html#wrinkle",
    "title": "The dice rolls problem",
    "section": "Wrinkle",
    "text": "Wrinkle\nAs far as I can tell, this pretty much matches the Interview Cake solution.\nThe format of Interview Cake is to give hints, and one of the hints suggested that “Did you know you can do this with only two calls to rand5() (per loop), not 3?”\nThis seemed to be odd, and very prescriptive (i.e. guess the answer I am thinking of) instead of “let’s think of the best answer”. Let’s make the assumption that calls to rand5() are really expensive. Is it really best to minimize the number of calls in the loop?\nAs we saw above, we are expected to run the loop above 1.2 times, and rand5() is called twice per loop. Ergo, the number of expected calls is 2.4. Since this is less than 3 (the minimum we could do with 3 calls per loop is 3 calls.)\nStill, it is interesting to know what 3 calls in a loop would look like:\n\nWe would be generating a number from 0 to \\(5**3 - 1\\), or 125 possible options.\nWe have 125 % 7= 6, so the probability of rejection is 6 / 125; probability of acceptance is (125 - 6)/125\nThe expected number of runs is 125/119 ~ 1.05\n\nThe expected number of calls to rand5() would be 1.05 x 3 = 3.15\nIn general, letting - \\(n\\) be the number of times rand5() is called - \\(N=5^N\\) as the number of options - \\(m = N mod 7\\) as the modulus We have a rejection probability of m/N, an acceptance probability per loop of (N-m)/N, and an expected number of loop executions of N/(N-m). The expected number of calls to rand5() is \\(n N / (N-m) \\leq nN / (N - 6)\\), as 6 is the worst case sceanario for the modulus."
  },
  {
    "objectID": "posts/ggplot2-tips/composing/composing-plots.html",
    "href": "posts/ggplot2-tips/composing/composing-plots.html",
    "title": "Splitting plots into functions",
    "section": "",
    "text": "Problem\nWe want to be able to build up our plots in library functions.\nThis is most useful when we want to annotate a standard graph – by having a function that creates the plot, and then being able to call the function and continue adding things to the plot. Conceptually:\nplot_sales(df)\n# Shows a plot of sales\n\n\n# This doesn't work, but want to highlight a particular month\nplot_sales(df) + p9.geom_rect(\n    mapping=p9.aes(xmin='2024-03-01', xmax='2024-04-01', ymin=float(\"-inf\"), ymax=float(\"inf\"))\n)\n\n\nSolution\nMake a list instead, and add the list.\nThe above example, done correctly, is\nplot_sales(df)\n# Shows a plot of sales\n\n\n# This DOES work\nplot_sales(df) + [\n    p9.geom_rect(mapping=p9.aes(xmin='2024-03-01', xmax='2024-04-01', ymin=float(\"-inf\"), ymax=float(\"inf\")))\n]\n\n\nMinimal Example\n\nimport plotnine as p9\nfrom plotnine.data import mtcars\n\n\noriginal = (\n    p9.ggplot(mtcars, p9.aes(x='mpg', y='hp'))\n    + p9.geom_point()\n)\noriginal\n\n\n\n\n\n\n\n\nLet’s say that this is the type of graph we would normally create (e.g. we could make a function that generates it, and only takes the data frame as input).\nIf we watned to take this same graph and add to it for a particular report, we can add a list of plotnine objects, as shown below:\n\noriginal + [\n    p9.geom_smooth(method='lm', color='blue'),\n    p9.labs(title='Do more powerful cars have worse milage?',\n            subtitle='Simple linear extrapolation', \n            y='horsepower'),\n    p9.theme_bw()\n]\n\n\n\n\n\n\n\n\nYou would not trust that linear extrapolation as it is starting to head below zero. There are more sophisticated smoothing options (e.g. using 'loess' as the method instead of 'lm') but they have additional dependencies. The emphasis here is on plotting.\n\n\nMore useful example - event annotation\nThe example above was a simple example, but not particularly motivating. Let’s look at an example of a conversion rate from emails.\nThis is a graph you produce frequently, so you have a function for it\n\nimport pandas as pd\nimport plotnine as p9\n\nemails = pd.read_csv('email.csv')\nemails['date'] = pd.to_datetime(emails['date'])\nemails.head()\n\n\n\n\n\n\n\n\n\ndate\nrecipients\nclicks\nctr\n\n\n\n\n0\n2024-02-01\n99886\n1456\n0.014577\n\n\n1\n2024-02-02\n100220\n1491\n0.014877\n\n\n2\n2024-02-03\n99637\n1498\n0.015035\n\n\n3\n2024-02-04\n99344\n1543\n0.015532\n\n\n4\n2024-02-05\n100091\n1559\n0.015576\n\n\n\n\n\n\n\n\n\ndef make_ctr_plot(email_df):\n    \"\"\"Creates a plotnine plot of email conversion rates against time\n    \n    email_df contains the following columns\n        - date (as a datetime object)\n        - ctr (click through rate, in the range 0-1)\n    \"\"\"\n    assert 'date' in email_df, 'need a date column'\n    assert 'ctr' in email_df, 'need a ctr column'\n    return (\n        p9.ggplot(email_df, p9.aes(x='date', y='ctr'))\n        + p9.geom_line(color='blue', alpha=0.7)\n        + p9.scale_y_continuous(labels=lambda labs:[f\"{value:.1%}\" for value in labs])\n        + p9.scale_x_date(breaks='2 week')\n        + p9.labs(x=\"\", y=\"Click-Thru Rate\")\n        + p9.theme_bw()\n    )\n\nmake_ctr_plot(emails)\n\n\n\n\n\n\n\n\nWe see in the marketing department materials that they launched a new template on 2024-02-21, which helps explain the increase.\nIt is less clear what happened on April 1st which caused the CTR to drop! Digging in a little bit:\n\n(\n    p9.ggplot(\n        emails.drop('ctr', axis=1).melt(['date']), \n        p9.aes(x='date', y='value', color='variable')\n    )\n    + p9.geom_line()\n)\n\n\n\n\n\n\n\n\nWe see we massively expanded our audience (recipients) and clicks. It makes sense that as you expand the audience, the CTR drops. We still see incremental clicks. But it makes sense that we would want to add that information!\n\nevents = pd.DataFrame([\n    {'date': '2024-02-19', 'label': 'New email launched'},\n    {'date': '2024-04-01', 'label': 'Audience expansion via XYZ.com'}\n])\nevents['date'] = pd.to_datetime(events['date'])\nevents\n\n\n\n\n\n\n\n\n\ndate\nlabel\n\n\n\n\n0\n2024-02-19\nNew email launched\n\n\n1\n2024-04-01\nAudience expansion via XYZ.com\n\n\n\n\n\n\n\n\n\nmake_ctr_plot(emails) + [\n    p9.geom_vline(mapping=p9.aes(xintercept='date'), data=events, linetype='dashed'),\n    p9.geom_text(mapping=p9.aes(x='date', y=0.0165, label='label'), data=events, angle=90, nudge_x=-2),\n    p9.geom_rect(mapping=p9.aes(\n        xmin=pd.to_datetime('2024-04-01'), xmax=emails['date'].max(), ymin=float(\"-inf\"), ymax=float(\"inf\")\n    ), alpha=0.005, fill='cyan'),\n    p9.annotate('text', x='2024-04-15', y=0.0183, label=\"Expanded TAM\")\n]\n\n\n\n\n\n\n\n\nThis way we can annotate graphs to highlight special events, without having to copy all the plotting code."
  },
  {
    "objectID": "posts/ggplot2-tips/making-manual-error-bars.html",
    "href": "posts/ggplot2-tips/making-manual-error-bars.html",
    "title": "Making manual error bars / error regions",
    "section": "",
    "text": "Problem\nThere are lots of examples of being able to do a linear regression, and have plotnine draw the region of uncertainity automatically.\nThere are fewer examples (or they are harder to find) of how to add error bars or error regions that you have already calculated.\n\n\nSolution\nFor error regions, we use geom_ribbon:\np9.geom_ribbon(mapping=p9.aes(ymin='columnname', ymax='columnname'))\nFor error bars, we use geom_errorbar or geom_pointrange:\np9.geom_errorbar(mapping=p9.aes(ymin='columnname', ymax='columnname'))\np9.geom_pointrange(mapping=p9.aes(ymin='columnname', ymax='columnname'))\nThe difference is that the geom_errorbar draws crossbars at the top and bottom, while geom_pointrange only draws the vertical line.\n\n\nExample of a ribbon\nLet’s look at an A/B test, where we are looking for a change in conversion from an email\n\nimport pandas as pd \nimport plotnine as p9\nimport numpy as np\n\n\nemails = pd.read_csv('composing/email.csv')\nemails['date'] = pd.to_datetime(emails['date'])\nemails.head()\n\n\n\n\n\n\n\n\n\ndate\nrecipients\nclicks\nctr\n\n\n\n\n0\n2024-02-01\n99886\n1456\n0.014577\n\n\n1\n2024-02-02\n100220\n1491\n0.014877\n\n\n2\n2024-02-03\n99637\n1498\n0.015035\n\n\n3\n2024-02-04\n99344\n1543\n0.015532\n\n\n4\n2024-02-05\n100091\n1559\n0.015576\n\n\n\n\n\n\n\n\n\nemails['lower'] =  emails['ctr'] - 1.96 * np.sqrt(emails['ctr']*(1-emails['ctr']) / emails['recipients'])\nemails['upper'] =  emails['ctr'] + 1.96 * np.sqrt(emails['ctr']*(1-emails['ctr']) / emails['recipients'])\n\n\n(\n    p9.ggplot(emails, p9.aes(x='date', y='ctr'))\n    + p9.geom_line()\n    + p9.geom_ribbon(mapping=p9.aes(ymin='lower', ymax='upper'), fill='blue', alpha=0.3)\n    + p9.labs(x=\"\", y=\"click-thru-rate\")\n)\n\n\n\n\n\n\n\n\n\n\nExample of error bars\nWe can use the same dataset with points and error bars instead:\n\n(\n    p9.ggplot(emails, p9.aes(x='date', y='ctr'))\n    + p9.geom_point()\n    + p9.geom_errorbar(mapping=p9.aes(ymin='lower', ymax='upper'), alpha=0.3)\n    + p9.labs(x=\"\", y=\"click-thru-rate\")\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/making-boxes-on-ggplots.html",
    "href": "posts/ggplot2-tips/making-boxes-on-ggplots.html",
    "title": "Shading regions on a plot",
    "section": "",
    "text": "Problem\nWe want to shade a region on a plot. Note that matplotlib and altair can have annoyances about finding the “edges” of the range.\n\n\nSolution\n\nUse p9.geom_rect with p9.aes(xmin=..., xmax=..., ymin=..., ymax=...) as its first argument\nUse the alpha argument to make the box semi-transparent\n\n\n\nExample\nWe will make a normal distribution, with the negative region blocked out\n\nimport pandas as pd\nimport plotnine as p9\nimport scipy.stats\nimport numpy as np\n\n\nx_values = np.linspace(-2, 6, 100)\ny_values = scipy.stats.norm(2, 1).pdf(x_values)\n\nshifted_gaussian_df = pd.DataFrame({'x': x_values, 'density': y_values})\nshifted_gaussian_df.head()\n\n\n\n\n\n\n\n\n\nx\ndensity\n\n\n\n\n0\n-2.000000\n0.000134\n\n\n1\n-1.919192\n0.000184\n\n\n2\n-1.838384\n0.000252\n\n\n3\n-1.757576\n0.000343\n\n\n4\n-1.676768\n0.000463\n\n\n\n\n\n\n\n\nLet’s start by just plotting the normal distribution:\n\n(\n    p9.ggplot(shifted_gaussian_df, p9.aes(x='x', y='density'))\n    + p9.geom_line()\n)\n\n\n\n\n\n\n\n\nLet’s shade the area of the plot with \\(x &lt; 0\\)\n\n(\n    p9.ggplot(shifted_gaussian_df, p9.aes(x='x', y='density'))\n    + p9.geom_line()\n    + p9.geom_rect(\n        mapping=p9.aes(xmin=float(\"-inf\"), xmax=0, ymin=float(\"-inf\"), ymax=float(\"inf\")),\n        alpha=0.005, fill='red'\n    )\n    + p9.theme_bw()\n    # Add some text as well\n    + p9.annotate(\"text\", x=-1, y=0.35, label=\"Rejection zone\", color=\"blue\")\n    + p9.labs(x=\"\", y=\"\", title=\"A gaussian distribution faking a decision process\")\n)\n\n\n\n\n\n\n\n\nIf you want the area under the curve instead, it looks a little different:\n\n(\n    p9.ggplot(shifted_gaussian_df, p9.aes(x='x', y='density'))\n    + p9.geom_area(data=shifted_gaussian_df[shifted_gaussian_df['x'] &gt;= 0], fill=\"blue\")\n    + p9.geom_line()\n    + p9.geom_rect(\n        mapping=p9.aes(xmin=float(\"-inf\"), xmax=0, ymin=float(\"-inf\"), ymax=float(\"inf\")),\n        alpha=0.005, fill='red'\n    )\n    + p9.theme_bw()\n    # Add some text as well\n    + p9.annotate(\"text\", x=-1, y=0.35, label=\"Rejection zone\", color=\"blue\")\n    + p9.labs(x=\"\", y=\"\", title=\"A gaussian distribution faking a decision process\")\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/gallery/error_bars_with_shaded_region.html",
    "href": "posts/ggplot2-tips/gallery/error_bars_with_shaded_region.html",
    "title": "Ensemble of Confidence Intervals",
    "section": "",
    "text": "This shows how to create a plot of Monte Carlo’ed confidence intervals. In this particular problem, we know that the parameter we are estimating – earliest time to failure (ETTF) – cannot be greater than the smallest failure time in the data set.\nThe data set shows: - The true value of ETTF as a dashed line - The value of the smallest data point (dot) - The confidence interval - Shades the region of values higher than the smallest value, as an experimenter in this regieme would know this is an infeasible regieme for the ETTF.\nShows how to shade a region, and remove the background grid points / borders / etc.\nThe model in this case is to use data to estimate the ETTF \\(\\Theta\\), where the probability of failure follows the distribution\n\\[ p(t) = \\exp(-(t-\\Theta)) \\text{ if }t &gt; \\Theta,\\text{ 0 otherwise}\\]\n\n\nCode\nimport numpy as np\nimport plotnine as p9\nimport pandas as pd\n\n\nnp.random.seed(26)\n\nTHETA = 10\nN_SIM = 20\nSAMPLE_SIZE = 40\n\nworlds = THETA + np.random.exponential(size=(SAMPLE_SIZE, N_SIM))\nmeans = worlds.mean(axis=0)\nsmallest = worlds.min(axis=0)\n\n# intervals for Theta\nci_lower = (means - 1) - 1.96*1/np.sqrt(SAMPLE_SIZE)\nci_upper = (means - 1) + 1.96*1/np.sqrt(SAMPLE_SIZE)\n\n# How often is THETA in the CI?\nvalue_in_ci =  ((ci_lower&lt;=THETA) & (THETA &lt;= ci_upper)).mean()\n# How often do we some infeasible values?\n# Here means the ci_upper is greater than the lowest value in the data set\nhas_some_infeasible = (ci_upper &gt; smallest).mean()\n# How often is the entire CI infeasible (meaning we know from the data set\n# that the value logically cannot be in the interval)\nentire_ci_infeasible = (ci_lower &gt; smallest).mean()\n\nplot_data = pd.DataFrame({'lower': ci_lower, 'upper': ci_upper, 'smallest': smallest}).reset_index()\ntitle = f\"Confidence Intervals for ETTF (True value = 10 s, sample size is {SAMPLE_SIZE})\"\n(\n    p9.ggplot(\n        plot_data, \n        p9.aes(y='index')\n    )\n    + p9.geom_errorbarh(mapping=p9.aes(xmin='lower', xmax='upper'))\n    + p9.geom_point(mapping=p9.aes(x='smallest'), color='red')\n    + p9.labs(x=\"\", y=\"\", title=title)\n    + p9.geom_vline(xintercept=THETA, linetype=\"dashed\")\n    + p9.theme_bw()\n    + p9.theme(\n        axis_ticks=p9.element_blank(),\n        panel_grid_major=p9.element_blank(),\n        panel_border=p9.element_blank(),\n    )\n    + p9.scale_y_continuous(breaks=[])\n    + p9.geom_rect(\n        mapping=p9.aes(xmax=float('inf'), xmin='smallest', ymin='index-0.5', ymax='index+0.5'), \n        alpha=0.5\n    )\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/annotating-a-single-point.html",
    "href": "posts/ggplot2-tips/annotating-a-single-point.html",
    "title": "Annotating single points in datasets",
    "section": "",
    "text": "We don’t want to annotate all data points, only one or two extraordinary ones."
  },
  {
    "objectID": "posts/ggplot2-tips/annotating-a-single-point.html#alternative",
    "href": "posts/ggplot2-tips/annotating-a-single-point.html#alternative",
    "title": "Annotating single points in datasets",
    "section": "Alternative",
    "text": "Alternative\nThis approach isn’t quite as attractive, but it is a lot less manual (you don’t really have to look at the data and decide exactly where to put the labels).\n\nmtcars['rank'] = mtcars['hp'].rank(ascending=False)\n\n\n(\n    p9.ggplot(mtcars, mapping=p9.aes(x='mpg', y='hp', label='name'))\n    + p9.geom_point()\n    + p9.geom_label(data=mtcars[mtcars['rank']==1], color='orange', va=\"bottom\", ha=\"left\", alpha=0.8)\n    + p9.geom_point(data=mtcars[mtcars['rank']==1], color='orange', size=2)\n)"
  },
  {
    "objectID": "posts/new_blog_post/post.html",
    "href": "posts/new_blog_post/post.html",
    "title": "Quick EDA settings hacks",
    "section": "",
    "text": "import pandas as pd \npd.set_option('display.min_rows', 500)\npd.set_option('display.max_rows', 500)\nSetting to None will also allow you to show all rows.\nI really like the ability to do this on a one-off basis too:\nfrom IPython.display import display \n\nwith pd.option_context('display.max_rows', 100, 'display.max_columns', 10):\n  display(df)"
  },
  {
    "objectID": "posts/new_blog_post/post.html#setting-the-maximum-number-of-rows",
    "href": "posts/new_blog_post/post.html#setting-the-maximum-number-of-rows",
    "title": "Quick EDA settings hacks",
    "section": "",
    "text": "import pandas as pd \npd.set_option('display.min_rows', 500)\npd.set_option('display.max_rows', 500)\nSetting to None will also allow you to show all rows.\nI really like the ability to do this on a one-off basis too:\nfrom IPython.display import display \n\nwith pd.option_context('display.max_rows', 100, 'display.max_columns', 10):\n  display(df)"
  },
  {
    "objectID": "posts/new_blog_post/post.html#retina-display-quality",
    "href": "posts/new_blog_post/post.html#retina-display-quality",
    "title": "Quick EDA settings hacks",
    "section": "Retina display quality",
    "text": "Retina display quality\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format='retina'"
  },
  {
    "objectID": "posts/presto/dates.html",
    "href": "posts/presto/dates.html",
    "title": "Dates in presto",
    "section": "",
    "text": "Common date problems\nConvert a string to a datetime object\nSELECT DATE_PARSE('2020-06-10', '%Y-%m-%d')\nConvert one date format to another\nSELECT DATE_FORMAT(DATE_PARSE('2020-06-10', '%Y-%m-%d'), '%Y%m%d')\nAdd a day"
  },
  {
    "objectID": "posts/ggplot2-tips/waterfall/gallery-waterfall.html",
    "href": "posts/ggplot2-tips/waterfall/gallery-waterfall.html",
    "title": "Making a waterfall chart",
    "section": "",
    "text": "This makes a plot of the milk cow cost per head over year, but makes it a waterfall plot. This is taken from the ggplot flipbook, who in turn took their data from the #TidyTuesday project\n\nimport pandas as pd\nimport plotnine as p9\n\n# Original data available at\n# https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-01-29/milkcow_facts.csv\ncows = pd.read_csv(\"milkcow_facts.csv\").query('year&gt;=2004')\n\n\ncows.head()\n\n\n\n\n\n\n\n\n\nyear\navg_milk_cow_number\nmilk_per_cow\nmilk_production_lbs\navg_price_milk\ndairy_ration\nmilk_feed_price_ratio\nmilk_cow_cost_per_animal\nmilk_volume_to_buy_cow_in_lbs\nalfalfa_hay_price\nslaughter_cow_price\n\n\n\n\n24\n2004.0\n9010000.0\n18960\n1.708320e+11\n0.161\n0.052007\n3.10\n1580\n9813.664596\n95.133333\n0.5266\n\n\n25\n2005.0\n9050000.0\n19550\n1.769310e+11\n0.151\n0.046825\n3.24\n1770\n11721.854305\n102.525000\n0.5394\n\n\n26\n2006.0\n9137000.0\n19895\n1.817820e+11\n0.129\n0.050371\n2.57\n1730\n13410.852713\n107.708333\n0.4908\n\n\n27\n2007.0\n9189000.0\n20204\n1.856540e+11\n0.191\n0.067958\n2.80\n1830\n9581.151832\n130.583333\n0.4951\n\n\n28\n2008.0\n9314000.0\n20397\n1.899780e+11\n0.183\n0.091663\n2.01\n1950\n10655.737705\n161.333333\n0.5144\n\n\n\n\n\n\n\n\n\ncows['milk_cow_cost_per_animal_lag'] = cows['milk_cow_cost_per_animal'].shift(1)\ncows['percent_change'] = cows['milk_cow_cost_per_animal']/cows['milk_cow_cost_per_animal_lag'] - 1\ncows\n\n\n\n\n\n\n\n\n\nyear\navg_milk_cow_number\nmilk_per_cow\nmilk_production_lbs\navg_price_milk\ndairy_ration\nmilk_feed_price_ratio\nmilk_cow_cost_per_animal\nmilk_volume_to_buy_cow_in_lbs\nalfalfa_hay_price\nslaughter_cow_price\nmilk_cow_cost_per_animal_lag\npercent_change\n\n\n\n\n24\n2004.0\n9010000.0\n18960\n1.708320e+11\n0.161\n0.052007\n3.10\n1580\n9813.664596\n95.133333\n0.526600\nNaN\nNaN\n\n\n25\n2005.0\n9050000.0\n19550\n1.769310e+11\n0.151\n0.046825\n3.24\n1770\n11721.854305\n102.525000\n0.539400\n1580.0\n0.120253\n\n\n26\n2006.0\n9137000.0\n19895\n1.817820e+11\n0.129\n0.050371\n2.57\n1730\n13410.852713\n107.708333\n0.490800\n1770.0\n-0.022599\n\n\n27\n2007.0\n9189000.0\n20204\n1.856540e+11\n0.191\n0.067958\n2.80\n1830\n9581.151832\n130.583333\n0.495100\n1730.0\n0.057803\n\n\n28\n2008.0\n9314000.0\n20397\n1.899780e+11\n0.183\n0.091663\n2.01\n1950\n10655.737705\n161.333333\n0.514400\n1830.0\n0.065574\n\n\n29\n2009.0\n9202000.0\n20561\n1.892020e+11\n0.128\n0.072685\n1.78\n1390\n10859.375000\n122.916667\n0.443767\n1950.0\n-0.287179\n\n\n30\n2010.0\n9123000.0\n21142\n1.928770e+11\n0.163\n0.072030\n2.26\n1330\n8159.509202\n116.416667\n0.561000\n1390.0\n-0.043165\n\n\n31\n2011.0\n9199000.0\n21334\n1.962550e+11\n0.201\n0.107560\n1.90\n1420\n7064.676617\n176.083333\n0.683000\n1330.0\n0.067669\n\n\n32\n2012.0\n9237000.0\n21722\n2.006420e+11\n0.185\n0.121500\n1.52\n1430\n7729.729730\n206.083333\n0.777100\n1420.0\n0.007042\n\n\n33\n2013.0\n9224000.0\n21816\n2.012310e+11\n0.201\n0.117092\n1.75\n1380\n6865.671642\n205.830000\n0.775600\n1430.0\n-0.034965\n\n\n34\n2014.0\n9257000.0\n22259\n2.060540e+11\n0.240\n0.095100\n2.54\n1830\n7625.000000\n200.250000\n1.020400\n1380.0\n0.326087\n\n\n\n\n\n\n\n\nStart with a simple barplot to visualize the data\n\n(\n    p9.ggplot(cows)\n    + p9.aes(x='year', y='milk_cow_cost_per_animal')\n    + p9.geom_bar(stat='identity')\n    + p9.scale_x_continuous(breaks=cows.year.tolist())\n)\n\n\n\n\n\n\n\n\nNow transform it into a waterfall chart:\n\n(\n    p9.ggplot(cows)\n    # Main waterfall\n    + p9.aes(\n        xmin='year-0.4', xmax='year + 0.4', \n        ymax='milk_cow_cost_per_animal', ymin='milk_cow_cost_per_animal_lag')\n    + p9.geom_rect(fill='blue', alpha=0.3)\n    + p9.scale_x_continuous(breaks=cows.year.tolist())\n    + p9.geom_col(\n        data=cows[cows.year==2004], \n        mapping=p9.aes(x='year', y='milk_cow_cost_per_animal'), \n        fill='grey'\n    )\n    \n    # Dashed lines between\n    + p9.geom_segment(\n        data = cows[cows.year &lt; 2014], \n        mapping=p9.aes(x='year+0.4', xend='year+0.6', y='milk_cow_cost_per_animal', yend='milk_cow_cost_per_animal'), \n        linetype = \"dashed\", color = \"grey\")\n    + p9.theme_bw(base_family = \"Times\") \n    \n    # The % annotations\n    + p9.geom_text(\n        data=cows.dropna(),\n        mapping = p9.aes(\n            y = 'milk_cow_cost_per_animal', \n            x = 'year', \n            color = (cows.dropna().percent_change &gt; 0),\n            label=[f'{p:.0%}' for p in cows.dropna().percent_change]),  \n        size = 10, \n        nudge_y = [45 if p &gt; 0 else -45 for p in cows.dropna().percent_change],\n        show_legend = False)\n    + p9.scale_color_manual(values = (\"red\", \"grey\"))\n\n    # titles\n    + p9.labs(\n        x=\"\", \n        y=\"Cost per Cow (USD)\", \n        title = \"Cost of milk cows in the United States\", \n        subtitle = \"Per animal cost, 2004-2014\"\n    )\n)\n\n/Users/damienmartin/anaconda3/envs/blog/lib/python3.12/site-packages/plotnine/layer.py:364: PlotnineWarning: geom_rect : Removed 1 rows containing missing values."
  },
  {
    "objectID": "posts/ggplot2-tips/making-categorical-variables.html",
    "href": "posts/ggplot2-tips/making-categorical-variables.html",
    "title": "Forcing Variables to be Categorical",
    "section": "",
    "text": "Problem\nPlotnine assumes that numeric variables are continuous, rather than discrete. When plotting, some attributes require discrete / categorical variables (e.g. shape). Some variables can support either continuous or discrete features (e.g. size, colour), but the legends can be clearer for discrete variables.\nExamples of where a numeric feature is categorical:\n\nCell ids in A/B tests (e.g. cell 1 is control, cell 2 is treatment)\nSKU ids (SKU 542354 and SKU 542355 should not be considered “close”, they are two arbitrary numbers that label the products)\n\nThe example that I will use here is the number of cylinders in a car in the mtcars dataset.\n\n\nSolution\nUse \"factor(variable_name)\" in the athestics, rather than just \"variable_name\".\n\n\nExample\n\nimport plotnine as p9\nfrom plotnine.data import mtcars \n\n# Example without factor, cylinders are discrete but the colour scale is \n# continuous \n(\n    p9.ggplot(mtcars, p9.aes(x='mpg', y='wt', color='cyl'))\n    + p9.geom_point()\n    + p9.theme_bw()\n)\n\n\n\n\nAn example that doesn’t use factor, so the color shows as a gradient\n\n\n\n\nUsing \"factor(cyl)\" to force the integer number of cylinders to be seen as discrete, even though it is a numeric variable.\n\n(\n    p9.ggplot(mtcars, p9.aes(x='mpg', y='wt', color='factor(cyl)'))\n    + p9.geom_point()\n    + p9.theme_bw()\n)\n\n\n\n\nAn example using factor; colours show discretely and are easier to identify\n\n\n\n\n\n(\n    p9.ggplot(mtcars, p9.aes(x='mpg', y='wt', color='factor(cyl)'))\n    + p9.geom_point()\n    + p9.theme_bw()\n    + p9.scale_color_discrete(name=\"Cylinders\")\n)\n\n\n\n\nAn example using factor and renaming the colour scale to something readable"
  },
  {
    "objectID": "posts/ggplot2-tips/making-percentage-labels.html",
    "href": "posts/ggplot2-tips/making-percentage-labels.html",
    "title": "Changing labels to percentages",
    "section": "",
    "text": "Problem\nWe want to apply custom formatting to the labels on an axis.\nThe bad news is that we don’t have a shortcut for common formatters (e.g. percentages), but the good news is that we have a method that allows us a lot of flexibility.\n\n\nSolution\nWe need a function that takes an iterable of labels we have by default, and outputs an iterable of formatted labels (in the same order). For example\ndef percent_formatter(list_of_labels: list[str]) -&gt; list[str]:\n    return [f\"{label:.0%}\" for label in list_of_labels]\nWe can then pass this into one of the scale functions as the labels parameter, for example\np9.scale_x_continuous(labels=percent_formatter)\nBecause the formatters are frequently pretty simple, they are often implemented as lambda functions, rather than standalone functions.\n\n\nExample\nOur example is going to be pretty straightforward – looking at the rating distribution for a single product on Amazon.\n\nimport plotnine as p9\nimport pandas as pd\n\n\nlens_review = pd.DataFrame([\n    {'stars': 5, 'num_customers': 121},\n    {'stars': 4, 'num_customers': 6},\n    {'stars': 3, 'num_customers': 0},\n    {'stars': 2, 'num_customers': 1},\n    {'stars': 1, 'num_customers': 3}\n])\n\nlens_review['frac_customers'] = lens_review['num_customers'] / lens_review['num_customers'].sum()\n\n\n(\n    p9.ggplot(lens_review, p9.aes(x='stars', y='frac_customers'))\n    + p9.geom_bar(stat='identity')\n)\n\n\n\n\n\n\n\n\nLet’s make the y axis formatted as percentages\n\n(\n    p9.ggplot(lens_review, p9.aes(x='stars', y='frac_customers'))\n    + p9.geom_bar(stat='identity')\n    + p9.scale_y_continuous(labels=lambda labels: [f\"{label:.0%}\" for label in labels])\n)\n\n\n\n\n\n\n\n\nWe can also make it more similar to the Amazon reviews by flipping the axes, and removing some of the distracting background\n\n(\n    p9.ggplot(lens_review, p9.aes(x='stars', y='frac_customers'))\n    + p9.geom_bar(stat='identity', width=0.8, fill='orange')\n    + p9.scale_y_continuous(labels=lambda labels: [f\"{label:.0%}\" for label in labels])\n    + p9.scale_x_continuous(labels=lambda labels: [f\"1 star\" if label==1 else f\"{label:.0f} stars\" for label in labels])\n    + p9.coord_flip()\n    + p9.theme_bw()\n    + p9.theme(\n        panel_border = p9.element_blank(),\n        panel_grid = p9.element_blank(),\n    )\n    + p9.labs(x=\"\", y=\"\", title=\"Amazon ratings for lens\")\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/dual_bar/dual_bar.html",
    "href": "posts/ggplot2-tips/dual_bar/dual_bar.html",
    "title": "Making dual bar charts",
    "section": "",
    "text": "Problem\nShow a bar chart in two different directions from the central dividing line (two bars).\nThe typical example is showing the male and female population by age bracket in a population pyramid.\n\n\nSolution\nWe can use geom_rect or geom_tile to create filled rectangle.\n\ngeom_rect takes the coordinates of the upper-left and lower-right corners of the rectangle.\ngeom_fill takes the coordiantes of the center of the rectangle, as well as it’s width and height.\n\n\n\nExample 1\nFrom the US Census we have data on the US population by age bracket and gender. We want to make a population pyramid out of this data. We start by loading the data and converting the data into long-form:\n\nimport pandas as pd\nimport plotnine as p9\n\npop = (\n    pd.read_csv('2022_acs_us_pop.csv')\n    [['age_bracket_label', 'pop_male', 'pop_female']]\n    .rename(\n        columns={'pop_male': 'male', 'pop_female': 'female'}\n    ).melt(\n        'age_bracket_label',\n        var_name='gender',\n        value_name='population'\n    )\n)\npop.head()\n\n\n\n\n\n\n\n\n\nage_bracket_label\ngender\npopulation\n\n\n\n\n0\nUnder 5 years\nmale\n9725644\n\n\n1\n5 to 9 years\nmale\n10210019\n\n\n2\n10 to 14 years\nmale\n10974635\n\n\n3\n15 to 19 years\nmale\n11196816\n\n\n4\n20 to 24 years\nmale\n11400730\n\n\n\n\n\n\n\n\nBefore creating the population pyramid, let’s look at what we get from a using geom_bar (it is a stacked bar chart)\n\n(\n    p9.ggplot(pop, p9.aes(x='age_bracket_label', y='population', fill='gender'))\n    + p9.geom_bar(stat='identity')\n)\n\n\n\n\n\n\n\n\n\npop['signed_pop'] = pop.apply(lambda row:  row.population if row.gender == 'male'  else -row.population, axis=1)\n# We are ending one side of the rectangle on 0,\n# and the other side on 'signed_pop', so the center is \n# half of the signed_pop\npop['center'] = pop['signed_pop'] / 2\n\n\npop.age_bracket_label.unique().tolist()\n\n['Under 5 years',\n '5 to 9 years',\n '10 to 14 years',\n '15 to 19 years',\n '20 to 24 years',\n '25 to 29 years',\n '30 to 34 years',\n '35 to 39 years',\n '40 to 44 years',\n '45 to 49 years',\n '50 to 54 years',\n '55 to 59 years',\n '60 to 64 years',\n '65 to 69 years',\n '70 to 74 years',\n '75 to 79 years',\n '80 to 84 years',\n '85 years and over']\n\n\nThe p9.geom_tile takes an x, y, width and height to draw a filled rectangle. The x and y positions are the locations are the “center” of the rectangle, which is why we have calculated the center attribute.\n\n(\n    p9.ggplot(pop, p9.aes(x='age_bracket_label', y='population', fill='gender'))\n    + p9.geom_tile(mapping=p9.aes(y='center', width=1, height='signed_pop'))\n)\n\n\n\n\n\n\n\n\nWe can rotate the text, but let’s swap the axes. We will also order the categories correctly.\n\n(\n    p9.ggplot(pop, p9.aes(y='age_bracket_label', x='population', fill='gender'))\n    + p9.geom_tile(mapping=p9.aes(x='center', height=1, width='signed_pop'))\n    + p9.labs(y=\"\")\n    # This gets the order the same as they appear in the dataframe\n    # (otherwise is alphabetical)\n    + p9.scale_y_discrete(limits=pop.age_bracket_label.unique().tolist())\n)\n\n\n\n\n\n\n\n\nLet’s also format the population, so we are not showing the female population as negative\n\n(\n    p9.ggplot(pop, p9.aes(y='age_bracket_label', x='population', fill='gender'))\n    + p9.geom_tile(mapping=p9.aes(x='center', height=0.9, width='signed_pop'), alpha=0.6)\n    + p9.labs(y=\"\", title=\"Population Pyramid (US 2022)\", x=\"\")\n    # This gets the order the same as they appear in the dataframe\n    # (otherwise is alphabetical)\n    + p9.scale_y_discrete(limits=pop.age_bracket_label.unique().tolist())\n    + p9.scale_x_continuous(labels=lambda labs: [f\"{abs(l/1e6):.0f} M\" for l in labs])\n    + p9.theme_linedraw()\n    + p9.theme(panel_border=p9.element_blank())\n)\n\n\n\n\n\n\n\n\n\n\nExample 2\nThis example is taken from the ggplot examples of Albert Rapp (the original post is called “How to create diverging bar plots”)."
  },
  {
    "objectID": "posts/ggplot2-tips/palette/making-a-fixed-color-palette.html",
    "href": "posts/ggplot2-tips/palette/making-a-fixed-color-palette.html",
    "title": "Making a custom palette",
    "section": "",
    "text": "Example\nWe are going to use the Kaggle retail sales dataset, and make predictions for Store 1. The modelling is not very sophisticated (no partial pooling amongst the different stores, or use of exogeneous variables). Instead I built two simple models:\n\nA SARIMA model\nA Prophet model (generalized additive components)\n\nThis is just to give a couple of different forecasts to track. The business requirement is that we always want the actuals to be black.\n\nimport pandas as pd\nimport plotnine as p9\n\nsales = pd.read_pickle('single_store_forecasts.pickle')\ntoday = sales.loc[sales['forecast']=='actuals', 'Date'].max()\nsales['is_future'] = sales['Date'] &gt; today\nsales\n\n\n\n\n\n\n\n\n\nDate\nWeekly_Sales\nforecast\nis_future\n\n\n\n\n0\n2010-02-05\n1.643691\nactuals\nFalse\n\n\n45\n2010-02-12\n1.641957\nactuals\nFalse\n\n\n90\n2010-02-19\n1.611968\nactuals\nFalse\n\n\n135\n2010-02-26\n1.409728\nactuals\nFalse\n\n\n180\n2010-03-05\n1.554807\nactuals\nFalse\n\n\n...\n...\n...\n...\n...\n\n\n503\n2013-10-18\n1.585226\nprophet\nTrue\n\n\n504\n2013-10-19\n1.582930\nprophet\nTrue\n\n\n505\n2013-10-20\n1.581724\nprophet\nTrue\n\n\n506\n2013-10-21\n1.581733\nprophet\nTrue\n\n\n507\n2013-10-22\n1.583054\nprophet\nTrue\n\n\n\n\n704 rows × 4 columns\n\n\n\n\nLet’s make the simplest plot out of the box:\n\n(\n    p9.ggplot(\n        sales, \n        p9.aes(x='Date', y='Weekly_Sales', group='forecast', color='forecast')\n    )\n    + p9.geom_line()\n    + p9.labs(x=\"\", y=\"Weekly Sales (in $M)\")\n)\n\n\n\n\n\n\n\n\nWe can make a version that only applies to the actuals:\n\n(\n    p9.ggplot(\n        sales, \n        p9.aes(x='Date', y='Weekly_Sales', group='forecast', color='forecast', linetype='is_future')\n    )\n    + p9.geom_line()\n    + p9.labs(x=\"\", y=\"Weekly Sales (in $M)\")\n    + p9.scale_color_manual(\n        values={\"actuals\": \"black\",}\n    )\n)\n\n/Users/damienmartin/anaconda3/envs/blog/lib/python3.12/site-packages/plotnine/scales/scale_manual.py:44: PlotnineWarning: The palette of scale_color_manual can return a maximum of 1 values. 3 were requested from it.\n/Users/damienmartin/anaconda3/envs/blog/lib/python3.12/site-packages/plotnine/scales/scale_manual.py:44: PlotnineWarning: The palette of scale_color_manual can return a maximum of 1 values. 3 were requested from it.\n\n\n\n\n\n\n\n\n\n\n# We can also be more prescriptive\n(\n    p9.ggplot(\n        sales, \n        p9.aes(x='Date', y='Weekly_Sales', group='forecast', color='forecast', linetype='is_future')\n    )\n    + p9.geom_line(alpha=0.3)\n    + p9.geom_line(data=sales[sales['forecast']=='actuals'], alpha=1)\n    + p9.labs(x=\"\", y=\"Weekly Sales (in $M)\")\n    + p9.scale_color_manual(\n        values={\"actuals\": \"black\", \"SARIMA\": \"red\", \"prophet\": \"blue\"}\n    )\n    + p9.theme_bw()\n    + p9.geom_vline(xintercept=today, linetype='dotted')\n    + p9.annotate('text', x=today, y=2.3, label='Today', nudge_x=-20, angle=90)\n)"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "ggplot-series.html",
    "href": "ggplot-series.html",
    "title": "Series: ggplot2-tips",
    "section": "",
    "text": "Ensemble of Confidence Intervals\n\n\n\n\n\nMake a plot with confidence intervals, and infeasible regions shaded out\n\n\n\n\n\nMay 11, 2024\n\n\n1 min\n\n\n\n\n\n\n\nMaking a custom palette\n\n\n\n\n\nHow to create a custom palette in plotnine\n\n\n\n\n\nMay 2, 2024\n\n\n1 min\n\n\n\n\n\n\n\nMaking a waterfall chart\n\n\n\n\n\nTranslates an example of a waterfall chart from the ggplot flipbook\n\n\n\n\n\nMay 1, 2024\n\n\n1 min\n\n\n\n\n\n\n\nAnnotating single points in datasets\n\n\n\n\n\nHow to annotate single data points in a plot\n\n\n\n\n\nMay 1, 2024\n\n\n1 min\n\n\n\n\n\n\n\nSplitting plots into functions\n\n\n\n\n\nShows how to decompose a plot into different functions, with the goal of being able to modularize the code used.\n\n\n\n\n\nApr 29, 2024\n\n\n2 min\n\n\n\n\n\n\n\nShading regions on a plot\n\n\n\n\n\nShowed how to shade a region on a plot\n\n\n\n\n\nApr 29, 2024\n\n\n1 min\n\n\n\n\n\n\n\nChanging labels to percentages\n\n\n\n\n\nCustomizing the labels on a scale is one of the things that is different from ggplot. We use a function to format the label.\n\n\n\n\n\nApr 29, 2024\n\n\n1 min\n\n\n\n\n\n\n\nMaking manual error bars / error regions\n\n\n\n\n\nShows how to add error bars/regions manually to plots, both as a region of uncertainty, and on individual data points.\n\n\n\n\n\nApr 28, 2024\n\n\n1 min\n\n\n\n\n\n\n\nMaking dual bar charts\n\n\n\n\n\nPopulations pyramids often show not just the number of people in each age bracket, but also seggregate it by gender.\n\n\n\n\n\nApr 28, 2024\n\n\n2 min\n\n\n\n\n\n\n\nForcing Variables to be Categorical\n\n\n\n\n\nPlotnine will treat numeric quantities as continuous, and generators continuous legends. We can use ‘factor’ to force the variable to be treated as cateogrical.\n\n\n\n\n\nApr 28, 2024\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  }
]