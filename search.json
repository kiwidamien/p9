[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "p9",
    "section": "",
    "text": ":::: {.columns}\n\n\nSeries\n\nggplot2-series\nThis series contains a great deal of tips, tricks and packages that you can use to level up your ggplot game.\n\n\npresto-series\nLookup SQL syntax quirks.\n\n\npython-series\nLookup pandas and python package management tricks.\n\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTiming imports\n\n\n\n\n\n\npython\n\n\npackage-management\n\n\n\nTracing particularly slow imports\n\n\n\n\n\nOct 14, 2024\n\n\n2 min\n\n\n\n\n\n\n\nHistograms\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\nhistogram\n\n\n\nMaking simple and relative histograms\n\n\n\n\n\nOct 9, 2024\n\n\n4 min\n\n\n\n\n\n\n\nStats Models: Linear Regression\n\n\n\n\n\n\npandas\n\n\nstatsmodels\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n1 min\n\n\n\n\n\n\n\nPandas: Time-series on multiple columns (grouper)\n\n\n\n\n\n\npandas\n\n\n\n\n\n\n\n\n\nSep 10, 2024\n\n\n2 min\n\n\n\n\n\n\n\nGnatt Charts\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\ncategorical\n\n\n\nMaking a Gnatt chart\n\n\n\n\n\nAug 18, 2024\n\n\n1 min\n\n\n\n\n\n\n\nPandas: Transform like behavior using data from multiple columns\n\n\n\n\n\n\npandas\n\n\n\n\n\n\n\n\n\nJul 3, 2024\n\n\n1 min\n\n\n\n\n\n\n\nChanging the size of plots\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\n\nMaking plots wider\n\n\n\n\n\nMay 27, 2024\n\n\n1 min\n\n\n\n\n\n\n\nDealing with dates on axis\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\n\nDisplaying dates in plots\n\n\n\n\n\nMay 27, 2024\n\n\n2 min\n\n\n\n\n\n\n\nPublishing Python Packages\n\n\n\n\n\n\npython\n\n\npackage-management\n\n\n\nHow to push packages to artifactory with either setup.py or pyproject.toml\n\n\n\n\n\nMay 27, 2024\n\n\n1 min\n\n\n\n\n\n\n\nThe dice rolls problem\n\n\n\n\n\n\ninterview\n\n\npuzzles\n\n\n\n\n\n\n\n\n\nMay 22, 2024\n\n\n5 min\n\n\n\n\n\n\n\nAnnotated area charts with plotnine\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\ngallery\n\n\n\nGallery plot showing clean anntoations and usage of areas\n\n\n\n\n\nMay 11, 2024\n\n\n1 min\n\n\n\n\n\n\n\nEnsemble of Confidence Intervals\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\ngallery\n\n\n\nMake a plot with confidence intervals, and infeasible regions shaded out\n\n\n\n\n\nMay 11, 2024\n\n\n1 min\n\n\n\n\n\n\n\nDates in presto\n\n\n\n\n\n\npresto\n\n\nsql\n\n\ndates\n\n\n\nEverytime I go to do dates in presto I have to look up how to do conversions.\n\n\n\n\n\nMay 7, 2024\n\n\n1 min\n\n\n\n\n\n\n\nMaking a custom palette\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\n\nHow to create a custom palette in plotnine\n\n\n\n\n\nMay 2, 2024\n\n\n1 min\n\n\n\n\n\n\n\nMaking a waterfall chart\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\ngallery\n\n\n\nTranslates an example of a waterfall chart from the ggplot flipbook\n\n\n\n\n\nMay 1, 2024\n\n\n1 min\n\n\n\n\n\n\n\nAnnotating single points in datasets\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\n\nHow to annotate single data points in a plot\n\n\n\n\n\nMay 1, 2024\n\n\n1 min\n\n\n\n\n\n\n\nQuick EDA settings hacks\n\n\n\n\n\n\npandas\n\n\nmatplotlib\n\n\nnotebook\n\n\nEDA\n\n\n\nSettings that make notebooks easier to use, especially for EDA\n\n\n\n\n\nMay 1, 2024\n\n\n1 min\n\n\n\n\n\n\n\nWorking with categories\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\ncategorical\n\n\n\nWorking with categorical data types in plotnine\n\n\n\n\n\nApr 30, 2024\n\n\n1 min\n\n\n\n\n\n\n\nSplitting plots into functions\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\n\nShows how to decompose a plot into different functions, with the goal of being able to modularize the code used.\n\n\n\n\n\nApr 29, 2024\n\n\n2 min\n\n\n\n\n\n\n\nShading regions on a plot\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\n\nShowed how to shade a region on a plot\n\n\n\n\n\nApr 29, 2024\n\n\n1 min\n\n\n\n\n\n\n\nChanging labels to percentages\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\n\nCustomizing the labels on a scale is one of the things that is different from ggplot. We use a function to format the label.\n\n\n\n\n\nApr 29, 2024\n\n\n1 min\n\n\n\n\n\n\n\nMaking manual error bars / error regions\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\n\nShows how to add error bars/regions manually to plots, both as a region of uncertainty, and on individual data points.\n\n\n\n\n\nApr 28, 2024\n\n\n1 min\n\n\n\n\n\n\n\nMaking dual bar charts\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\n\nPopulations pyramids often show not just the number of people in each age bracket, but also seggregate it by gender.\n\n\n\n\n\nApr 28, 2024\n\n\n2 min\n\n\n\n\n\n\n\nForcing Variables to be Categorical\n\n\n\n\n\n\ndata-vis\n\n\nplotnine\n\n\nsnippet\n\n\n\nPlotnine will treat numeric quantities as continuous, and generators continuous legends. We can use ‘factor’ to force the variable to be treated as cateogrical.\n\n\n\n\n\nApr 28, 2024\n\n\n1 min\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nApr 24, 2024\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "ggplot-series.html",
    "href": "ggplot-series.html",
    "title": "Series: ggplot2-tips",
    "section": "",
    "text": "Histograms\n\n\n\n\n\nMaking simple and relative histograms\n\n\n\n\n\nOct 9, 2024\n\n\n4 min\n\n\n\n\n\n\n\nGnatt Charts\n\n\n\n\n\nMaking a Gnatt chart\n\n\n\n\n\nAug 18, 2024\n\n\n1 min\n\n\n\n\n\n\n\nChanging the size of plots\n\n\n\n\n\nMaking plots wider\n\n\n\n\n\nMay 27, 2024\n\n\n1 min\n\n\n\n\n\n\n\nDealing with dates on axis\n\n\n\n\n\nDisplaying dates in plots\n\n\n\n\n\nMay 27, 2024\n\n\n2 min\n\n\n\n\n\n\n\nAnnotated area charts with plotnine\n\n\n\n\n\nGallery plot showing clean anntoations and usage of areas\n\n\n\n\n\nMay 11, 2024\n\n\n1 min\n\n\n\n\n\n\n\nEnsemble of Confidence Intervals\n\n\n\n\n\nMake a plot with confidence intervals, and infeasible regions shaded out\n\n\n\n\n\nMay 11, 2024\n\n\n1 min\n\n\n\n\n\n\n\nMaking a custom palette\n\n\n\n\n\nHow to create a custom palette in plotnine\n\n\n\n\n\nMay 2, 2024\n\n\n1 min\n\n\n\n\n\n\n\nMaking a waterfall chart\n\n\n\n\n\nTranslates an example of a waterfall chart from the ggplot flipbook\n\n\n\n\n\nMay 1, 2024\n\n\n1 min\n\n\n\n\n\n\n\nAnnotating single points in datasets\n\n\n\n\n\nHow to annotate single data points in a plot\n\n\n\n\n\nMay 1, 2024\n\n\n1 min\n\n\n\n\n\n\n\nWorking with categories\n\n\n\n\n\nWorking with categorical data types in plotnine\n\n\n\n\n\nApr 30, 2024\n\n\n1 min\n\n\n\n\n\n\n\nSplitting plots into functions\n\n\n\n\n\nShows how to decompose a plot into different functions, with the goal of being able to modularize the code used.\n\n\n\n\n\nApr 29, 2024\n\n\n2 min\n\n\n\n\n\n\n\nShading regions on a plot\n\n\n\n\n\nShowed how to shade a region on a plot\n\n\n\n\n\nApr 29, 2024\n\n\n1 min\n\n\n\n\n\n\n\nChanging labels to percentages\n\n\n\n\n\nCustomizing the labels on a scale is one of the things that is different from ggplot. We use a function to format the label.\n\n\n\n\n\nApr 29, 2024\n\n\n1 min\n\n\n\n\n\n\n\nMaking manual error bars / error regions\n\n\n\n\n\nShows how to add error bars/regions manually to plots, both as a region of uncertainty, and on individual data points.\n\n\n\n\n\nApr 28, 2024\n\n\n1 min\n\n\n\n\n\n\n\nMaking dual bar charts\n\n\n\n\n\nPopulations pyramids often show not just the number of people in each age bracket, but also seggregate it by gender.\n\n\n\n\n\nApr 28, 2024\n\n\n2 min\n\n\n\n\n\n\n\nForcing Variables to be Categorical\n\n\n\n\n\nPlotnine will treat numeric quantities as continuous, and generators continuous legends. We can use ‘factor’ to force the variable to be treated as cateogrical.\n\n\n\n\n\nApr 28, 2024\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/pandas/apply_across_multiple_columns.html",
    "href": "posts/pandas/apply_across_multiple_columns.html",
    "title": "Pandas: Transform like behavior using data from multiple columns",
    "section": "",
    "text": "The problem\nWe have a couple of different patterns for aggregating data collections:\n\ngroupby([group_cols]).apply: Allows us to group, and then apply an arbitrary function on the group. Resulting dataframe will have a different index (the grouped variable).\ngroupby([group_cols])['column'].transform: Allows us to group, and then apply a transformation on the column data, and return the aggregate result for each row. ie. the index returned will be the same (we preserve rows)\n\nFor example, if we"
  },
  {
    "objectID": "posts/pandas/stats_models_regression.html",
    "href": "posts/pandas/stats_models_regression.html",
    "title": "Stats Models: Linear Regression",
    "section": "",
    "text": "Problem\nEverytime I want to do a linear regression with statsmodels, I have to look up the imports for linear regressions and formulas, so here it is.\nWe want to do a linear regression of Y on X1 (numeric) interacting with a discrete feature category\n\nimport pandas as pd\nimport numpy as np\n\nX_cat1 = np.random.uniform(low=0, high=10, size=100) \nY_cat1 = 3 * X_cat1 + np.random.rand(100)\n\nX_cat2 = np.random.uniform(low=0, high=10, size=100)\nY_cat2 = 2.5 * X_cat2 + 2*np.random.rand(100)\n\nX_cat3 = np.random.uniform(low=0, high=10, size=100)\nY_cat3 = X_cat3 + 0.5*np.random.rand(100)\n\ndata = pd.concat(\n    [\n        pd.DataFrame({'X': X_cat1, 'category': 'cat1', 'Y': Y_cat1}),\n        pd.DataFrame({'X': X_cat2, 'category': 'cat2', 'Y': Y_cat2}),\n        pd.DataFrame({'X': X_cat3, 'category': 'cat3', 'Y': Y_cat3}),\n    ]\n).reset_index(drop=True)\n\ndata\n\n\n\n\n\n\n\n\n\nX\ncategory\nY\n\n\n\n\n0\n9.178447\ncat1\n28.017036\n\n\n1\n5.934363\ncat1\n18.521566\n\n\n2\n9.746314\ncat1\n29.970701\n\n\n3\n3.484075\ncat1\n10.955776\n\n\n4\n1.067588\ncat1\n3.801292\n\n\n...\n...\n...\n...\n\n\n295\n7.024184\ncat3\n7.421889\n\n\n296\n7.752491\ncat3\n8.027998\n\n\n297\n5.215522\ncat3\n5.620281\n\n\n298\n8.841100\ncat3\n8.887814\n\n\n299\n1.441572\ncat3\n1.738866\n\n\n\n\n300 rows × 3 columns\n\n\n\n\n\n# statsmodels imports\nimport statsmodels.formula.api as smf\n\nmodel = smf.ols('Y ~ X:category', data=data).fit()\n\n\nmodel.summary()\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nY\nR-squared:\n0.997\n\n\nModel:\nOLS\nAdj. R-squared:\n0.997\n\n\nMethod:\nLeast Squares\nF-statistic:\n3.569e+04\n\n\nDate:\nSun, 29 Sep 2024\nProb (F-statistic):\n0.00\n\n\nTime:\n19:49:57\nLog-Likelihood:\n-185.90\n\n\nNo. Observations:\n300\nAIC:\n379.8\n\n\nDf Residuals:\n296\nBIC:\n394.6\n\n\nDf Model:\n3\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.6070\n0.049\n12.304\n0.000\n0.510\n0.704\n\n\nX:category[cat1]\n2.9904\n0.010\n288.525\n0.000\n2.970\n3.011\n\n\nX:category[cat2]\n2.5620\n0.011\n238.462\n0.000\n2.541\n2.583\n\n\nX:category[cat3]\n0.9419\n0.012\n80.026\n0.000\n0.919\n0.965\n\n\n\n\n\n\nOmnibus:\n24.061\nDurbin-Watson:\n2.022\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n28.088\n\n\nSkew:\n0.664\nProb(JB):\n7.96e-07\n\n\nKurtosis:\n3.695\nCond. No.\n6.84\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "posts/python-tips/publish_python.html",
    "href": "posts/python-tips/publish_python.html",
    "title": "Publishing Python Packages",
    "section": "",
    "text": "Suppose you have a company python package index with the name our-user-libs (the default is to publish to PYPI).\nThere are two ways to publish to a package index:"
  },
  {
    "objectID": "posts/python-tips/publish_python.html#old-way-setup.py",
    "href": "posts/python-tips/publish_python.html#old-way-setup.py",
    "title": "Publishing Python Packages",
    "section": "Old way (setup.py)",
    "text": "Old way (setup.py)\npython setup.py sdist upload -r our-user-libs"
  },
  {
    "objectID": "posts/python-tips/publish_python.html#new-way-pyproject.toml",
    "href": "posts/python-tips/publish_python.html#new-way-pyproject.toml",
    "title": "Publishing Python Packages",
    "section": "New way (pyproject.toml)",
    "text": "New way (pyproject.toml)\npython -m twine upload -r our-user-libs dist/\nYou’ll need to install the twine package first."
  },
  {
    "objectID": "posts/interview_questions/dice_rolls.html",
    "href": "posts/interview_questions/dice_rolls.html",
    "title": "The dice rolls problem",
    "section": "",
    "text": "I came across this Interview Cake problem:\n\nYou have a function rand7() that generates a random integer from 1 to 7. Use it to write a function rand5() that generates a random integer from 1 to 5.\nrand7() returns each integer with equal probability. rand5() must also return each integer with equal probability.\n\nThey had a solution, but also had a callout that I thought was interesting."
  },
  {
    "objectID": "posts/interview_questions/dice_rolls.html#wrinkle",
    "href": "posts/interview_questions/dice_rolls.html#wrinkle",
    "title": "The dice rolls problem",
    "section": "Wrinkle",
    "text": "Wrinkle\nAs far as I can tell, this pretty much matches the Interview Cake solution.\nThe format of Interview Cake is to give hints, and one of the hints suggested that “Did you know you can do this with only two calls to rand5() (per loop), not 3?”\nThis seemed to be odd, and very prescriptive (i.e. guess the answer I am thinking of) instead of “let’s think of the best answer”. Let’s make the assumption that calls to rand5() are really expensive. Is it really best to minimize the number of calls in the loop?\nAs we saw above, we are expected to run the loop above 1.2 times, and rand5() is called twice per loop. Ergo, the number of expected calls is 2.4. Since this is less than 3 (the minimum we could do with 3 calls per loop is 3 calls.)\nStill, it is interesting to know what 3 calls in a loop would look like:\n\nWe would be generating a number from 0 to \\(5**3 - 1\\), or 125 possible options.\nWe have 125 % 7= 6, so the probability of rejection is 6 / 125; probability of acceptance is (125 - 6)/125\nThe expected number of runs is 125/119 ~ 1.05\n\nThe expected number of calls to rand5() would be 1.05 x 3 = 3.15\nIn general, letting - \\(n\\) be the number of times rand5() is called - \\(N=5^N\\) as the number of options - \\(m = N \\mod 7\\) as the modulus We have a rejection probability of m/N, an acceptance probability per loop of (N-m)/N, and an expected number of loop executions of N/(N-m). The expected number of calls to rand5() is \\(n N / (N-m) \\leq nN / (N - 6)\\) as 6 is the worst case sceanario for the modulus.\nWe can get a rough feel for how the number of calls to rand5() grows as you make each loop more efficent by calling more times in a loop (exchanging a lower rejection rate per loop for more work per loop). The tradeoff is roughly n(1 + 6/N), from expanding the \\(N/(N - 6) = 1/(1-6/N) = (1 + 6/N + \\ldots)\\).\nSo the interview cake solution of only using two rand5() calls was correct:\n\nIt yields an infinite worst case (but so does every solution)\nIt yields a lower minumum number of calls (2), and a lower expected number of calls (2.4) than rolling three\nIt yields a higher variance, though (not shown)"
  },
  {
    "objectID": "posts/interview_questions/dice_rolls.html#going-one-step-deeper",
    "href": "posts/interview_questions/dice_rolls.html#going-one-step-deeper",
    "title": "The dice rolls problem",
    "section": "Going one step deeper",
    "text": "Going one step deeper\nLet’s say that we rand5() really was the long poll in the tent, and we could only call it n times. What would be the probability of a timeout?\n\n\n\nn dice\nP(loops &gt; 1)\nP(loops &gt; 2)\nP(loops &gt; 3)\n\n\n\n\n2\n16.0%\n2.56%\n0.41%\n\n\n3\n4.8%\n0.23%\n0.011%\n\n\n\nSo if we had time to roll up to 6 die, we could do 3 loops of 2 dice (and have a failure rate of 0.41%), or up to two loops of a 3 roll (with a 0.23% failure rate). If we did \\(n=6\\), there would only be a 0.0064% failure rate (cut a lot more guaranteed work per roll)."
  },
  {
    "objectID": "posts/ggplot2-tips/composing/composing-plots.html",
    "href": "posts/ggplot2-tips/composing/composing-plots.html",
    "title": "Splitting plots into functions",
    "section": "",
    "text": "Problem\nWe want to be able to build up our plots in library functions.\nThis is most useful when we want to annotate a standard graph – by having a function that creates the plot, and then being able to call the function and continue adding things to the plot. Conceptually:\nplot_sales(df)\n# Shows a plot of sales\n\n\n# This doesn't work, but want to highlight a particular month\nplot_sales(df) + p9.geom_rect(\n    mapping=p9.aes(xmin='2024-03-01', xmax='2024-04-01', ymin=float(\"-inf\"), ymax=float(\"inf\"))\n)\n\n\nSolution\nMake a list instead, and add the list.\nThe above example, done correctly, is\nplot_sales(df)\n# Shows a plot of sales\n\n\n# This DOES work\nplot_sales(df) + [\n    p9.geom_rect(mapping=p9.aes(xmin='2024-03-01', xmax='2024-04-01', ymin=float(\"-inf\"), ymax=float(\"inf\")))\n]\n\n\nMinimal Example\n\nimport plotnine as p9\nfrom plotnine.data import mtcars\n\n\noriginal = (\n    p9.ggplot(mtcars, p9.aes(x='mpg', y='hp'))\n    + p9.geom_point()\n)\noriginal\n\n\n\n\n\n\n\n\nLet’s say that this is the type of graph we would normally create (e.g. we could make a function that generates it, and only takes the data frame as input).\nIf we watned to take this same graph and add to it for a particular report, we can add a list of plotnine objects, as shown below:\n\noriginal + [\n    p9.geom_smooth(method='lm', color='blue'),\n    p9.labs(title='Do more powerful cars have worse milage?',\n            subtitle='Simple linear extrapolation', \n            y='horsepower'),\n    p9.theme_bw()\n]\n\n\n\n\n\n\n\n\nYou would not trust that linear extrapolation as it is starting to head below zero. There are more sophisticated smoothing options (e.g. using 'loess' as the method instead of 'lm') but they have additional dependencies. The emphasis here is on plotting.\n\n\nMore useful example - event annotation\nThe example above was a simple example, but not particularly motivating. Let’s look at an example of a conversion rate from emails.\nThis is a graph you produce frequently, so you have a function for it\n\nimport pandas as pd\nimport plotnine as p9\n\nemails = pd.read_csv('email.csv')\nemails['date'] = pd.to_datetime(emails['date'])\nemails.head()\n\n\n\n\n\n\n\n\n\ndate\nrecipients\nclicks\nctr\n\n\n\n\n0\n2024-02-01\n99886\n1456\n0.014577\n\n\n1\n2024-02-02\n100220\n1491\n0.014877\n\n\n2\n2024-02-03\n99637\n1498\n0.015035\n\n\n3\n2024-02-04\n99344\n1543\n0.015532\n\n\n4\n2024-02-05\n100091\n1559\n0.015576\n\n\n\n\n\n\n\n\n\ndef make_ctr_plot(email_df):\n    \"\"\"Creates a plotnine plot of email conversion rates against time\n    \n    email_df contains the following columns\n        - date (as a datetime object)\n        - ctr (click through rate, in the range 0-1)\n    \"\"\"\n    assert 'date' in email_df, 'need a date column'\n    assert 'ctr' in email_df, 'need a ctr column'\n    return (\n        p9.ggplot(email_df, p9.aes(x='date', y='ctr'))\n        + p9.geom_line(color='blue', alpha=0.7)\n        + p9.scale_y_continuous(labels=lambda labs:[f\"{value:.1%}\" for value in labs])\n        + p9.scale_x_date(breaks='2 week')\n        + p9.labs(x=\"\", y=\"Click-Thru Rate\")\n        + p9.theme_bw()\n    )\n\nmake_ctr_plot(emails)\n\n\n\n\n\n\n\n\nWe see in the marketing department materials that they launched a new template on 2024-02-21, which helps explain the increase.\nIt is less clear what happened on April 1st which caused the CTR to drop! Digging in a little bit:\n\n(\n    p9.ggplot(\n        emails.drop('ctr', axis=1).melt(['date']), \n        p9.aes(x='date', y='value', color='variable')\n    )\n    + p9.geom_line()\n)\n\n\n\n\n\n\n\n\nWe see we massively expanded our audience (recipients) and clicks. It makes sense that as you expand the audience, the CTR drops. We still see incremental clicks. But it makes sense that we would want to add that information!\n\nevents = pd.DataFrame([\n    {'date': '2024-02-19', 'label': 'New email launched'},\n    {'date': '2024-04-01', 'label': 'Audience expansion via XYZ.com'}\n])\nevents['date'] = pd.to_datetime(events['date'])\nevents\n\n\n\n\n\n\n\n\n\ndate\nlabel\n\n\n\n\n0\n2024-02-19\nNew email launched\n\n\n1\n2024-04-01\nAudience expansion via XYZ.com\n\n\n\n\n\n\n\n\n\nmake_ctr_plot(emails) + [\n    p9.geom_vline(mapping=p9.aes(xintercept='date'), data=events, linetype='dashed'),\n    p9.geom_text(mapping=p9.aes(x='date', y=0.0165, label='label'), data=events, angle=90, nudge_x=-2),\n    p9.geom_rect(mapping=p9.aes(\n        xmin=pd.to_datetime('2024-04-01'), xmax=emails['date'].max(), ymin=float(\"-inf\"), ymax=float(\"inf\")\n    ), alpha=0.005, fill='cyan'),\n    p9.annotate('text', x='2024-04-15', y=0.0183, label=\"Expanded TAM\")\n]\n\n\n\n\n\n\n\n\nThis way we can annotate graphs to highlight special events, without having to copy all the plotting code."
  },
  {
    "objectID": "posts/ggplot2-tips/dropping-categories.html",
    "href": "posts/ggplot2-tips/dropping-categories.html",
    "title": "Working with categories",
    "section": "",
    "text": "When creating a categorical datatype, part of the datatype is knowing all the different possibilities of a category. They will show up by default in plotnine, even if those categories don’t appear in the data. This can be cumbersome to deal with.\nWe will give an example using the mtcars dataset that shows how this can occur."
  },
  {
    "objectID": "posts/ggplot2-tips/dropping-categories.html#loading-data-and-showing-the-problem",
    "href": "posts/ggplot2-tips/dropping-categories.html#loading-data-and-showing-the-problem",
    "title": "Working with categories",
    "section": "Loading data and showing the problem",
    "text": "Loading data and showing the problem\nLet’s start by loading the miles-per-gallon example from plotnine, and converting the manufacturer to a categorical datatype. This might be done to save memory, or may come from some other transformation (e.g. pd.cut).\nWhen plotting all categories, there isn’t an issue.\n\nimport plotnine as p9\nimport pandas as pd\nfrom plotnine.data import mpg\n\nmpg['manufacturer'] = mpg['manufacturer'].astype('category')\nmpg.head()\n\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\ntrans\ndrv\ncty\nhwy\nfl\nclass\n\n\n\n\n0\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\n1\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\n2\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\n3\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\n4\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n\n\n\n\n\n\n\n\n\n# Plot all categories\n(\n    p9.ggplot(mpg, mapping=p9.aes(x='hwy', y='cty', color=\"manufacturer\"))\n    + p9.geom_point()\n)\n\n\n\n\n\n\n\n\nLet’s restrict to some non-US manufacturers:\n\nto_track = [\n    'honda',\n    'hyundai',\n    'nissan',\n    'toyota',\n    'voltsswagen'\n]\nsubset = mpg[mpg['manufacturer'].cat.as_ordered().isin(to_track)].copy()\n\n\n# Plot just these categories\n(\n    p9.ggplot(subset, mapping=p9.aes(x='hwy', y='cty', color=\"manufacturer\"))\n    + p9.geom_point()\n)\n\n\n\n\n\n\n\n\nNote that the legend includes all the unused categories! We would like to eliminate this, especially if it allows more divergent colors.\n\nto_track = [\n    'honda',\n    'hyundai',\n    'nissan',\n    'toyota',\n    'voltsswagen'\n]\nsubset = mpg[mpg['manufacturer'].cat.as_ordered().isin(to_track)].copy()\n# This is the magic line\nsubset['manufacturer'] = subset['manufacturer'].cat.remove_unused_categories()\n\n\n# Plot just these categories\n(\n    p9.ggplot(subset, mapping=p9.aes(x='hwy', y='cty', color=\"manufacturer\"))\n    + p9.geom_point()\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/making-percentage-labels.html",
    "href": "posts/ggplot2-tips/making-percentage-labels.html",
    "title": "Changing labels to percentages",
    "section": "",
    "text": "Problem\nWe want to apply custom formatting to the labels on an axis.\nThe bad news is that we don’t have a shortcut for common formatters (e.g. percentages), but the good news is that we have a method that allows us a lot of flexibility.\n\n\nSolution\nWe need a function that takes an iterable of labels we have by default, and outputs an iterable of formatted labels (in the same order). For example\ndef percent_formatter(list_of_labels: list[str]) -&gt; list[str]:\n    return [f\"{label:.0%}\" for label in list_of_labels]\nWe can then pass this into one of the scale functions as the labels parameter, for example\np9.scale_x_continuous(labels=percent_formatter)\nBecause the formatters are frequently pretty simple, they are often implemented as lambda functions, rather than standalone functions.\n\n\nExample\nOur example is going to be pretty straightforward – looking at the rating distribution for a single product on Amazon.\n\nimport plotnine as p9\nimport pandas as pd\n\n\nlens_review = pd.DataFrame([\n    {'stars': 5, 'num_customers': 121},\n    {'stars': 4, 'num_customers': 6},\n    {'stars': 3, 'num_customers': 0},\n    {'stars': 2, 'num_customers': 1},\n    {'stars': 1, 'num_customers': 3}\n])\n\nlens_review['frac_customers'] = lens_review['num_customers'] / lens_review['num_customers'].sum()\n\n\n(\n    p9.ggplot(lens_review, p9.aes(x='stars', y='frac_customers'))\n    + p9.geom_bar(stat='identity')\n)\n\n\n\n\n\n\n\n\nLet’s make the y axis formatted as percentages\n\n(\n    p9.ggplot(lens_review, p9.aes(x='stars', y='frac_customers'))\n    + p9.geom_bar(stat='identity')\n    + p9.scale_y_continuous(labels=lambda labels: [f\"{label:.0%}\" for label in labels])\n)\n\n\n\n\n\n\n\n\nWe can also make it more similar to the Amazon reviews by flipping the axes, and removing some of the distracting background\n\n(\n    p9.ggplot(lens_review, p9.aes(x='stars', y='frac_customers'))\n    + p9.geom_bar(stat='identity', width=0.8, fill='orange')\n    + p9.scale_y_continuous(labels=lambda labels: [f\"{label:.0%}\" for label in labels])\n    + p9.scale_x_continuous(labels=lambda labels: [f\"1 star\" if label==1 else f\"{label:.0f} stars\" for label in labels])\n    + p9.coord_flip()\n    + p9.theme_bw()\n    + p9.theme(\n        panel_border = p9.element_blank(),\n        panel_grid = p9.element_blank(),\n    )\n    + p9.labs(x=\"\", y=\"\", title=\"Amazon ratings for lens\")\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/datetime-axes.html",
    "href": "posts/ggplot2-tips/datetime-axes.html",
    "title": "Dealing with dates on axis",
    "section": "",
    "text": "Dates are often cumbersome as their format is really long. This post gives a few techniques for formatting that dates a little nicer."
  },
  {
    "objectID": "posts/ggplot2-tips/datetime-axes.html#angled-text",
    "href": "posts/ggplot2-tips/datetime-axes.html#angled-text",
    "title": "Dealing with dates on axis",
    "section": "Angled text",
    "text": "Angled text\n\nUse p9.theme(axis_text_x=p9.element_text(angle=xx))\n\n\n(\n    p9.ggplot(weather_recent, p9.aes(x=\"DATE\")) \n    + p9.geom_line(mapping=p9.aes(y=\"TMAX\"), color=\"red\")\n    + p9.geom_line(mapping=p9.aes(y=\"TMIN\"), color=\"blue\")\n    + p9.labs(\n        title=\"Daily Highs and Lows in Seattle\", \n        y=\"Temperature (F)\", \n        x=\"\"\n    )\n    + p9.scale_x_date(breaks='1 month')\n    + p9.theme(axis_text_x=p9.element_text(angle=60))\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/datetime-axes.html#format-the-date-e.g.-yyyy-mm",
    "href": "posts/ggplot2-tips/datetime-axes.html#format-the-date-e.g.-yyyy-mm",
    "title": "Dealing with dates on axis",
    "section": "Format the date (e.g. YYYY-MM)",
    "text": "Format the date (e.g. YYYY-MM)\n\nUse scale_x_date(date_labels=date_format_string)\n\ndate_format_string defaults to ISO standard: \"%Y-%m-%d\"\n\n\n\n(\n    p9.ggplot(weather_recent, p9.aes(x=\"DATE\")) \n    + p9.geom_line(mapping=p9.aes(y=\"TMAX\"), color=\"red\")\n    + p9.geom_line(mapping=p9.aes(y=\"TMIN\"), color=\"blue\")\n    + p9.labs(\n        title=\"Daily Highs and Lows in Seattle\", \n        y=\"Temperature (F)\", \n        x=\"\"\n    )\n    + p9.scale_x_date(breaks='1 month', date_labels=\"%Y-%m\")\n    + p9.theme(axis_text_x=p9.element_text(angle=60, hjust=1))\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/datetime-axes.html#manually-set-the-labels",
    "href": "posts/ggplot2-tips/datetime-axes.html#manually-set-the-labels",
    "title": "Dealing with dates on axis",
    "section": "Manually set the labels",
    "text": "Manually set the labels\n\nUse the labels argument\nCan use an explicit list, but you give up plotnine determining the breaks for you\nCan use a lambda to take the Timestamp object, and transform it to a string\n\nHere is a way we can do the previous version (setting date_format to \"%Y-%m\").\n\n(\n    p9.ggplot(weather_recent, p9.aes(x=\"DATE\")) \n    + p9.geom_line(mapping=p9.aes(y=\"TMAX\"), color=\"red\")\n    + p9.geom_line(mapping=p9.aes(y=\"TMIN\"), color=\"blue\")\n    + p9.labs(\n        title=\"Daily Highs and Lows in Seattle\", \n        y=\"Temperature (F)\", \n        x=\"\"\n    )\n    + p9.scale_x_date(breaks='1 month', labels=lambda d: [entry.strftime(\"%Y-%m\") for entry in d])\n    + p9.theme(axis_text_x=p9.element_text(angle=60, hjust=1))\n)\n\n\n\n\n\n\n\n\nLet’s do a different version, which only shows the year in January\n\nlabel_func = lambda d: [date.strftime(\"%Y\") if date.month==1 else date.strftime(\"%m\") for date in d]\n\n(\n    p9.ggplot(weather_recent, p9.aes(x=\"DATE\")) \n    + p9.geom_line(mapping=p9.aes(y=\"TMAX\"), color=\"red\")\n    + p9.geom_line(mapping=p9.aes(y=\"TMIN\"), color=\"blue\")\n    + p9.labs(\n        title=\"Daily Highs and Lows in Seattle\", \n        y=\"Temperature (F)\", \n        x=\"\"\n    )\n    + p9.scale_x_date(breaks='1 month', labels=label_func)\n    + p9.theme(axis_text_x=p9.element_text(angle=60))\n)\n\n\n\n\n\n\n\n\nEven better:\n\nlabel_func = lambda d: [date.strftime(\"%Y\\n%b\") if date.month==1 else date.strftime(\"%b\") for date in d]\n\n(\n    p9.ggplot(weather_recent, p9.aes(x=\"DATE\")) \n    + p9.geom_line(mapping=p9.aes(y=\"TMAX\"), color=\"red\")\n    + p9.geom_line(mapping=p9.aes(y=\"TMIN\"), color=\"blue\")\n    + p9.labs(\n        title=\"Daily Highs and Lows in Seattle\", \n        y=\"Temperature (F)\", \n        x=\"\"\n    )\n    + p9.scale_x_date(breaks='2 month', labels=label_func)\n    + p9.theme()\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/datetime-axes.html#make-the-plot-wider",
    "href": "posts/ggplot2-tips/datetime-axes.html#make-the-plot-wider",
    "title": "Dealing with dates on axis",
    "section": "Make the plot wider",
    "text": "Make the plot wider\n\nUse the figure_size=(h, w) argument in p9.theme\n\n\n(\n    p9.ggplot(weather_recent, p9.aes(x=\"DATE\")) \n    + p9.geom_line(mapping=p9.aes(y=\"TMAX\"), color=\"red\")\n    + p9.geom_line(mapping=p9.aes(y=\"TMIN\"), color=\"blue\")\n    + p9.labs(\n        title=\"Daily Highs and Lows in Seattle\", \n        y=\"Temperature (F)\", \n        x=\"\"\n    )\n    + p9.scale_x_date(breaks='1 month', labels=lambda d: [entry.strftime(\"%Y-%m\") for entry in d])\n    + p9.theme(axis_text_x=p9.element_text(angle=60, hjust=1), figure_size=(10, 6))\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/making-categorical-variables.html",
    "href": "posts/ggplot2-tips/making-categorical-variables.html",
    "title": "Forcing Variables to be Categorical",
    "section": "",
    "text": "Problem\nPlotnine assumes that numeric variables are continuous, rather than discrete. When plotting, some attributes require discrete / categorical variables (e.g. shape). Some variables can support either continuous or discrete features (e.g. size, colour), but the legends can be clearer for discrete variables.\nExamples of where a numeric feature is categorical:\n\nCell ids in A/B tests (e.g. cell 1 is control, cell 2 is treatment)\nSKU ids (SKU 542354 and SKU 542355 should not be considered “close”, they are two arbitrary numbers that label the products)\n\nThe example that I will use here is the number of cylinders in a car in the mtcars dataset.\n\n\nSolution\nUse \"factor(variable_name)\" in the athestics, rather than just \"variable_name\".\n\n\nExample\n\nimport plotnine as p9\nfrom plotnine.data import mtcars \n\n# Example without factor, cylinders are discrete but the colour scale is \n# continuous \n(\n    p9.ggplot(mtcars, p9.aes(x='mpg', y='wt', color='cyl'))\n    + p9.geom_point()\n    + p9.theme_bw()\n)\n\n\n\n\nAn example that doesn’t use factor, so the color shows as a gradient\n\n\n\n\nUsing \"factor(cyl)\" to force the integer number of cylinders to be seen as discrete, even though it is a numeric variable.\n\n(\n    p9.ggplot(mtcars, p9.aes(x='mpg', y='wt', color='factor(cyl)'))\n    + p9.geom_point()\n    + p9.theme_bw()\n)\n\n\n\n\nAn example using factor; colours show discretely and are easier to identify\n\n\n\n\n\n(\n    p9.ggplot(mtcars, p9.aes(x='mpg', y='wt', color='factor(cyl)'))\n    + p9.geom_point()\n    + p9.theme_bw()\n    + p9.scale_color_discrete(name=\"Cylinders\")\n)\n\n\n\n\nAn example using factor and renaming the colour scale to something readable"
  },
  {
    "objectID": "posts/ggplot2-tips/gallery/gallery_annotated_area.html",
    "href": "posts/ggplot2-tips/gallery/gallery_annotated_area.html",
    "title": "Annotated area charts with plotnine",
    "section": "",
    "text": "Beautiful plot!\nThis is an amazing plot showing annotations and area plots. It was part of the 2024 Plotnine Gallery Contest.\nOriginal source for the post is here.\nImage from the original blog post is\n\n\n\nchart"
  },
  {
    "objectID": "posts/ggplot2-tips/waterfall/gallery-waterfall.html",
    "href": "posts/ggplot2-tips/waterfall/gallery-waterfall.html",
    "title": "Making a waterfall chart",
    "section": "",
    "text": "This makes a plot of the milk cow cost per head over year, but makes it a waterfall plot. This is taken from the ggplot flipbook, who in turn took their data from the #TidyTuesday project\n\nimport pandas as pd\nimport plotnine as p9\n\n# Original data available at\n# https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-01-29/milkcow_facts.csv\ncows = pd.read_csv(\"milkcow_facts.csv\").query('year&gt;=2004')\n\n\ncows.head()\n\n\n\n\n\n\n\n\n\nyear\navg_milk_cow_number\nmilk_per_cow\nmilk_production_lbs\navg_price_milk\ndairy_ration\nmilk_feed_price_ratio\nmilk_cow_cost_per_animal\nmilk_volume_to_buy_cow_in_lbs\nalfalfa_hay_price\nslaughter_cow_price\n\n\n\n\n24\n2004.0\n9010000.0\n18960\n1.708320e+11\n0.161\n0.052007\n3.10\n1580\n9813.664596\n95.133333\n0.5266\n\n\n25\n2005.0\n9050000.0\n19550\n1.769310e+11\n0.151\n0.046825\n3.24\n1770\n11721.854305\n102.525000\n0.5394\n\n\n26\n2006.0\n9137000.0\n19895\n1.817820e+11\n0.129\n0.050371\n2.57\n1730\n13410.852713\n107.708333\n0.4908\n\n\n27\n2007.0\n9189000.0\n20204\n1.856540e+11\n0.191\n0.067958\n2.80\n1830\n9581.151832\n130.583333\n0.4951\n\n\n28\n2008.0\n9314000.0\n20397\n1.899780e+11\n0.183\n0.091663\n2.01\n1950\n10655.737705\n161.333333\n0.5144\n\n\n\n\n\n\n\n\n\ncows['milk_cow_cost_per_animal_lag'] = cows['milk_cow_cost_per_animal'].shift(1)\ncows['percent_change'] = cows['milk_cow_cost_per_animal']/cows['milk_cow_cost_per_animal_lag'] - 1\ncows\n\n\n\n\n\n\n\n\n\nyear\navg_milk_cow_number\nmilk_per_cow\nmilk_production_lbs\navg_price_milk\ndairy_ration\nmilk_feed_price_ratio\nmilk_cow_cost_per_animal\nmilk_volume_to_buy_cow_in_lbs\nalfalfa_hay_price\nslaughter_cow_price\nmilk_cow_cost_per_animal_lag\npercent_change\n\n\n\n\n24\n2004.0\n9010000.0\n18960\n1.708320e+11\n0.161\n0.052007\n3.10\n1580\n9813.664596\n95.133333\n0.526600\nNaN\nNaN\n\n\n25\n2005.0\n9050000.0\n19550\n1.769310e+11\n0.151\n0.046825\n3.24\n1770\n11721.854305\n102.525000\n0.539400\n1580.0\n0.120253\n\n\n26\n2006.0\n9137000.0\n19895\n1.817820e+11\n0.129\n0.050371\n2.57\n1730\n13410.852713\n107.708333\n0.490800\n1770.0\n-0.022599\n\n\n27\n2007.0\n9189000.0\n20204\n1.856540e+11\n0.191\n0.067958\n2.80\n1830\n9581.151832\n130.583333\n0.495100\n1730.0\n0.057803\n\n\n28\n2008.0\n9314000.0\n20397\n1.899780e+11\n0.183\n0.091663\n2.01\n1950\n10655.737705\n161.333333\n0.514400\n1830.0\n0.065574\n\n\n29\n2009.0\n9202000.0\n20561\n1.892020e+11\n0.128\n0.072685\n1.78\n1390\n10859.375000\n122.916667\n0.443767\n1950.0\n-0.287179\n\n\n30\n2010.0\n9123000.0\n21142\n1.928770e+11\n0.163\n0.072030\n2.26\n1330\n8159.509202\n116.416667\n0.561000\n1390.0\n-0.043165\n\n\n31\n2011.0\n9199000.0\n21334\n1.962550e+11\n0.201\n0.107560\n1.90\n1420\n7064.676617\n176.083333\n0.683000\n1330.0\n0.067669\n\n\n32\n2012.0\n9237000.0\n21722\n2.006420e+11\n0.185\n0.121500\n1.52\n1430\n7729.729730\n206.083333\n0.777100\n1420.0\n0.007042\n\n\n33\n2013.0\n9224000.0\n21816\n2.012310e+11\n0.201\n0.117092\n1.75\n1380\n6865.671642\n205.830000\n0.775600\n1430.0\n-0.034965\n\n\n34\n2014.0\n9257000.0\n22259\n2.060540e+11\n0.240\n0.095100\n2.54\n1830\n7625.000000\n200.250000\n1.020400\n1380.0\n0.326087\n\n\n\n\n\n\n\n\nStart with a simple barplot to visualize the data\n\n(\n    p9.ggplot(cows)\n    + p9.aes(x='year', y='milk_cow_cost_per_animal')\n    + p9.geom_bar(stat='identity')\n    + p9.scale_x_continuous(breaks=cows.year.tolist())\n)\n\n\n\n\n\n\n\n\nNow transform it into a waterfall chart:\n\n(\n    p9.ggplot(cows)\n    # Main waterfall\n    + p9.aes(\n        xmin='year-0.4', xmax='year + 0.4', \n        ymax='milk_cow_cost_per_animal', ymin='milk_cow_cost_per_animal_lag')\n    + p9.geom_rect(fill='blue', alpha=0.3)\n    + p9.scale_x_continuous(breaks=cows.year.tolist())\n    + p9.geom_col(\n        data=cows[cows.year==2004], \n        mapping=p9.aes(x='year', y='milk_cow_cost_per_animal'), \n        fill='grey'\n    )\n    \n    # Dashed lines between\n    + p9.geom_segment(\n        data = cows[cows.year &lt; 2014], \n        mapping=p9.aes(x='year+0.4', xend='year+0.6', y='milk_cow_cost_per_animal', yend='milk_cow_cost_per_animal'), \n        linetype = \"dashed\", color = \"grey\")\n    + p9.theme_bw(base_family = \"Times\") \n    \n    # The % annotations\n    + p9.geom_text(\n        data=cows.dropna(),\n        mapping = p9.aes(\n            y = 'milk_cow_cost_per_animal', \n            x = 'year', \n            color = (cows.dropna().percent_change &gt; 0),\n            label=[f'{p:.0%}' for p in cows.dropna().percent_change]),  \n        size = 10, \n        nudge_y = [45 if p &gt; 0 else -45 for p in cows.dropna().percent_change],\n        show_legend = False)\n    + p9.scale_color_manual(values = (\"red\", \"grey\"))\n\n    # titles\n    + p9.labs(\n        x=\"\", \n        y=\"Cost per Cow (USD)\", \n        title = \"Cost of milk cows in the United States\", \n        subtitle = \"Per animal cost, 2004-2014\"\n    )\n)\n\n/Users/damienmartin/anaconda3/envs/blog/lib/python3.12/site-packages/plotnine/layer.py:364: PlotnineWarning: geom_rect : Removed 1 rows containing missing values."
  },
  {
    "objectID": "posts/ggplot2-tips/histograms.html",
    "href": "posts/ggplot2-tips/histograms.html",
    "title": "Histograms",
    "section": "",
    "text": "For many plotting methods, each row leads to a graphical element on the page. The simplest example is geom_point, where there is a 1:1 correspondence between rows and points on the plot.\nFor histograms, they preprocess the data. By default, they count the number of rows belonging to a bin, and then that group of rows corresponds to a bar. We can control transformations on this data (e.g. making a relative histogram), but need to know some specialized commands to do so."
  },
  {
    "objectID": "posts/ggplot2-tips/histograms.html#eliminate-stacking-solution-1-faceting",
    "href": "posts/ggplot2-tips/histograms.html#eliminate-stacking-solution-1-faceting",
    "title": "Histograms",
    "section": "Eliminate stacking: solution 1 – faceting",
    "text": "Eliminate stacking: solution 1 – faceting\nWe can use a facet plot to eliminate the stacking\n\n(\n    p9.ggplot(penguins.dropna(), p9.aes(x='body_mass_g', fill='sex'))\n    + p9.geom_histogram(binwidth=200)\n    + p9.labs(title='Distribution of penguin weights', subtitle=\"Facet to eliminate stacking\", x='mass (g)')\n    + p9.facet_wrap('sex')\n)\n\n\n\n\n\n\n\n\nThis has the downside that direct comparisons are more difficult e.g. there is a dip in weights, presumably from different species or islands. This dip is at 4 kg for female penguins, and around 4.75 kg for male penguins. It isn’t immediately obvious that these dips are in different places."
  },
  {
    "objectID": "posts/ggplot2-tips/histograms.html#eliminate-stacking-solution-2-overlay",
    "href": "posts/ggplot2-tips/histograms.html#eliminate-stacking-solution-2-overlay",
    "title": "Histograms",
    "section": "Eliminate stacking: solution 2 – overlay",
    "text": "Eliminate stacking: solution 2 – overlay\nWe can use the position=\"identity\" argument to geom_histogram to overlay the graph. By default, the argument to position is \"stack\".\n\n(\n    p9.ggplot(penguins.dropna(), p9.aes(x='body_mass_g', fill='sex'))\n    + p9.geom_histogram(binwidth=200, alpha=0.5, position=\"identity\")\n    + p9.labs(title='Distribution of penguin weights', subtitle=\"Use position='identity' to overlay the graphs\", x='mass (g)')\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/histograms.html#making-relative-histograms",
    "href": "posts/ggplot2-tips/histograms.html#making-relative-histograms",
    "title": "Histograms",
    "section": "Making relative histograms",
    "text": "Making relative histograms\nIf we just want to compare the distributions, we can pass in y='stat(density)' instead. The default for a histogram is 'stat(count)'. In R’s ggplot, this would be written as ..density.. instead.\n\n(\n    p9.ggplot(penguins.dropna(), p9.aes(x='body_mass_g', y='stat(density)', fill='sex'))\n    + p9.geom_histogram(binwidth=200, alpha=0.5, position=\"identity\")\n    + p9.labs(title='Distribution of penguin weights', subtitle=\"Use position='identity' to overlay the graphs\", x='mass (g)')\n    + p9.scale_y_continuous(labels=lambda v: [f\"{vv:.3%}\" for vv in v])\n)\n\n\n\n\n\n\n\n\nThis doesn’t make much of a difference, as the total gender numbers were pretty even:\n\npenguins.dropna().sex.value_counts()\n\nsex\nmale      168\nfemale    165\nName: count, dtype: int64\n\n\nWe can see a more dramatic result if we partition by species instead:\n\n(\n    p9.ggplot(penguins.dropna(), p9.aes(x='body_mass_g', fill='species'))\n    + p9.geom_histogram(binwidth=200, alpha=0.5, position=\"identity\")\n    + p9.labs(title='Distribution of penguin weights', subtitle=\"Count graph\", x='mass (g)')\n)\n\n\n\n\n\n\n\n\n\n(\n    p9.ggplot(penguins.dropna(), p9.aes(x='body_mass_g', y='stat(density)', fill='species'))\n    + p9.geom_histogram(binwidth=200, alpha=0.5, position=\"identity\")\n    + p9.labs(title='Distribution of penguin weights', subtitle=\"Relative Graph\", x='mass (g)')\n    + p9.scale_y_continuous(labels=lambda v: [f\"{vv:.3%}\" for vv in v])\n)\n\n\n\n\n\n\n\n\nIf you were intereseted in the question, “at a given mass, what % of penguins are which species?” you could use position=\"fill\" instead. This scales each bin from 0 to 100%, and tells us how much of the data is in each bin.\n\n(\n    p9.ggplot(penguins.dropna(), p9.aes(x='body_mass_g', y='stat(count)', fill='species'))\n    + p9.geom_histogram(binwidth=200, alpha=0.5, position=\"fill\")\n    + p9.labs(title='Distribution of penguin weights', subtitle=\"Filled Graph\", x='mass (g)')\n    + p9.scale_y_continuous(labels=lambda v: [f\"{vv:.0%}\" for vv in v])\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/histograms.html#putting-counts-above-the-bars",
    "href": "posts/ggplot2-tips/histograms.html#putting-counts-above-the-bars",
    "title": "Histograms",
    "section": "Putting counts above the bars",
    "text": "Putting counts above the bars\nThis is tricker than regular labels, because we are trying to access the result of a computation (if we already have the counts, we could use geom_label to add them automatically). Below is a way of adding when using the identity position using stat_bin. Example inspired from stack overflow.\n\n(\n    p9.ggplot(penguins.dropna(), p9.aes(x='body_mass_g', fill=\"species\"))\n    + p9.geom_histogram(binwidth=200, alpha=0.5, position=\"identity\")\n    + p9.labs(title='Distribution of penguin weights', subtitle=\"Count graph\", x='mass (g)')\n    + p9.stat_bin(binwidth=200, geom=\"text\", mapping=p9.aes(label=\"..count..\", color='species'), position=p9.position_identity(), nudge_y=1)\n)\n\n\n\n\n\n\n\n\nIf we have the slightly simpler use-case of only one species, the position_stack provides a slightly nicer interface. Note that vjust is as a fraction of the bar height (so 1 places right on top of the bar). This defaults at 0.5 (inside the bar). You can still use nudge_y if you want the values to be a constant offset upwards.\n\n(\n    p9.ggplot(penguins.dropna(), p9.aes(x='body_mass_g'))\n    + p9.geom_histogram(binwidth=200, alpha=0.5)\n    + p9.labs(title='Distribution of penguin weights', subtitle=\"Count graph\", x='mass (g)')\n    + p9.stat_bin(binwidth=200, geom=\"text\", mapping=p9.aes(label=\"..count..\"), position=p9.position_stack(vjust=1.1))\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/histograms.html#building-histograms-with-pre-collected-data",
    "href": "posts/ggplot2-tips/histograms.html#building-histograms-with-pre-collected-data",
    "title": "Histograms",
    "section": "Building histograms with pre-collected data",
    "text": "Building histograms with pre-collected data\nWe can imagine that we pull data already binned from some external source (e.g. a database), and we want to plot that, as we only need the summary statistics\n\nprebinned = ( \n    penguins\n    .assign(mass_bin=lambda x: x.body_mass_g//100)\n    .assign(individuals=1)\n    .groupby('mass_bin')\n    ['individuals']\n    .count()\n    .reset_index()\n)\n# our simulation of what we pull\nprebinned.head()\n\n\n\n\n\n\n\n\n\nmass_bin\nindividuals\n\n\n\n\n0\n27.0\n1\n\n\n1\n28.0\n2\n\n\n2\n29.0\n6\n\n\n3\n30.0\n7\n\n\n4\n31.0\n7\n\n\n\n\n\n\n\n\n\n(\n    p9.ggplot(prebinned, p9.aes(x='mass_bin'))\n    + p9.geom_line(mapping=p9.aes(y='individuals'))\n)\n\n\n\n\n\n\n\n\nWe can turn this into a bar chart / histogram in the following way:\n\n(\n    p9.ggplot(prebinned, p9.aes(x='mass_bin', y='individuals'))\n    + p9.geom_bar(stat=\"identity\")\n)"
  },
  {
    "objectID": "presto-series.html",
    "href": "presto-series.html",
    "title": "Series: Presto tips",
    "section": "",
    "text": "Dates in presto\n\n\n\n\n\nEverytime I go to do dates in presto I have to look up how to do conversions.\n\n\n\n\n\nMay 7, 2024\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/presto/dates.html",
    "href": "posts/presto/dates.html",
    "title": "Dates in presto",
    "section": "",
    "text": "Common date problem format problems\nConvert a string to a datetime object\nSELECT DATE_PARSE('2020-06-10', '%Y-%m-%d')\nConvert one date format to another\nSELECT DATE_FORMAT(DATE_PARSE('2020-06-10', '%Y-%m-%d'), '%Y%m%d')\n\n\nCreate a date spine\nWant to do weekly smoothing, but worried that some days might be missing from the date range? Here is a way to generate a date range that you can left join to.\nSELECT\n    *\nFROM UNNEST(\n        SEQUENCE(\n            FROM_ISO8601_DATE('2010-01-20'),\n            FROM_ISO8601_DATE('2010-01-24'),\n            INTERVAL '1' DAY\n        )\n    )\n AS t1(date_array)\n\n\nDate Differences\nSELECT DATE_DIFF('day', DATE('2024-09-16'), DATE('2024-09-20'))\nThis returns 4. Things that can catch you:\n\nHas to be day (not days) for the unit\nThe earlier date goes first (unlike subtraction, where we would start with the later date)"
  },
  {
    "objectID": "posts/ggplot2-tips/annotating-a-single-point.html",
    "href": "posts/ggplot2-tips/annotating-a-single-point.html",
    "title": "Annotating single points in datasets",
    "section": "",
    "text": "We don’t want to annotate all data points, only one or two extraordinary ones."
  },
  {
    "objectID": "posts/ggplot2-tips/annotating-a-single-point.html#alternative",
    "href": "posts/ggplot2-tips/annotating-a-single-point.html#alternative",
    "title": "Annotating single points in datasets",
    "section": "Alternative",
    "text": "Alternative\nThis approach isn’t quite as attractive, but it is a lot less manual (you don’t really have to look at the data and decide exactly where to put the labels).\n\nmtcars['rank'] = mtcars['hp'].rank(ascending=False)\n\n\n(\n    p9.ggplot(mtcars, mapping=p9.aes(x='mpg', y='hp', label='name'))\n    + p9.geom_point()\n    + p9.geom_label(data=mtcars[mtcars['rank']==1], color='orange', va=\"bottom\", ha=\"left\", alpha=0.8)\n    + p9.geom_point(data=mtcars[mtcars['rank']==1], color='orange', size=2)\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/making-plots-wider.html",
    "href": "posts/ggplot2-tips/making-plots-wider.html",
    "title": "Changing the size of plots",
    "section": "",
    "text": "We want to control the size of plots in plotnine."
  },
  {
    "objectID": "posts/ggplot2-tips/making-plots-wider.html#example",
    "href": "posts/ggplot2-tips/making-plots-wider.html#example",
    "title": "Changing the size of plots",
    "section": "Example",
    "text": "Example\nLet’s start with a standard, default plot.\n\nimport plotnine as p9\nfrom plotnine.data import mtcars\n\n\n(\n    p9.ggplot(mtcars, mapping=p9.aes(x=\"wt\", y=\"mpg\", color=\"factor(cyl)\"))\n    + p9.geom_point()\n)\n\n\n\n\n\n\n\n\nNow we can change the title of the legend (to be something a little friendlier) and the figure size\n\n(\n    p9.ggplot(mtcars, mapping=p9.aes(x=\"wt\", y=\"mpg\", color=\"factor(cyl)\"))\n    + p9.geom_point()\n    + p9.scale_color_discrete(name=\"# Cylinders\")\n    + p9.theme(\n        figure_size=(8, 4)\n    )\n)\n\n\n\n\n\n\n\n\nThemes can also place legends inside the box (which can free more space)\n\n(\n    p9.ggplot(mtcars, mapping=p9.aes(x=\"wt\", y=\"mpg\", color=\"factor(cyl)\"))\n    + p9.geom_point()\n    + p9.scale_color_discrete(name=\"# Cylinders\")\n    + p9.theme(\n        figure_size=(8, 4),\n        legend_position=(1,1)\n    )\n)\n\n\n\n\n\n\n\n\nWhen using a theme, place it after any custom theme you want to use\n\n(\n    p9.ggplot(mtcars, mapping=p9.aes(x=\"wt\", y=\"mpg\", color=\"factor(cyl)\"))\n    + p9.geom_point()\n    + p9.scale_color_discrete(name=\"# Cylinders\")\n    + p9.theme_538()\n    + p9.theme(\n        figure_size=(8, 4),\n        legend_position=(1,1)\n    )\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/gallery/error_bars_with_shaded_region.html",
    "href": "posts/ggplot2-tips/gallery/error_bars_with_shaded_region.html",
    "title": "Ensemble of Confidence Intervals",
    "section": "",
    "text": "This shows how to create a plot of Monte Carlo’ed confidence intervals. In this particular problem, we know that the parameter we are estimating – earliest time to failure (ETTF) – cannot be greater than the smallest failure time in the data set.\nThe data set shows: - The true value of ETTF as a dashed line - The value of the smallest data point (dot) - The confidence interval - Shades the region of values higher than the smallest value, as an experimenter in this regieme would know this is an infeasible regieme for the ETTF.\nShows how to shade a region, and remove the background grid points / borders / etc.\nThe model in this case is to use data to estimate the ETTF \\(\\Theta\\), where the probability of failure follows the distribution\n\\[ p(t) = \\exp(-(t-\\Theta)) \\text{ if }t &gt; \\Theta,\\text{ 0 otherwise}\\]\n\n\nCode\nimport numpy as np\nimport plotnine as p9\nimport pandas as pd\n\n\nnp.random.seed(26)\n\nTHETA = 10\nN_SIM = 20\nSAMPLE_SIZE = 40\n\nworlds = THETA + np.random.exponential(size=(SAMPLE_SIZE, N_SIM))\nmeans = worlds.mean(axis=0)\nsmallest = worlds.min(axis=0)\n\n# intervals for Theta\nci_lower = (means - 1) - 1.96*1/np.sqrt(SAMPLE_SIZE)\nci_upper = (means - 1) + 1.96*1/np.sqrt(SAMPLE_SIZE)\n\n# How often is THETA in the CI?\nvalue_in_ci =  ((ci_lower&lt;=THETA) & (THETA &lt;= ci_upper)).mean()\n# How often do we some infeasible values?\n# Here means the ci_upper is greater than the lowest value in the data set\nhas_some_infeasible = (ci_upper &gt; smallest).mean()\n# How often is the entire CI infeasible (meaning we know from the data set\n# that the value logically cannot be in the interval)\nentire_ci_infeasible = (ci_lower &gt; smallest).mean()\n\nplot_data = pd.DataFrame({'lower': ci_lower, 'upper': ci_upper, 'smallest': smallest}).reset_index()\ntitle = f\"Confidence Intervals for ETTF (True value = 10 s, sample size is {SAMPLE_SIZE})\"\n(\n    p9.ggplot(\n        plot_data, \n        p9.aes(y='index')\n    )\n    + p9.geom_errorbarh(mapping=p9.aes(xmin='lower', xmax='upper'))\n    + p9.geom_point(mapping=p9.aes(x='smallest'), color='red')\n    + p9.labs(x=\"\", y=\"\", title=title)\n    + p9.geom_vline(xintercept=THETA, linetype=\"dashed\")\n    + p9.theme_bw()\n    + p9.theme(\n        axis_ticks=p9.element_blank(),\n        panel_grid_major=p9.element_blank(),\n        panel_border=p9.element_blank(),\n    )\n    + p9.scale_y_continuous(breaks=[])\n    + p9.geom_rect(\n        mapping=p9.aes(xmax=float('inf'), xmin='smallest', ymin='index-0.5', ymax='index+0.5'), \n        alpha=0.5\n    )\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/making-boxes-on-ggplots.html",
    "href": "posts/ggplot2-tips/making-boxes-on-ggplots.html",
    "title": "Shading regions on a plot",
    "section": "",
    "text": "Problem\nWe want to shade a region on a plot. Note that matplotlib and altair can have annoyances about finding the “edges” of the range.\n\n\nSolution\n\nUse p9.geom_rect with p9.aes(xmin=..., xmax=..., ymin=..., ymax=...) as its first argument\nUse the alpha argument to make the box semi-transparent\n\n\n\nExample\nWe will make a normal distribution, with the negative region blocked out\n\nimport pandas as pd\nimport plotnine as p9\nimport scipy.stats\nimport numpy as np\n\n\nx_values = np.linspace(-2, 6, 100)\ny_values = scipy.stats.norm(2, 1).pdf(x_values)\n\nshifted_gaussian_df = pd.DataFrame({'x': x_values, 'density': y_values})\nshifted_gaussian_df.head()\n\n\n\n\n\n\n\n\n\nx\ndensity\n\n\n\n\n0\n-2.000000\n0.000134\n\n\n1\n-1.919192\n0.000184\n\n\n2\n-1.838384\n0.000252\n\n\n3\n-1.757576\n0.000343\n\n\n4\n-1.676768\n0.000463\n\n\n\n\n\n\n\n\nLet’s start by just plotting the normal distribution:\n\n(\n    p9.ggplot(shifted_gaussian_df, p9.aes(x='x', y='density'))\n    + p9.geom_line()\n)\n\n\n\n\n\n\n\n\nLet’s shade the area of the plot with \\(x &lt; 0\\)\n\n(\n    p9.ggplot(shifted_gaussian_df, p9.aes(x='x', y='density'))\n    + p9.geom_line()\n    + p9.geom_rect(\n        mapping=p9.aes(xmin=float(\"-inf\"), xmax=0, ymin=float(\"-inf\"), ymax=float(\"inf\")),\n        alpha=0.005, fill='red'\n    )\n    + p9.theme_bw()\n    # Add some text as well\n    + p9.annotate(\"text\", x=-1, y=0.35, label=\"Rejection zone\", color=\"blue\")\n    + p9.labs(x=\"\", y=\"\", title=\"A gaussian distribution faking a decision process\")\n)\n\n\n\n\n\n\n\n\nIf you want the area under the curve instead, it looks a little different:\n\n(\n    p9.ggplot(shifted_gaussian_df, p9.aes(x='x', y='density'))\n    + p9.geom_area(data=shifted_gaussian_df[shifted_gaussian_df['x'] &gt;= 0], fill=\"blue\")\n    + p9.geom_line()\n    + p9.geom_rect(\n        mapping=p9.aes(xmin=float(\"-inf\"), xmax=0, ymin=float(\"-inf\"), ymax=float(\"inf\")),\n        alpha=0.005, fill='red'\n    )\n    + p9.theme_bw()\n    # Add some text as well\n    + p9.annotate(\"text\", x=-1, y=0.35, label=\"Rejection zone\", color=\"blue\")\n    + p9.labs(x=\"\", y=\"\", title=\"A gaussian distribution faking a decision process\")\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/gnattt_charts.html",
    "href": "posts/ggplot2-tips/gnattt_charts.html",
    "title": "Gnatt Charts",
    "section": "",
    "text": "We are going to make a Gnatt chart, which shows projects from different priorities, start-end dates, and names.\nThings we will need to be able to do:\n\nOrder categorical variables\nMake plots treat continuous variables as categorical\nHide legends\n\nThe overall approach will be to use thick lines"
  },
  {
    "objectID": "posts/ggplot2-tips/gnattt_charts.html#first-attempt",
    "href": "posts/ggplot2-tips/gnattt_charts.html#first-attempt",
    "title": "Gnatt Charts",
    "section": "First attempt",
    "text": "First attempt\nSimply create a line graph with thick lines. To do this, we need to have the start and end be in the same column. We will melt the data to do this\n\ngnatt_melted = gnatt.melt([\"name\", \"priority\"], var_name='timestamp_type', value_name='timestamp_val').sort_values('timestamp_val')\ngnatt_melted\n\n\n\n\n\n\n\n\n\nname\npriority\ntimestamp_type\ntimestamp_val\n\n\n\n\n0\ndraft post\n1\nstart\n2024-02-15 00:00:00\n\n\n1\nreview post\n1\nstart\n2024-02-20 00:00:00\n\n\n8\ndraft post\n1\nmid\n2024-02-20 00:00:00\n\n\n9\nreview post\n1\nmid\n2024-02-24 00:00:00\n\n\n4\ndraft post\n1\nend\n2024-02-25 00:00:00\n\n\n2\nprep outreach emails\n2\nstart\n2024-02-26 00:00:00\n\n\n5\nreview post\n1\nend\n2024-02-28 00:00:00\n\n\n10\nprep outreach emails\n2\nmid\n2024-03-01 00:00:00\n\n\n6\nprep outreach emails\n2\nend\n2024-03-05 00:00:00\n\n\n3\npublish\n1\nstart\n2024-03-07 00:00:00\n\n\n11\npublish\n1\nmid\n2024-03-07 12:00:00\n\n\n7\npublish\n1\nend\n2024-03-08 00:00:00\n\n\n\n\n\n\n\n\n\n(\n    p9.ggplot(gnatt_melted, p9.aes(x='timestamp_val', y='name', group='name', color='factor(priority)'))\n    + p9.geom_line(size=10)\n    + p9.scale_x_date(breaks='1 week')\n)\n\n\n\n\n\n\n\n\nProblems with this graph:\n\nOrdering of the y-axis\ny-labels could be moved into the bars\nCan use text labels to replace y text label, and the priority\nRemove the labels on axes as they are not adding value"
  },
  {
    "objectID": "posts/ggplot2-tips/gnattt_charts.html#second-attempt---ordering-categories",
    "href": "posts/ggplot2-tips/gnattt_charts.html#second-attempt---ordering-categories",
    "title": "Gnatt Charts",
    "section": "Second attempt - Ordering categories",
    "text": "Second attempt - Ordering categories\nggplot / plotnine makes arranging categories much harder than it needs to be. The way that I have found that works is to make the names explicitly categorical:\n\ngnatt['label'] = gnatt.apply(lambda row: f\"{row[\"name\"]} (P{row.priority})\", axis=1)\n# Here is the categorical part\ngnatt['name'] = pd.Categorical(gnatt['name'], categories=gnatt.sort_values('start')['name'].tolist()[::-1], ordered=True)\n\ngnatt_melted = gnatt.drop('label', axis=1).melt([\"name\", \"priority\"], var_name='timestamp_type', value_name='timestamp_val').sort_values('timestamp_val')\n\nNow the same plotting code will line the categories up properly:\n\n(\n    p9.ggplot(gnatt_melted, p9.aes(x='timestamp_val', y='name', group='name', color='factor(priority)'))\n    + p9.geom_line(size=10)\n    + p9.scale_x_date(breaks='1 week')\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/gnattt_charts.html#third-attempt-tidying-up",
    "href": "posts/ggplot2-tips/gnattt_charts.html#third-attempt-tidying-up",
    "title": "Gnatt Charts",
    "section": "Third attempt: Tidying up",
    "text": "Third attempt: Tidying up\nNow let’s make some of the other improvements!\n\n(\n    p9.ggplot(gnatt_melted, p9.aes(x='timestamp_val', y='name', group='name', color='factor(priority)'))\n    + p9.geom_line(size=10)\n    + p9.scale_x_date(breaks='1 week')\n    + p9.labs(title='Blog release timeline', x='', y='')\n    + p9.geom_text(data=gnatt, mapping=p9.aes(label='label', x='mid', y='name'), color=\"black\")\n    + p9.scale_color_discrete(guide=None)\n    + p9.scale_y_discrete(labels= lambda v:[\"\" for _ in v]) \n    + p9.theme_bw()\n    + p9.theme(figure_size=(8, 3), )\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/making-manual-error-bars.html",
    "href": "posts/ggplot2-tips/making-manual-error-bars.html",
    "title": "Making manual error bars / error regions",
    "section": "",
    "text": "Problem\nThere are lots of examples of being able to do a linear regression, and have plotnine draw the region of uncertainity automatically.\nThere are fewer examples (or they are harder to find) of how to add error bars or error regions that you have already calculated.\n\n\nSolution\nFor error regions, we use geom_ribbon:\np9.geom_ribbon(mapping=p9.aes(ymin='columnname', ymax='columnname'))\nFor error bars, we use geom_errorbar or geom_pointrange:\np9.geom_errorbar(mapping=p9.aes(ymin='columnname', ymax='columnname'))\np9.geom_pointrange(mapping=p9.aes(ymin='columnname', ymax='columnname'))\nThe difference is that the geom_errorbar draws crossbars at the top and bottom, while geom_pointrange only draws the vertical line.\n\n\nExample of a ribbon\nLet’s look at an A/B test, where we are looking for a change in conversion from an email\n\nimport pandas as pd \nimport plotnine as p9\nimport numpy as np\n\n\nemails = pd.read_csv('composing/email.csv')\nemails['date'] = pd.to_datetime(emails['date'])\nemails.head()\n\n\n\n\n\n\n\n\n\ndate\nrecipients\nclicks\nctr\n\n\n\n\n0\n2024-02-01\n99886\n1456\n0.014577\n\n\n1\n2024-02-02\n100220\n1491\n0.014877\n\n\n2\n2024-02-03\n99637\n1498\n0.015035\n\n\n3\n2024-02-04\n99344\n1543\n0.015532\n\n\n4\n2024-02-05\n100091\n1559\n0.015576\n\n\n\n\n\n\n\n\n\nemails['lower'] =  emails['ctr'] - 1.96 * np.sqrt(emails['ctr']*(1-emails['ctr']) / emails['recipients'])\nemails['upper'] =  emails['ctr'] + 1.96 * np.sqrt(emails['ctr']*(1-emails['ctr']) / emails['recipients'])\n\n\n(\n    p9.ggplot(emails, p9.aes(x='date', y='ctr'))\n    + p9.geom_line()\n    + p9.geom_ribbon(mapping=p9.aes(ymin='lower', ymax='upper'), fill='blue', alpha=0.3)\n    + p9.labs(x=\"\", y=\"click-thru-rate\")\n)\n\n\n\n\n\n\n\n\n\n\nExample of error bars\nWe can use the same dataset with points and error bars instead:\n\n(\n    p9.ggplot(emails, p9.aes(x='date', y='ctr'))\n    + p9.geom_point()\n    + p9.geom_errorbar(mapping=p9.aes(ymin='lower', ymax='upper'), alpha=0.3)\n    + p9.labs(x=\"\", y=\"click-thru-rate\")\n)"
  },
  {
    "objectID": "posts/ggplot2-tips/dual_bar/dual_bar.html",
    "href": "posts/ggplot2-tips/dual_bar/dual_bar.html",
    "title": "Making dual bar charts",
    "section": "",
    "text": "Problem\nShow a bar chart in two different directions from the central dividing line (two bars).\nThe typical example is showing the male and female population by age bracket in a population pyramid.\n\n\nSolution\nWe can use geom_rect or geom_tile to create filled rectangle.\n\ngeom_rect takes the coordinates of the upper-left and lower-right corners of the rectangle.\ngeom_fill takes the coordiantes of the center of the rectangle, as well as it’s width and height.\n\n\n\nExample 1\nFrom the US Census we have data on the US population by age bracket and gender. We want to make a population pyramid out of this data. We start by loading the data and converting the data into long-form:\n\nimport pandas as pd\nimport plotnine as p9\n\npop = (\n    pd.read_csv('2022_acs_us_pop.csv')\n    [['age_bracket_label', 'pop_male', 'pop_female']]\n    .rename(\n        columns={'pop_male': 'male', 'pop_female': 'female'}\n    ).melt(\n        'age_bracket_label',\n        var_name='gender',\n        value_name='population'\n    )\n)\npop.head()\n\n\n\n\n\n\n\n\n\nage_bracket_label\ngender\npopulation\n\n\n\n\n0\nUnder 5 years\nmale\n9725644\n\n\n1\n5 to 9 years\nmale\n10210019\n\n\n2\n10 to 14 years\nmale\n10974635\n\n\n3\n15 to 19 years\nmale\n11196816\n\n\n4\n20 to 24 years\nmale\n11400730\n\n\n\n\n\n\n\n\nBefore creating the population pyramid, let’s look at what we get from a using geom_bar (it is a stacked bar chart)\n\n(\n    p9.ggplot(pop, p9.aes(x='age_bracket_label', y='population', fill='gender'))\n    + p9.geom_bar(stat='identity')\n)\n\n\n\n\n\n\n\n\n\npop['signed_pop'] = pop.apply(lambda row:  row.population if row.gender == 'male'  else -row.population, axis=1)\n# We are ending one side of the rectangle on 0,\n# and the other side on 'signed_pop', so the center is \n# half of the signed_pop\npop['center'] = pop['signed_pop'] / 2\n\n\npop.age_bracket_label.unique().tolist()\n\n['Under 5 years',\n '5 to 9 years',\n '10 to 14 years',\n '15 to 19 years',\n '20 to 24 years',\n '25 to 29 years',\n '30 to 34 years',\n '35 to 39 years',\n '40 to 44 years',\n '45 to 49 years',\n '50 to 54 years',\n '55 to 59 years',\n '60 to 64 years',\n '65 to 69 years',\n '70 to 74 years',\n '75 to 79 years',\n '80 to 84 years',\n '85 years and over']\n\n\nThe p9.geom_tile takes an x, y, width and height to draw a filled rectangle. The x and y positions are the locations are the “center” of the rectangle, which is why we have calculated the center attribute.\n\n(\n    p9.ggplot(pop, p9.aes(x='age_bracket_label', y='population', fill='gender'))\n    + p9.geom_tile(mapping=p9.aes(y='center', width=1, height='signed_pop'))\n)\n\n\n\n\n\n\n\n\nWe can rotate the text, but let’s swap the axes. We will also order the categories correctly.\n\n(\n    p9.ggplot(pop, p9.aes(y='age_bracket_label', x='population', fill='gender'))\n    + p9.geom_tile(mapping=p9.aes(x='center', height=1, width='signed_pop'))\n    + p9.labs(y=\"\")\n    # This gets the order the same as they appear in the dataframe\n    # (otherwise is alphabetical)\n    + p9.scale_y_discrete(limits=pop.age_bracket_label.unique().tolist())\n)\n\n\n\n\n\n\n\n\nLet’s also format the population, so we are not showing the female population as negative\n\n(\n    p9.ggplot(pop, p9.aes(y='age_bracket_label', x='population', fill='gender'))\n    + p9.geom_tile(mapping=p9.aes(x='center', height=0.9, width='signed_pop'), alpha=0.6)\n    + p9.labs(y=\"\", title=\"Population Pyramid (US 2022)\", x=\"\")\n    # This gets the order the same as they appear in the dataframe\n    # (otherwise is alphabetical)\n    + p9.scale_y_discrete(limits=pop.age_bracket_label.unique().tolist())\n    + p9.scale_x_continuous(labels=lambda labs: [f\"{abs(l/1e6):.0f} M\" for l in labs])\n    + p9.theme_linedraw()\n    + p9.theme(panel_border=p9.element_blank())\n)\n\n\n\n\n\n\n\n\n\n\nExample 2\nThis example is taken from the ggplot examples of Albert Rapp (the original post is called “How to create diverging bar plots”)."
  },
  {
    "objectID": "posts/ggplot2-tips/palette/making-a-fixed-color-palette.html",
    "href": "posts/ggplot2-tips/palette/making-a-fixed-color-palette.html",
    "title": "Making a custom palette",
    "section": "",
    "text": "Example\nWe are going to use the Kaggle retail sales dataset, and make predictions for Store 1. The modelling is not very sophisticated (no partial pooling amongst the different stores, or use of exogeneous variables). Instead I built two simple models:\n\nA SARIMA model\nA Prophet model (generalized additive components)\n\nThis is just to give a couple of different forecasts to track. The business requirement is that we always want the actuals to be black.\n\nimport pandas as pd\nimport plotnine as p9\n\nsales = pd.read_pickle('single_store_forecasts.pickle')\ntoday = sales.loc[sales['forecast']=='actuals', 'Date'].max()\nsales['is_future'] = sales['Date'] &gt; today\nsales\n\n\n\n\n\n\n\n\n\nDate\nWeekly_Sales\nforecast\nis_future\n\n\n\n\n0\n2010-02-05\n1.643691\nactuals\nFalse\n\n\n45\n2010-02-12\n1.641957\nactuals\nFalse\n\n\n90\n2010-02-19\n1.611968\nactuals\nFalse\n\n\n135\n2010-02-26\n1.409728\nactuals\nFalse\n\n\n180\n2010-03-05\n1.554807\nactuals\nFalse\n\n\n...\n...\n...\n...\n...\n\n\n503\n2013-10-18\n1.585226\nprophet\nTrue\n\n\n504\n2013-10-19\n1.582930\nprophet\nTrue\n\n\n505\n2013-10-20\n1.581724\nprophet\nTrue\n\n\n506\n2013-10-21\n1.581733\nprophet\nTrue\n\n\n507\n2013-10-22\n1.583054\nprophet\nTrue\n\n\n\n\n704 rows × 4 columns\n\n\n\n\nLet’s make the simplest plot out of the box:\n\n(\n    p9.ggplot(\n        sales, \n        p9.aes(x='Date', y='Weekly_Sales', group='forecast', color='forecast')\n    )\n    + p9.geom_line()\n    + p9.labs(x=\"\", y=\"Weekly Sales (in $M)\")\n)\n\n\n\n\n\n\n\n\nWe can make a version that only applies to the actuals:\n\n(\n    p9.ggplot(\n        sales, \n        p9.aes(x='Date', y='Weekly_Sales', group='forecast', color='forecast', linetype='is_future')\n    )\n    + p9.geom_line()\n    + p9.labs(x=\"\", y=\"Weekly Sales (in $M)\")\n    + p9.scale_color_manual(\n        values={\"actuals\": \"black\",}\n    )\n)\n\n/Users/damienmartin/anaconda3/envs/blog/lib/python3.12/site-packages/plotnine/scales/scale_manual.py:44: PlotnineWarning: The palette of scale_color_manual can return a maximum of 1 values. 3 were requested from it.\n/Users/damienmartin/anaconda3/envs/blog/lib/python3.12/site-packages/plotnine/scales/scale_manual.py:44: PlotnineWarning: The palette of scale_color_manual can return a maximum of 1 values. 3 were requested from it.\n\n\n\n\n\n\n\n\n\n\n# We can also be more prescriptive\n(\n    p9.ggplot(\n        sales, \n        p9.aes(x='Date', y='Weekly_Sales', group='forecast', color='forecast', linetype='is_future')\n    )\n    + p9.geom_line(alpha=0.3)\n    + p9.geom_line(data=sales[sales['forecast']=='actuals'], alpha=1)\n    + p9.labs(x=\"\", y=\"Weekly Sales (in $M)\")\n    + p9.scale_color_manual(\n        values={\"actuals\": \"black\", \"SARIMA\": \"red\", \"prophet\": \"blue\"}\n    )\n    + p9.theme_bw()\n    + p9.geom_vline(xintercept=today, linetype='dotted')\n    + p9.annotate('text', x=today, y=2.3, label='Today', nudge_x=-20, angle=90)\n)"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/python-tips/timing_imports.html",
    "href": "posts/python-tips/timing_imports.html",
    "title": "Timing imports",
    "section": "",
    "text": "When importing particular packages, they are importing slowly. You’d like to find why, and if there is a way you can refactor your code / your team’s code to improve import speed.\n\n\nToo much is being done at import time\nImporting should typically not “do work” (e.g. it should not load datafiles, connect to databases, instantiate drivers, etc). The biggest exception to this is probably setting up loggers, because the first caller to logging.basicConfig can change what all the other loggers can see.\nBecause logging is a cross-cutting concern, loggers are also generally made as singletons at the module level.\nThe reason that imports can be so slow is if we have a package structure like\nworld/\n│\n├── africa/\n│   ├── __init__.py\n│   └── zimbabwe.py\n│\n├── europe/\n│   ├── __init__.py\n│   ├── greece.py\n│   ├── norway.py\n│   └── spain.py\n│\n└── __init__.py\nand you write import world.europe.spain into Python, then\n\nFirst world.__init__.py gets imported (if it hasn’t been imported already)\nThen world.europe.__init__.py gets imported (if it hasn’t been imported already)\nFinally, world.europe.spain.py gets imported\n\nIf you did import world.europe.greece after importing spain, then only world.europe.greece.py would be imported on this second call (each module is only imported once).\nBut this means if your __init__.py files anywhere in the tree “do work”, then every subpackage that you import is also going to be slow. This is one of the reasons for recommending that __init__.py files are kept relatively sparse."
  },
  {
    "objectID": "posts/python-tips/timing_imports.html#what-slow-imports-usually-indicate",
    "href": "posts/python-tips/timing_imports.html#what-slow-imports-usually-indicate",
    "title": "Timing imports",
    "section": "",
    "text": "Too much is being done at import time\nImporting should typically not “do work” (e.g. it should not load datafiles, connect to databases, instantiate drivers, etc). The biggest exception to this is probably setting up loggers, because the first caller to logging.basicConfig can change what all the other loggers can see.\nBecause logging is a cross-cutting concern, loggers are also generally made as singletons at the module level.\nThe reason that imports can be so slow is if we have a package structure like\nworld/\n│\n├── africa/\n│   ├── __init__.py\n│   └── zimbabwe.py\n│\n├── europe/\n│   ├── __init__.py\n│   ├── greece.py\n│   ├── norway.py\n│   └── spain.py\n│\n└── __init__.py\nand you write import world.europe.spain into Python, then\n\nFirst world.__init__.py gets imported (if it hasn’t been imported already)\nThen world.europe.__init__.py gets imported (if it hasn’t been imported already)\nFinally, world.europe.spain.py gets imported\n\nIf you did import world.europe.greece after importing spain, then only world.europe.greece.py would be imported on this second call (each module is only imported once).\nBut this means if your __init__.py files anywhere in the tree “do work”, then every subpackage that you import is also going to be slow. This is one of the reasons for recommending that __init__.py files are kept relatively sparse."
  },
  {
    "objectID": "posts/pandas/post.html",
    "href": "posts/pandas/post.html",
    "title": "Quick EDA settings hacks",
    "section": "",
    "text": "import pandas as pd \npd.set_option('display.min_rows', 500)\npd.set_option('display.max_rows', 500)\nSetting to None will also allow you to show all rows.\nI really like the ability to do this on a one-off basis too:\nfrom IPython.display import display \n\nwith pd.option_context('display.max_rows', 100, 'display.max_columns', 10):\n  display(df)"
  },
  {
    "objectID": "posts/pandas/post.html#setting-the-maximum-number-of-rows",
    "href": "posts/pandas/post.html#setting-the-maximum-number-of-rows",
    "title": "Quick EDA settings hacks",
    "section": "",
    "text": "import pandas as pd \npd.set_option('display.min_rows', 500)\npd.set_option('display.max_rows', 500)\nSetting to None will also allow you to show all rows.\nI really like the ability to do this on a one-off basis too:\nfrom IPython.display import display \n\nwith pd.option_context('display.max_rows', 100, 'display.max_columns', 10):\n  display(df)"
  },
  {
    "objectID": "posts/pandas/post.html#retina-display-quality",
    "href": "posts/pandas/post.html#retina-display-quality",
    "title": "Quick EDA settings hacks",
    "section": "Retina display quality",
    "text": "Retina display quality\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format='retina'"
  },
  {
    "objectID": "posts/pandas/grouper.html",
    "href": "posts/pandas/grouper.html",
    "title": "Pandas: Time-series on multiple columns (grouper)",
    "section": "",
    "text": "This problem is one that seems simple, but the “missing date” problem can make it really tricky. Let’s start with a motivating example. We have some daily data, but some of the days are missing (e.g. we have no sales)\n\nimport pandas as pd\nimport plotnine as p9\n\nsales = pd.DataFrame([\n    {'date': '2024-01-01', 'num_sold': 10},\n    {'date': '2024-01-02', 'num_sold': 12},\n    {'date': '2024-01-03', 'num_sold': 6},\n    {'date': '2024-01-04', 'num_sold': 16},\n    #{'date': '2024-01-05', 'num_sold': 22},\n    {'date': '2024-01-06', 'num_sold': 3},\n    #{'date': '2024-01-07', 'num_sold': 8},\n    #{'date': '2024-01-08', 'num_sold': 12},\n    #{'date': '2024-01-09', 'num_sold': 14},\n    {'date': '2024-01-10', 'num_sold': 9},\n    {'date': '2024-01-11', 'num_sold': 15},\n    {'date': '2024-01-12', 'num_sold': 20},\n    {'date': '2024-01-13', 'num_sold': 5},\n    #{'date': '2024-01-14', 'num_sold': 7},\n    #{'date': '2024-01-15', 'num_sold': 16},\n    {'date': '2024-01-16', 'num_sold': 18},\n    {'date': '2024-01-17', 'num_sold': 10},\n    {'date': '2024-01-18', 'num_sold': 17},\n    {'date': '2024-01-19', 'num_sold': 19},\n    {'date': '2024-01-20', 'num_sold': 6},\n    #{'date': '2024-01-21', 'num_sold': 6},\n    #{'date': '2024-01-22', 'num_sold': 18},\n    {'date': '2024-01-23', 'num_sold': 22},\n    {'date': '2024-01-24', 'num_sold': 12},\n    #{'date': '2024-01-25', 'num_sold': 21},\n    {'date': '2024-01-26', 'num_sold': 22},\n    {'date': '2024-01-27', 'num_sold': 8},\n    #{'date': '2024-01-28', 'num_sold': 5},\n    {'date': '2024-01-29', 'num_sold': 17},\n    {'date': '2024-01-30', 'num_sold': 8},\n    {'date': '2024-01-31', 'num_sold': 11},\n    {'date': '2024-02-01', 'num_sold': 13},\n    {'date': '2024-02-02', 'num_sold': 4},\n])\nsales['date'] = pd.to_datetime(sales['date'])\n\nWe have some sales, with some dates missing (here it is the 5, 7, 8, 9, 14, 15, 21, 22, 25, and 28th of Jan). Maybe the store was closed, or we just didn’t have any sales that day. If we calculated a naive moving average, or a weekly sum, it can be challenging.\n\nsales\n\n\n\n\n\n\n\n\n\ndate\nnum_sold\n\n\n\n\n0\n2024-01-01\n10\n\n\n1\n2024-01-02\n12\n\n\n2\n2024-01-03\n6\n\n\n3\n2024-01-04\n16\n\n\n4\n2024-01-06\n3\n\n\n5\n2024-01-10\n9\n\n\n6\n2024-01-11\n15\n\n\n7\n2024-01-12\n20\n\n\n8\n2024-01-13\n5\n\n\n9\n2024-01-16\n18\n\n\n10\n2024-01-17\n10\n\n\n11\n2024-01-18\n17\n\n\n12\n2024-01-19\n19\n\n\n13\n2024-01-20\n6\n\n\n14\n2024-01-23\n22\n\n\n15\n2024-01-24\n12\n\n\n16\n2024-01-26\n22\n\n\n17\n2024-01-27\n8\n\n\n18\n2024-01-29\n17\n\n\n19\n2024-01-30\n8\n\n\n20\n2024-01-31\n11\n\n\n21\n2024-02-01\n13\n\n\n22\n2024-02-02\n4\n\n\n\n\n\n\n\n\nJust doing a lag of 7 would be completely wrong!\nThere are a couple of ways of doing this. For a single data source, the easiest is to resample on a date index, and then sum:\n\nsales.set_index('date').resample('W')['num_sold'].sum()\n\ndate\n2024-01-07    47\n2024-01-14    49\n2024-01-21    70\n2024-01-28    64\n2024-02-04    53\nFreq: W-SUN, Name: num_sold, dtype: int64\n\n\nWe can see even if we eliminate a complete week that the zero still shows\n\nsales[\n    (sales['date'] &lt; '2024-01-14') | (sales['date'] &gt; '2024-01-21')\n].set_index('date').resample('W')['num_sold'].sum()\n\ndate\n2024-01-07    47\n2024-01-14    49\n2024-01-21     0\n2024-01-28    64\n2024-02-04    53\nFreq: W-SUN, Name: num_sold, dtype: int64\n\n\n\n\nSome plotting software makes seeing the missing dates easier / harder, so be careful. With a default pandas plot, it can be difficult to notice the missing dates:\n\nsales.plot(x='date', y='num_sold')\n\n\n\n\n\n\n\n\nAdding points to the graph (possible in plotnine or using matplotlib) makes it easier to notice.\n\n(\n    p9.ggplot(sales, p9.aes(x='date', y='num_sold'))\n    + p9.geom_line() + p9.geom_point()\n    + p9.scale_x_date(breaks='4 day')\n)\n\n\n\n\n\n\n\n\n\n\n\nSuppose we had two SKUs we wanted to track: sales of apples and oranges. Then the index trick doesn’t work as well. Let’s use a much smaller data frame (from this stackoverflow question)\n\nsimple = pd.DataFrame([\n    {'fruit': 'apple', 'date': '2017-07-11', 'quantity': 20},\n    {'fruit': 'orange', 'date': '2017-07-14', 'quantity': 20},\n    {'fruit': 'apple', 'date': '2017-07-14', 'quantity': 70},\n    {'fruit': 'orange', 'date': '2017-07-25', 'quantity': 40},\n    {'fruit': 'apple', 'date': '2017-07-20', 'quantity': 30},\n])\nsimple['date'] = pd.to_datetime(simple['date'])\nsimple\n\n\n\n\n\n\n\n\n\nfruit\ndate\nquantity\n\n\n\n\n0\napple\n2017-07-11\n20\n\n\n1\norange\n2017-07-14\n20\n\n\n2\napple\n2017-07-14\n70\n\n\n3\norange\n2017-07-25\n40\n\n\n4\napple\n2017-07-20\n30\n\n\n\n\n\n\n\n\nYou could do a pivot on the fruits, and use a similar resample and sum. This does not scale to having hundreds or thousands of different products.\nInstead, you can use a pandas.Grouper on the date. This allows us to transform the column, and then group by the transformed output!\n\n(\n    simple.groupby(['fruit', pd.Grouper(key='date', freq='W-MON')])['quantity']\n    .sum()\n    .reset_index()\n    .sort_values('date')\n)\n\n\n\n\n\n\n\n\n\nfruit\ndate\nquantity\n\n\n\n\n0\napple\n2017-07-17\n90\n\n\n2\norange\n2017-07-17\n20\n\n\n1\napple\n2017-07-24\n30\n\n\n3\norange\n2017-07-31\n40\n\n\n\n\n\n\n\n\n\n\n\n\nsimple2 = pd.DataFrame([\n    {'fruit': 'apple', 'date': '2017-07-11', 'quantity': 20},\n    {'fruit': 'orange', 'date': '2017-07-14', 'quantity': 20},\n    {'fruit': 'apple', 'date': '2017-07-14', 'quantity': 70},\n    {'fruit': 'orange', 'date': '2017-07-25', 'quantity': 40},\n    {'fruit': 'apple', 'date': '2017-07-20', 'quantity': 30},\n    {'fruit': 'apple', 'date': '2017-07-30', 'quantity': 30},\n    {'fruit': 'apple', 'date': '2017-08-05', 'quantity': 30},\n    {'fruit': 'apple', 'date': '2017-08-12', 'quantity': 30},\n])\nsimple2['date'] = pd.to_datetime(simple2['date'])\nsimple2\n\n\n\n\n\n\n\n\n\nfruit\ndate\nquantity\n\n\n\n\n0\napple\n2017-07-11\n20\n\n\n1\norange\n2017-07-14\n20\n\n\n2\napple\n2017-07-14\n70\n\n\n3\norange\n2017-07-25\n40\n\n\n4\napple\n2017-07-20\n30\n\n\n5\napple\n2017-07-30\n30\n\n\n6\napple\n2017-08-05\n30\n\n\n7\napple\n2017-08-12\n30\n\n\n\n\n\n\n\n\n\n(\n    simple2.groupby(['fruit', pd.Grouper(key='date', freq='W-MON')])['quantity']\n    .sum()\n    .reset_index()\n    .sort_values('date')\n)\n\n\n\n\n\n\n\n\n\nfruit\ndate\nquantity\n\n\n\n\n0\napple\n2017-07-17\n90\n\n\n5\norange\n2017-07-17\n20\n\n\n1\napple\n2017-07-24\n30\n\n\n2\napple\n2017-07-31\n30\n\n\n6\norange\n2017-07-31\n40\n\n\n3\napple\n2017-08-07\n30\n\n\n4\napple\n2017-08-14\n30"
  },
  {
    "objectID": "posts/pandas/grouper.html#side-note-plots",
    "href": "posts/pandas/grouper.html#side-note-plots",
    "title": "Pandas: Time-series on multiple columns (grouper)",
    "section": "",
    "text": "Some plotting software makes seeing the missing dates easier / harder, so be careful. With a default pandas plot, it can be difficult to notice the missing dates:\n\nsales.plot(x='date', y='num_sold')\n\n\n\n\n\n\n\n\nAdding points to the graph (possible in plotnine or using matplotlib) makes it easier to notice.\n\n(\n    p9.ggplot(sales, p9.aes(x='date', y='num_sold'))\n    + p9.geom_line() + p9.geom_point()\n    + p9.scale_x_date(breaks='4 day')\n)"
  },
  {
    "objectID": "posts/pandas/grouper.html#making-it-harder",
    "href": "posts/pandas/grouper.html#making-it-harder",
    "title": "Pandas: Time-series on multiple columns (grouper)",
    "section": "",
    "text": "Suppose we had two SKUs we wanted to track: sales of apples and oranges. Then the index trick doesn’t work as well. Let’s use a much smaller data frame (from this stackoverflow question)\n\nsimple = pd.DataFrame([\n    {'fruit': 'apple', 'date': '2017-07-11', 'quantity': 20},\n    {'fruit': 'orange', 'date': '2017-07-14', 'quantity': 20},\n    {'fruit': 'apple', 'date': '2017-07-14', 'quantity': 70},\n    {'fruit': 'orange', 'date': '2017-07-25', 'quantity': 40},\n    {'fruit': 'apple', 'date': '2017-07-20', 'quantity': 30},\n])\nsimple['date'] = pd.to_datetime(simple['date'])\nsimple\n\n\n\n\n\n\n\n\n\nfruit\ndate\nquantity\n\n\n\n\n0\napple\n2017-07-11\n20\n\n\n1\norange\n2017-07-14\n20\n\n\n2\napple\n2017-07-14\n70\n\n\n3\norange\n2017-07-25\n40\n\n\n4\napple\n2017-07-20\n30\n\n\n\n\n\n\n\n\nYou could do a pivot on the fruits, and use a similar resample and sum. This does not scale to having hundreds or thousands of different products.\nInstead, you can use a pandas.Grouper on the date. This allows us to transform the column, and then group by the transformed output!\n\n(\n    simple.groupby(['fruit', pd.Grouper(key='date', freq='W-MON')])['quantity']\n    .sum()\n    .reset_index()\n    .sort_values('date')\n)\n\n\n\n\n\n\n\n\n\nfruit\ndate\nquantity\n\n\n\n\n0\napple\n2017-07-17\n90\n\n\n2\norange\n2017-07-17\n20\n\n\n1\napple\n2017-07-24\n30\n\n\n3\norange\n2017-07-31\n40"
  },
  {
    "objectID": "posts/pandas/grouper.html#how-does-this-handle-missing-dates",
    "href": "posts/pandas/grouper.html#how-does-this-handle-missing-dates",
    "title": "Pandas: Time-series on multiple columns (grouper)",
    "section": "",
    "text": "simple2 = pd.DataFrame([\n    {'fruit': 'apple', 'date': '2017-07-11', 'quantity': 20},\n    {'fruit': 'orange', 'date': '2017-07-14', 'quantity': 20},\n    {'fruit': 'apple', 'date': '2017-07-14', 'quantity': 70},\n    {'fruit': 'orange', 'date': '2017-07-25', 'quantity': 40},\n    {'fruit': 'apple', 'date': '2017-07-20', 'quantity': 30},\n    {'fruit': 'apple', 'date': '2017-07-30', 'quantity': 30},\n    {'fruit': 'apple', 'date': '2017-08-05', 'quantity': 30},\n    {'fruit': 'apple', 'date': '2017-08-12', 'quantity': 30},\n])\nsimple2['date'] = pd.to_datetime(simple2['date'])\nsimple2\n\n\n\n\n\n\n\n\n\nfruit\ndate\nquantity\n\n\n\n\n0\napple\n2017-07-11\n20\n\n\n1\norange\n2017-07-14\n20\n\n\n2\napple\n2017-07-14\n70\n\n\n3\norange\n2017-07-25\n40\n\n\n4\napple\n2017-07-20\n30\n\n\n5\napple\n2017-07-30\n30\n\n\n6\napple\n2017-08-05\n30\n\n\n7\napple\n2017-08-12\n30\n\n\n\n\n\n\n\n\n\n(\n    simple2.groupby(['fruit', pd.Grouper(key='date', freq='W-MON')])['quantity']\n    .sum()\n    .reset_index()\n    .sort_values('date')\n)\n\n\n\n\n\n\n\n\n\nfruit\ndate\nquantity\n\n\n\n\n0\napple\n2017-07-17\n90\n\n\n5\norange\n2017-07-17\n20\n\n\n1\napple\n2017-07-24\n30\n\n\n2\napple\n2017-07-31\n30\n\n\n6\norange\n2017-07-31\n40\n\n\n3\napple\n2017-08-07\n30\n\n\n4\napple\n2017-08-14\n30"
  },
  {
    "objectID": "python-series.html",
    "href": "python-series.html",
    "title": "Series: python and pandas tips",
    "section": "",
    "text": "Timing imports\n\n\n\n\n\nTracing particularly slow imports\n\n\n\n\n\nOct 14, 2024\n\n\n2 min\n\n\n\n\n\n\n\nPublishing Python Packages\n\n\n\n\n\nHow to push packages to artifactory with either setup.py or pyproject.toml\n\n\n\n\n\nMay 27, 2024\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pandas-series.html",
    "href": "pandas-series.html",
    "title": "Series: Pandas tips",
    "section": "",
    "text": "Stats Models: Linear Regression\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\n1 min\n\n\n\n\n\n\n\nPandas: Time-series on multiple columns (grouper)\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2024\n\n\n2 min\n\n\n\n\n\n\n\nPandas: Transform like behavior using data from multiple columns\n\n\n\n\n\n\n\n\n\n\n\nJul 3, 2024\n\n\n1 min\n\n\n\n\n\n\n\nQuick EDA settings hacks\n\n\n\n\n\nSettings that make notebooks easier to use, especially for EDA\n\n\n\n\n\nMay 1, 2024\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  }
]